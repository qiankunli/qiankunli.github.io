---

layout: post
title: å¤§æ¨¡å‹æ¨ç†æœåŠ¡æ¡†æ¶
category: æ¶æ„
tags: MachineLearning
keywords: llm inference

---

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$']], // æ”¯æŒ $å’Œ$$ ä½œä¸ºè¡Œå†…å…¬å¼åˆ†éš”ç¬¦
      displayMath: [['$$', '$$']], // å—çº§å…¬å¼åˆ†éš”ç¬¦
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script async src="/public/js/mathjax/es5/tex-mml-chtml.js"></script>


* TOC
{:toc}

## ç®€ä»‹

åœ¨å¤§æ¨¡å‹ä¹‹å‰çš„æ—¶ä»£ï¼Œæ¨¡å‹ç»“æ„ä¸æ–­å‘æ•£ï¼Œä½†æ¨ç†åŠŸèƒ½çš„å½¢æ€æ˜¯ç¨³å®šçš„ï¼Œå› æ­¤æ¨ç†æ¡†æ¶æ¼”åŒ–çš„ç»“æœæ˜¯é‡Builderï¼Œè½»Runtimeã€‚ä½†å¤§æ¨¡å‹æ—¶ä»£æ°æ°ç›¸åï¼Œæ¨¡å‹ç»“æ„ç›¸å¯¹ç¨³å®šï¼Œæ¨ç†åŠŸèƒ½å´èŠ±æ ·è¿­å‡ºï¼Œå› æ­¤éœ€è¦ä¸€ä¸ªçµæ´»çš„ï¼Œé¢„ç•™è¶³å¤Ÿç©ºé—´çš„Runtimeæ¨¡å—ï¼ŒBuilderçš„å„ç§ä¼˜åŒ–åè€Œå˜å¾—ç®€å•ã€‚ç»¼åˆå‡ ç‚¹æ¥çœ‹ï¼Œå¤§æ¨¡å‹å¯¹äºæ¨ç†æ¡†æ¶çš„è®¾è®¡å“²å­¦çš„æ”¹å˜æ˜¯é©å‘½æ€§çš„ï¼Œå·²æœ‰çš„æˆç†Ÿçš„æ¨ç†æ¡†æ¶æ˜¾å¾—è¿‡äºé™ˆæ—§ï¼Œéš¾ä»¥å®Œæˆè¿™äº›å˜åŒ–ï¼Œè€Œæ–°çš„æ¨ç†æ¡†æ¶åˆè¿‡äºç®€å•(ä»¥vllmä¸ºä»£è¡¨)ï¼Œ24å¹´çš„åšå¼ˆä¼šéå¸¸ç²¾å½©ã€‚

1. vLLMæ˜¯ä¸€ä¸ªå¼€æºçš„å¤§æ¨¡å‹æ¨ç†åŠ é€Ÿæ¡†æ¶ï¼Œé€šè¿‡PagedAttentioné«˜æ•ˆåœ°ç®¡ç†attentionä¸­ç¼“å­˜çš„å¼ é‡ï¼Œå®ç°äº†æ¯”HuggingFace Transformersé«˜14-24å€çš„ååé‡ï¼Œå°±åƒåœ¨æ“ä½œç³»ç»Ÿä¸­ç®¡ç†CPUè™šæ‹Ÿå†…å­˜ä¸€æ ·
2. NVIDIA FasterTransformer (FT) æ˜¯ä¸€ä¸ªç”¨äºå®ç°åŸºäºTransformerçš„ç¥ç»ç½‘ç»œæ¨ç†çš„åŠ é€Ÿå¼•æ“ã€‚å®ƒåŒ…å«Transformerå—çš„é«˜åº¦ä¼˜åŒ–ç‰ˆæœ¬çš„å®ç°ï¼Œå…¶ä¸­åŒ…å«ç¼–ç å™¨å’Œè§£ç å™¨éƒ¨åˆ†ã€‚ä½¿ç”¨æ­¤æ¨¡å—ï¼Œæ‚¨å¯ä»¥è¿è¡Œç¼–ç å™¨-è§£ç å™¨æ¶æ„æ¨¡å‹ï¼ˆå¦‚ï¼šT5ï¼‰ã€ä»…ç¼–ç å™¨æ¶æ„æ¨¡å‹ï¼ˆå¦‚ï¼šBERTï¼‰å’Œä»…è§£ç å™¨æ¶æ„æ¨¡å‹ï¼ˆå¦‚ï¼šGPTï¼‰çš„æ¨ç†ã€‚FTæ¡†æ¶æ˜¯ç”¨C++/CUDAç¼–å†™çš„ï¼Œä¾èµ–äºé«˜åº¦ä¼˜åŒ–çš„ cuBLASã€cuBLASLt å’Œ cuSPARSELt åº“ï¼Œè¿™ä½¿æ‚¨å¯ä»¥åœ¨ GPU ä¸Šè¿›è¡Œå¿«é€Ÿçš„ Transformer æ¨ç†ã€‚ä¸ NVIDIA TensorRT ç­‰å…¶ä»–ç¼–è¯‘å™¨ç›¸æ¯”ï¼ŒFT çš„æœ€å¤§ç‰¹ç‚¹æ˜¯å®ƒæ”¯æŒä»¥åˆ†å¸ƒå¼æ–¹å¼è¿›è¡Œ Transformer å¤§æ¨¡å‹æ¨ç†ã€‚åœ¨åº•å±‚ï¼ŒèŠ‚ç‚¹é—´æˆ–èŠ‚ç‚¹å†…é€šä¿¡ä¾èµ–äº MPI ã€ NVIDIA NCCLã€Glooç­‰ã€‚å› æ­¤ï¼Œä½¿ç”¨FasterTransformerï¼Œæ‚¨å¯ä»¥åœ¨å¤šä¸ª GPU ä¸Šä»¥å¼ é‡å¹¶è¡Œè¿è¡Œå¤§å‹Transformerï¼Œä»¥å‡å°‘è®¡ç®—å»¶è¿Ÿã€‚åŒæ—¶ï¼ŒTP å’Œ PP å¯ä»¥ç»“åˆåœ¨ä¸€èµ·ï¼Œåœ¨å¤š GPU èŠ‚ç‚¹ç¯å¢ƒä¸­è¿è¡Œå…·æœ‰æ•°åäº¿ã€æ•°ä¸‡äº¿ä¸ªå‚æ•°çš„å¤§å‹ Transformer æ¨¡å‹ã€‚
3. DeepSpeed-MII æ˜¯ DeepSpeed çš„ä¸€ä¸ªæ–°çš„å¼€æº Python åº“ï¼Œæ—¨åœ¨ä½¿æ¨¡å‹ä¸ä»…ä½å»¶è¿Ÿå’Œä½æˆæœ¬æ¨ç†ï¼Œè€Œä¸”è¿˜æ˜“äºè®¿é—®ã€‚

ä½¿ç”¨å¤§æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬åœ¨huggingfaceæˆ–modelscope çœ‹åˆ°çš„ä»£ç ç±»ä¼¼ä¸‹é¢ï¼Œå¾ˆæ˜æ˜¾ä¸èƒ½ç›´æ¥å‘ç”¨æˆ·æä¾›æœåŠ¡ã€‚ 
```python
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True)
model = AutoModel.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True).half().cuda()
model = model.eval()
response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
print(response)
ä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚
response, history = model.chat(tokenizer, "æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ", history=history)
print(response)
æ™šä¸Šç¡ä¸ç€å¯èƒ½ä¼šè®©ä½ æ„Ÿåˆ°ç„¦è™‘æˆ–ä¸èˆ’æœ,ä½†ä»¥ä¸‹æ˜¯ä¸€äº›å¯ä»¥å¸®åŠ©ä½ å…¥ç¡çš„æ–¹æ³•:...
```

ä¸€èˆ¬æœ‰å‡ ä¸ªéœ€æ±‚
1. ç»Ÿä¸€apiï¼Œè¿™æ ·åˆ‡æ¢æ¨¡å‹æ—¶ä¸Šæ¸¸åº”ç”¨æ— æ„Ÿï¼Œæœ€å¥½æ˜¯ OpenAI-compatibleï¼Œå…¶api è¢«ä¸»è¦ä¸Šæ¸¸æ¡†æ¶ï¼ˆæ¯”å¦‚langchainï¼‰å…¼å®¹
    1. æ”¯æŒæµå¼è¾“å‡ºå’Œæ™®é€šè¾“å‡º
2. æ”¯æŒå¤šå®ä¾‹ï¼Œè¿›è€Œæ”¯æŒç°åº¦å‘å¸ƒç­‰
3. æ”¯æŒé€šç”¨çš„åŠ é€Ÿåº“ï¼Œæ¯”å¦‚vllmç­‰
4. promptæ‹¼å†™ï¼šllmæœ¬è´¨éƒ½æ˜¯è¯­è¨€æ¨¡å‹ï¼Œå› æ­¤æä¾›çš„åªæœ‰è¯­è¨€æ¨¡å‹è°ƒç”¨æ–¹å¼ï¼Œå°†æ‰€æœ‰è¯·æ±‚ç®€åŒ–ä¸ºè¾“å…¥ä¸€ä¸ªstringï¼Œè¾“å‡ºä¸€ä¸ªstringçš„æ¨¡å¼ã€‚ç„¶è€Œï¼Œä»è¯­è¨€æ¨¡å‹åˆ°chatåº”ç”¨ä¹‹é—´ä»ç„¶æœ‰ä¸€ä¸ªgapï¼šè¾“å…¥promptçš„æ‹¼å†™ã€‚text-in-text-outçš„è®¾è®¡å¯ä»¥ç®€åŒ–å¼•æ“å¼€å‘ï¼Œä½†æ˜¯promptæ‹¼å†™çš„éš¾é¢˜å°±è¢«ä¸¢ç»™äº†ç”¨æˆ·ã€‚æä¾›chatèƒ½åŠ›çš„æ ¸å¿ƒéœ€æ±‚æ˜¯å¦‚ä½•å°†å¤šè½®å¯¹è¯æŒ‰ç…§æ¨¡å‹è®­ç»ƒæ—¶çš„æ ¼å¼æ¸²æŸ“æˆæ¨¡å‹çš„input idã€‚[ä»Language Modelåˆ°Chat Applicationï¼šå¯¹è¯æ¥å£çš„è®¾è®¡ä¸å®ç°](https://mp.weixin.qq.com/s/DfMJVZnqFpsubKJ60H8s7g)
5. function call çš„å¤„ç†ã€‚ä»¥ReActæ¨¡æ¿çš„promptä¸ºä¾‹ï¼Œåœ¨æ¨¡å‹åå­—æ—¶ç•™ä¸Šä¸€å°å—bufferä¸è¿”å›ï¼Œå¦‚æœæ²¡æœ‰`\nAction:` é‚£å°±ç»§ç»­è¿”å›ï¼›å¦‚æœé‡åˆ°è¿™ä¸ªstringï¼Œåˆ™è¯´æ˜æ¨¡å‹å¯èƒ½è¦è¾“å‡ºfunction callï¼Œåœ¨æ­¤æ”¶é›†è¾“å‡ºç›´åˆ°é‡åˆ°eosæˆ–è€…ä½œä¸ºstop word çš„`\nObservation:`ï¼Œç„¶åå†æŠŠbufferä¸€æ¬¡æ€§parseæˆå‡½æ•°å¹¶è¿”å›ã€‚

è®¾è®¡æ€è·¯
1. å› ä¸ºè¦æ”¯æŒä¸åŒçš„llm åº“æˆ–åŠ é€Ÿåº“ï¼Œæ¯”å¦‚Transformerã€vllmç­‰ï¼Œä¸”ä¸åŒçš„llmåœ¨ä¸€äº›ç»†èŠ‚ä¸Šæœ‰å·®å¼‚ï¼Œå› æ­¤æ¨ç†ä¾§å¿…é¡»æœ‰ä¸€ä¸ªç»Ÿä¸€çš„LLM æŠ½è±¡ï¼Œåœ¨Fastchaté‡Œæ˜¯XXModelWorkerï¼Œåœ¨xinference é‡Œæ˜¯XXLLM
2. å°†python llm åº“ apiåŒ–ï¼Œä¸€ä¸ªapi è¦æœ‰ä¸€ä¸ªapi handler å‡½æ•°ï¼Œä¸€èˆ¬æŠ½è±¡ä¸ºä¸€ä¸ªå¯¹è±¡ ä½œä¸ºapi handlerçš„è½½ä½“ï¼Œè¿™ä¸ªå¯¹è±¡æŒæœ‰ä¸Šé¢çš„XxLLM æ‰§è¡Œchat/generate æ–¹æ³•ï¼Œæœ‰æ—¶å€™è¿˜è¦æ”¯æŒ/å°è£…åˆ†å¸ƒå¼ã€å¼‚æ­¥ç­‰ç»†èŠ‚ã€‚åœ¨Fastchaté‡Œæ˜¯ModelWorkerï¼Œåœ¨xinference é‡Œæ˜¯WorkerActor
3. ä¸åŒçš„llm è¿˜æœ‰å¾ˆå¤šå·®åˆ«çš„ï¼ˆæ¯”å¦‚åŠ è½½ load_modelã€è¿è¡Œchat/generateã€æ¨¡å‹é…ç½®è½¬æ¢ï¼‰ï¼Œä¹Ÿæœ‰å¾ˆå¤šå…±æ€§ï¼Œæ‰€ä»¥æ¨¡å‹è®¾è®¡çš„åˆ†å±‚æŠ½è±¡å¾ˆé‡è¦ï¼ŒFastchat çš„æ€è·¯æ˜¯ æä¾›äº†ä¸€ä¸ªModelAdapterï¼ˆä¸»è¦å·®å¼‚åŒ–äº†åŠ è½½ï¼‰ å’Œä¸€ä¸ª generate_stream_gate å‡½æ•°æˆå‘˜ï¼ˆå·®å¼‚åŒ–textç”Ÿæˆï¼‰ï¼Œinferenceçš„æ€è·¯æ˜¯ä¸€ä¸ªæ¨¡å‹ï¼ˆæ¯”å¦‚chatglmã€llamaç­‰ï¼‰ä¸€ä¸ªXXLLM
  1. è¿™é‡Œçš„æ¨¡å‹é…ç½®è½¬æ¢è¯´çš„æ˜¯ï¼Œæ¯”å¦‚ä¸€ä¸ªchat message åŒ…å«role å’Œcontent ä¸¤ä¸ªéƒ¨åˆ†ï¼Œrole=system/user/assistant å„å®¶å„æœ‰å·®å¼‚ï¼Œä½†å› ä¸ºå¯¹å¤–æä¾›çš„æ¥å£ä¸€èˆ¬æ˜¯openai é£æ ¼ï¼Œæ‰€ä»¥æœ‰ä¸€ä¸ªè½¬æ¢çš„è¿‡ç¨‹ã€‚
4. é™¤äº†æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œè¿˜ç»å¸¸éœ€è¦éƒ¨ç½²embeddingæ¨¡å‹ã€rerankæ¨¡å‹ã€å›¾ç”Ÿå›¾ã€æ–‡ç”Ÿå›¾ç­‰ï¼ˆå…¥å‚å‡ºå‚ä¸LLM è‚¯å®šä¸ä¸€æ ·äº†ï¼‰ï¼ŒFastchat çš„æ–¹å¼æ˜¯ è®©ModelWorkeræ”¯æŒé™¤äº†generate_stream_xx å¤–çš„get_embeddingsã€get_rerankæ–¹æ³•ï¼Œinferenceçš„æ€è·¯é™¤äº†LLMä¹‹å¤–è¿˜å®šä¹‰äº† EmbeddingModelã€RerankModelç­‰ã€‚

ç”±äºåƒGPU è¿™æ ·çš„åŠ é€Ÿå™¨å…·æœ‰å¤§é‡çš„å¹¶è¡Œè®¡ç®—å•å…ƒï¼Œæ¨ç†æœåŠ¡ç³»ç»Ÿé€šå¸¸ä¼šå¯¹ä½œä¸šè¿›è¡Œæ‰¹å¤„ç†ï¼Œä»¥æé«˜ç¡¬ä»¶åˆ©ç”¨ç‡å’Œç³»ç»Ÿååé‡ã€‚å¯ç”¨æ‰¹å¤„åï¼Œæ¥è‡ªå¤šä¸ªä½œä¸šçš„è¾“å…¥ä¼šè¢«åˆå¹¶åœ¨ä¸€èµ·ï¼Œå¹¶ä½œä¸ºæ•´ä½“è¾“å…¥æ¨¡å‹ã€‚ä½†æ˜¯æ­¤å‰æ¨ç†æœåŠ¡ç³»ç»Ÿä¸»è¦é’ˆå¯¹ç¡®å®šæ€§æ¨¡å‹è¿›è¡Œæ¨ç†ï¼ˆLLMè¾“å‡ºé•¿åº¦æœªçŸ¥ï¼Œä½¿å¾—ä¸€ä¸ªæ¨ç†ä»»åŠ¡æ€»æ‰§è¡Œæ—¶é—´æœªçŸ¥ï¼‰ï¼Œå®ƒä¾èµ–äºå‡†ç¡®çš„æ‰§è¡Œæ—¶é—´åˆ†ææ¥è¿›è¡Œè°ƒåº¦å†³ç­–ï¼Œè€Œè¿™å¯¹LLMå¹¶ä¸é€‚ç”¨ã€‚æ­¤å¤–ï¼Œæ‰¹å¤„ç†ä¸å•ä¸ªä½œä¸šæ‰§è¡Œç›¸æ¯”ï¼Œå†…å­˜å¼€é”€æ›´é«˜ï¼Œå› æ­¤LLMçš„å°ºå¯¸é™åˆ¶äº†å…¶æ¨ç†çš„æœ€å¤§æ‰¹å¤„ç†æ•°é‡ã€‚ä¼ ç»Ÿçš„ä½œä¸šè°ƒåº¦å°†ä½œä¸šæŒ‰ç…§æ‰¹æ¬¡è¿è¡Œï¼Œç›´åˆ°ä¸€ä¸ªæ‰¹æ¬¡ä¸­çš„æ‰€æœ‰ä½œä¸šå®Œæˆï¼Œæ‰è¿›è¡Œä¸‹ä¸€æ¬¡è°ƒåº¦ï¼Œè¿™ä¼šé€ æˆæå‰å®Œæˆçš„ä½œä¸šæ— æ³•è¿”å›ç»™å®¢æˆ·ç«¯ï¼Œè€Œæ–°åˆ°è¾¾çš„ä½œä¸šåˆ™å¿…é¡»ç­‰å½“å‰æ‰¹æ¬¡å®Œæˆã€‚å› æ­¤æå‡º iteration-level è°ƒåº¦ç­–ç•¥ï¼Œåœ¨æ¯æ¬¡æ‰¹æ¬¡ä¸Šåªè¿è¡Œå•ä¸ªè¿­ä»£ï¼Œå³æ¯ä¸ªä½œä¸šä»…ç”Ÿæˆä¸€ä¸ªtokenã€‚æ¯æ¬¡è¿­ä»£å®Œæˆåï¼Œå®Œæˆçš„ä½œä¸šå¯ä»¥ç¦»å¼€æ‰¹æ¬¡ï¼Œæ–°åˆ°è¾¾çš„ä½œä¸šå¯ä»¥åŠ å…¥æ‰¹æ¬¡ã€‚ 

## ç®€å•å°è£…

[ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) åœ¨github æœ‰ä¸€ä¸ªä»“åº“ï¼Œä¸€èˆ¬åŒ…å«
1. æ¨¡å‹ä»‹ç» README.md
2. æ¨¡å‹çš„å¯¹å¤–æ¥å£ api.py/cli_demo.py/web_demo.pyã€‚ è‡ªå·±ä½¿ç”¨ fastapi åŸºäºpythonåº“ç›´æ¥å¯¹å¤–æä¾›RESTful APIs.

ä»¥api.py ä¸ºä¾‹
```python
from fastapi import FastAPI, Request
from transformers import AutoTokenizer, AutoModel
import uvicorn, json, datetime
import torch

app = FastAPI()

@app.post("/")
async def create_item(request: Request):
    global model, tokenizer
    json_post_raw = await request.json()
    json_post = json.dumps(json_post_raw)
    json_post_list = json.loads(json_post)
    prompt = json_post_list.get('prompt')
    history = json_post_list.get('history')
    max_length = json_post_list.get('max_length')
    top_p = json_post_list.get('top_p')
    temperature = json_post_list.get('temperature')
    response, history = model.chat(tokenizer,prompt, history=history,
                                   max_length=max_length if max_length else 2048,
                                   top_p=top_p if top_p else 0.7,
                                   temperature=temperature if temperature else 0.95)
    now = datetime.datetime.now()
    time = now.strftime("%Y-%m-%d %H:%M:%S")
    answer = {"response": response,"history": history,"status": 200,"time": time}
    log = "[" + time + "] " + '", prompt:"' + prompt + '", response:"' + repr(response) + '"'
    print(log)
    torch_gc()
    return answer

if __name__ == '__main__':
    tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)
    model = AutoModel.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True).half().cuda()
    model.eval()
    uvicorn.run(app, host='0.0.0.0', port=8000, workers=1)
```

## FastChat


å¦‚ä½•ç†è§£FastChat éƒ½å¹²äº†ä»€ä¹ˆï¼Ÿæœ¬è´¨æ˜¯å¯¹ä¸‹é¢çš„ åŸå§‹çš„å¤§æ¨¡å‹æ¨ç†ä»£ç è¿›è¡ŒæŠ½è±¡ï¼ˆæ¨¡å‹åŠ è½½ã€æ¨¡å‹æ¨ç†=tokenizer+modelï¼‰å’Œå°è£…ï¼Œå¯¹å¤–æä¾›rest apiã€‚

```python
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True)
model = AutoModel.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True).half().cuda()
model = model.eval()
response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
print(response)
ä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚
response, history = model.chat(tokenizer, "æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ", history=history)
print(response)
æ™šä¸Šç¡ä¸ç€å¯èƒ½ä¼šè®©ä½ æ„Ÿåˆ°ç„¦è™‘æˆ–ä¸èˆ’æœ,ä½†ä»¥ä¸‹æ˜¯ä¸€äº›å¯ä»¥å¸®åŠ©ä½ å…¥ç¡çš„æ–¹æ³•:...
```

[ä¸€æ–‡å…¥é—¨æœ€çƒ­çš„LLMåº”ç”¨å¼€å‘æ¡†æ¶LangChain](https://mp.weixin.qq.com/s/bYzNNL3F0998Do2Jl0PQtw)FastChatåŠŸèƒ½è¦†ç›–è®­ç»ƒï¼Œæ¨ç†ï¼Œè¯„ä¼°çš„å…¨è¿‡ç¨‹ã€‚è®¾è®¡ç›®æ ‡éå¸¸æ˜ç¡®ï¼Œå°±æ˜¯åœ¨æ€§èƒ½ã€åŠŸèƒ½åŠé£æ ¼ä¸Šå…¨é¢å¯¹æ ‡OpenAI ChatGPTï¼Œä»¥æˆä¸ºChatGPTçš„å¼€æºå¹³æ›¿ã€‚åœ¨ç”Ÿæ€é›†æˆä¸Šï¼Œç”±äºå®ƒå®Œå…¨å…¼å®¹OpenAIçš„é£æ ¼ï¼ŒåŸºäºChatGPTçš„langchainåº”ç”¨ï¼Œå¯ä»¥æ— ç¼åœ°ä½¿ç”¨FastChatæ›¿ä»£ã€‚ æ¨ç†ä¾§ç±»ä¼¼å·¥å…·Xinference/OpenLLM/RayLLM

[FastChat](https://github.com/lm-sys/FastChat)æ˜¯ä¸€ä¸ªç”¨äºè®­ç»ƒã€æœåŠ¡å’Œè¯„ä¼°åŸºäºèŠå¤©æœºå™¨äººçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„å¼€æ”¾å¹³å°ã€‚The core features include:
1. The training and evaluation code for state-of-the-art models (e.g., Vicuna).
2. A distributed multi-model serving system with web UI and OpenAI-compatible RESTful APIs.


```sh
# å‘½ä»¤è¡Œæ–¹å¼ä¸llm äº¤äº’
python3 -m fastchat.serve.cli --model-path lmsys/vicuna-7b-v1.3
# webuiæ–¹å¼ä¸llmäº¤äº’ï¼Œæ­¤æ—¶éœ€å¯åŠ¨3ä¸ªç»„ä»¶ web servers ==> controller ==> model workers
python3 -m fastchat.serve.controller
python3 -m fastchat.serve.model_worker --model-path lmsys/vicuna-7b-v1.3
python3 -m fastchat.serve.gradio_web_server
# æä¾›OpenAI-compatible RESTful APIs  openai_api_server ==> controller ==> model workers
python3 -m fastchat.serve.controller
python3 -m fastchat.serve.model_worker --model-path lmsys/vicuna-7b-v1.3
python3 -m fastchat.serve.openai_api_server --host localhost --port 8000
```

![](/public/upload/machine/langchain_chatchat_call.jpg)

### FastChatæºç åˆ†æ

```
fastchat
    /fastchat
        /model
            /model_adapter.py        # BaseModelAdapter  ChatGLMAdapter
            /model_chatglm.py        # generate_stream_chatglm
            /model_exllama.py
            /model_registry.py
        /protocol
            /api_protocol.py
            /openai_api_protocol.py
        /serve
            /multi_model_worker.py   # ç»´æŠ¤äº†ä¸€ä¸ª worker_map, key=model name,value = ModelWorker
            /model_worker.py         # app = FastAPI()     ModelWorker
            /controller.py.          # app = FastAPI().    Controller
            /openai_api_server.py    # app = fastapi.FastAPI()
        /train
```

ä½¿ç”¨ModelWorker åŠ è½½model æä¾›http æ¥å£ 

```python
# /fastchat/serve/model_worker.py
app = FastAPI()
@app.post("/worker_generate")
async def api_generate(request: Request):
    params = await request.json()
    await acquire_worker_semaphore()
    output = worker.generate_gate(params)
    release_worker_semaphore()
    return JSONResponse(output)
if __name__ == "__main__":
    ...
    worker = ModelWorker(...,args.model_path,)
    uvicorn.run(app, host=args.host, port=args.port, log_level="info")
```

ModelWorkerå®ç°

```python
class BaseModelWorker
     init_heart_beat
         # å°†modelWorker idæ³¨å†Œåˆ°controllerï¼Œå¹¶ä¿æŒå¿ƒè·³ã€‚å‡é€šè¿‡httpæ¥å£

# åŠ è½½æ¨¡å‹ï¼Œè°ƒç”¨æ¨¡å‹ï¼ˆåº•å±‚éƒ½æ˜¯è°ƒç”¨æµå¼æ¥å£ï¼‰
class ModelWorker(BaseModelWorker):
     def __init__():
          self.model, self.tokenizer = load_model(model_path, device=device,...)
            # load_model å¯¹åº”ä¸€ä¸ªä¸“é—¨çš„ ModelAdapter æŠ½è±¡ï¼Œç”¨æ¥é€‚é…æ¨¡å‹çš„åŠ è½½
            adapter = get_model_adapter(model_path)
            model, tokenizer = adapter.load_model(model_path, kwargs)
     generate_stream_gate(self, params) 
     generate_gate(self, params)    # æ ¹æ®å‚æ•°è¿”å›è¾“å‡ºï¼Œè°ƒç”¨generate_stream_gate
        for x in self.generate_stream_gate(params):
            pass
        return json.loads(x[:-1].decode())
```
ä»api handler åˆ°è¯·æ±‚è¢«å¤„ç†çš„è¿‡ç¨‹ï¼š api => ModelWorker.generate_gate ==> ModelWorker.generate_stream_gate ==> ModelWorker.model.stream_generateï¼Œè¿™é‡ŒModelWorkeræŒæœ‰çš„model æ˜¯Transformer model ã€‚
```python
def generate_stream_gate
    get_generate_stream_function(model: torch.nn.Module, model_path: str)
       # æ ¹æ®æ¨¡å‹ä¸åŒé€‰æ‹©å¯¹åº”çš„å‡½æ•° 
       generate_stream_chatglm
            prompt = params["prompt"]
            inputs = tokenizer([prompt], return_tensors="pt").to(model.device)
            for total_ids in model.stream_generate(**inputs, **gen_kwargs):
                  response = tokenizer.decode(output_ids)
                  response = process_response(response)
```

### è¯·æ±‚å‚æ•°è½¬æ¢

![](/public/upload/machine/fastchat_request_flow.png)

openai å‚æ•°è½¬ä¸º  model_worker api å‚æ•°ï¼Œæœ€ç»ˆè½¬ä¸º ï¼ˆTransformeråº“æˆ–ç±»ä¼¼ï¼‰model.generateå‚æ•°ï¼ˆæ¯”å¦‚input_idsã€attention_maskç­‰ï¼‰ã€‚
1. ç”¨æˆ·è¾“å…¥ï¼ˆå¯¹äºcompletion æ¥å£æ˜¯promptï¼Œå¯¹äºchat æ¥å£æ˜¯messagesï¼‰è¢«è½¬ä¸ºpromptï¼Œæœ€ç»ˆè¢«model å¯¹åº”çš„tokenizer è½¬ä¸ºinput_idsã€‚
2. ç”¨æˆ·è¾“å…¥ åœ¨è¢«è½¬ä¸ºprompt è¿‡ç¨‹å¯¹ä¸åŒçš„æ¨¡å‹æœ‰ä¸€äº›ä¸åŒï¼Œå› æ­¤è¦è¿›è¡Œä¸€äº›è½¬æ¢ï¼ˆå…¶å®ƒçš„è¯¸å¦‚stop tokenæ¯ä¸ªæ¨¡å‹ä¹Ÿæœ‰å·®å¼‚ï¼‰ã€‚æ¯”å¦‚å¯¹äºä¼šè¯æ•°æ®ï¼Œè½¬ä¸ºchatglm3 çš„prompt ä¼šç±»ä¼¼äºä»¥ä¸‹å½¢å¼

    ```
    <|user|>
    ä»Šå¤©åŒ—äº¬çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ
    <|assistant|>
    å¥½çš„ï¼Œè®©æˆ‘ä»¬æ¥æŸ¥çœ‹ä»Šå¤©çš„å¤©æ°”
    ```
    å½“ä½¿ç”¨openai chat æ¥å£æ—¶ï¼Œä¼ å…¥çš„æ•°æ®ä¸€èˆ¬ä¸º
    ```
    completion = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "user", "content": "ä»Šå¤©åŒ—äº¬çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"}
        ]
    )
    ```


### FastChat, How to support a new model?

1. FastChat uses the Conversation class to handle prompt templates and BaseModelAdapter class to handle model loading.
2. Implement a conversation template for the new model at `fastchat/conversation.py`. You can follow existing examples and use register_conv_template to add a new one. Please also add a link to the official reference code if possible. PSï¼š æ¯•ç«Ÿfastcaht æœåŠ¡chat åœºæ™¯å˜›ï¼Œå¯¹è¯è¯·æ±‚ä¼ å…¥çš„æ—¶å€™ ä¸€èˆ¬æ˜¯ `prompt = "\n###user:å¤©ä¸ºä»€ä¹ˆè¿™ä¹ˆè“ï¼Ÿ\n###"`ï¼Œè¦æŠŠè¿™ä¸ªè¿˜åŸä¸º `history = [{"role": "user", "content": "å¤©ä¸ºä»€ä¹ˆè¿™ä¹ˆè“ï¼Ÿ"}]`ï¼Œä¸åŒçš„æ¨¡å‹ å¯¹roleçš„ç§°å‘¼ä¸åŒã€‚
3. Implement a model adapter for the new model at `fastchat/model/model_adapter.py`. You can follow existing examples and use register_model_adapter to add a new one. PSï¼šä¸åŒçš„æ¨¡å‹åŠ è½½æ—¶æœ‰ä¸€äº›ç‰¹å®šçš„å‚æ•°ï¼Œæ¯”å¦‚ chatglm çš„trust_remote_code å‚æ•°ï¼Œ`model = AutoModel.from_pretrained(model_path, trust_remote_code=True, **from_pretrained_kwargs)`
4. ModelWorker ä¸»è¦é€»è¾‘æ˜¯æ‰§è¡Œ `generate_stream(model,tokenizer,params)` ï¼Œå¾ˆå¸¸è§„çš„ `input_ids = tokenizer(prompt); output_ids = model(input_ids,xx)`ã€‚ å¦‚æœæ¨¡å‹çš„generate é€»è¾‘æœ‰ä¸€äº›ç‰¹åˆ«çš„å¤„ç†ï¼Œåˆ™éœ€è¦è‡ªå®šä¹‰generate_stream_xxï¼Œå¹¶åŠ å…¥get_generate_stream_function é€»è¾‘ï¼ˆæ ¹æ®æ¨¡å‹åç­‰ è·¯ç”±åˆ°ä¸åŒçš„generate_stream_xxï¼‰
5. (Optional) add the model name to the "Supported models" section above and add more information in `fastchat/model/model_registry.py.`


## TensorRT-LLM

[â€‹æ­ç§˜NVIDIAå¤§æ¨¡å‹æ¨ç†æ¡†æ¶ï¼šTensorRT-LLM](https://mp.weixin.qq.com/s/z8gDDluDwjRwfeu5GruLIA) æœªè¯»

[LLMæ¨ç†ï¼šGPUèµ„æºå’Œæ¨ç†æ¡†æ¶é€‰æ‹©](https://mp.weixin.qq.com/s/coxDO2z2_w17UbbiTseNpw)TensorRT-LLMæ˜¯Nvidiaå¼€æºçš„LLMæ¨ç†å¼•æ“ï¼Œç”±å¼€æºé¡¹ç›®FastTransformeræ¼”è¿›è€Œæ¥ï¼ŒTensorRT-LLMéœ€è¦ç»“åˆTriton Inference Serveræ‰èƒ½æ„å»ºå®Œæ•´çš„LLMæ¨ç†æœåŠ¡ã€‚å¦‚æœéœ€è¦æ”¯æŒåŸºäºRESTFul APIçš„æµå¼è¾“å‡ºï¼ˆä¾‹å¦‚ï¼Œç±»ä¼¼OpenAIçš„LLMæ¨ç†APIæ¥å£ï¼‰ï¼Œè¿˜éœ€è¦è¿›ä¸€æ­¥é…åˆFastAPIæ‰èƒ½æ”¯æŒæµå¼è¾“å‡ºã€‚TensorRT-LLMç›®å‰è¿˜ä¸æ˜¯å®Œå…¨å¼€æºçš„ï¼Œä¾‹å¦‚ï¼ŒBatch Manager ç›®å‰æ˜¯ä¸å¼€æºçš„ã€‚

[å¤§è¯­è¨€æ¨¡å‹æ¨ç†æé€Ÿï¼šTensorRT-LLM é«˜æ€§èƒ½æ¨ç†å®è·µ](https://mp.weixin.qq.com/s/jnQs5XhWeAqoitahmDhziQ)TensorRT-LLM ä¸»è¦åˆ©ç”¨ä»¥ä¸‹å››é¡¹ä¼˜åŒ–æŠ€æœ¯æå‡ LLM æ¨¡å‹æ¨ç†æ•ˆç‡ã€‚
1. é‡åŒ–
2. In-Flight Batching
3. Attentionï¼ŒAttention æœºåˆ¶æŒ‰ç…§æ¼”è¿›é¡ºåºå¯ä»¥åˆ†ä¸º MHAï¼ˆMulti-head Attentionï¼‰ã€MQAï¼ˆMulti-query Attentionï¼‰ä»¥åŠ GQAï¼ˆGroup-query Attentionï¼‰æœºåˆ¶ã€‚MQA å’Œ GQA éƒ½æ˜¯ MHA çš„å˜ç§ã€‚MHA æ˜¯æ ‡å‡†çš„å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ¯ä¸ª query å­˜å‚¨ä¸€ä»½ KVï¼Œå› æ­¤éœ€è¦ä½¿ç”¨è¾ƒå¤šçš„æ˜¾å­˜ã€‚MQA æ‰€æœ‰ query å…±äº«ä¸€ä»½ KVï¼Œæ¨ç†æ—¶å®¹æ˜“ä¸¢å¤±ä¸€äº›ç»†èŠ‚ä¿¡æ¯ã€‚GQA å°† query è¿›è¡Œåˆ†ç»„ï¼Œç»„å†…å…±äº«ä¸€ä»½ KVï¼Œå¯ä»¥æœ‰æ•ˆé¿å… MHA å’Œ MQA çš„é—®é¢˜ã€‚
    ![](/public/upload/machine/attention_iteration.jpg)
4. Graph Rewritingï¼ŒTensorRT-LLMæä¾›äº†ä¸€ç»„ Python API ç”¨äºå®šä¹‰ LLMsï¼Œå¹¶ä¸”ä½¿ç”¨æœ€æ–°çš„ä¼˜åŒ–æŠ€æœ¯å°† LLM æ¨¡å‹è½¬æ¢ä¸º TensorRT Enginesï¼Œåœ¨å°† LLM æ¨¡å‹ç¼–è¯‘ä¸º TensorRT Engines æ—¶ä¼šå¯¹ç¥ç»ç½‘ç»œè¿›è¡Œä¼˜åŒ–ï¼Œæ¨ç†æ—¶ç›´æ¥ä½¿ç”¨ä¼˜åŒ–åçš„ TensorRT Enginesã€‚




