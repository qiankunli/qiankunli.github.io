---

layout: post
title: LLMå·¥å…·æ ˆ
category: æ¶æ„
tags: MachineLearning
keywords: llm chatgpt gpt bert

---

## ç®€ä»‹

* TOC
{:toc}

![](/public/upload/machine/llm_tool.png)

æ¨¡å‹ä¼šé¢„æµ‹ä¸‹ä¸€ä¸ªtokenã€‚åœ¨ç”Ÿæˆtokenæ—¶ï¼Œæ¨¡å‹é€šå¸¸ä¼šå°†decorder è¾“å‡ºçš„æ¯ä¸ªtokençš„æ¦‚ç‡å½’ä¸€åŒ–ï¼Œå¦‚æœåªé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„tokenï¼Œç”Ÿæˆçš„å“åº”ä¼šæ¯”è¾ƒä¿å®ˆå’Œé‡å¤ã€‚å› æ­¤chatgpté€šå¸¸ä½¿ç”¨temperature æ¥è¡¨ç¤ºå¼•å…¥ä¸€å®šç¨‹åº¦çš„éšæœºæ€§ï¼Œä»¥ä½¿ç”Ÿæˆçš„å“åº”æ›´åŠ ä¸°å¯Œå¤šæ ·ã€‚è¾ƒå¤§çš„temperature ä¼šæœ‰æ›´å¤šæœºä¼šé€‰æ‹©éæœ€é«˜æ¦‚ç‡tokenï¼Œä¹Ÿå¯èƒ½å¯¼è‡´ç”Ÿæˆçš„å“åº”è¿‡äºéšæœºå’Œä¸åˆç†ã€‚ 

## HuggingFace

Hugging Face è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„å¼€æºå¹³å°å’Œç¤¾åŒºï¼Œä¸»è¦æä¾›äº†ä»¥ä¸‹å‡ ä¸ªäº§å“å’ŒæœåŠ¡ï¼š
1. Hubï¼šè¿™æ˜¯ä¸€ä¸ªæœºå™¨å­¦ä¹ çš„ä¸­å¿ƒï¼Œè®©ä½ å¯ä»¥åˆ›å»ºã€å‘ç°å’Œåä½œMLé¡¹ç›®ã€‚å¯ä»¥ä»æ’è¡Œæ¦œå¼€å§‹ï¼Œäº†è§£ç¤¾åŒºä¸­è¡¨ç°è¾ƒå¥½çš„æ¨¡å‹ã€‚å¦‚æœä½ æ²¡æœ‰ GPUï¼Œä½ å¿…é¡»ä½¿ç”¨å°çš„æ¨¡å‹ã€‚è½¬åˆ°æ–‡ä»¶ç›®å½•å¹¶æŸ¥çœ‹ .bin æ–‡ä»¶çš„å¤§å°ã€‚æœ‰çš„é¡¹ç›®åœ¨å‹å·å¡ä¸­ä¹Ÿä¼šæåˆ°æ‰€éœ€çš„æœ€ä½è§„æ ¼ã€‚PSï¼šå°±åƒgithub åŒ…å«ä»£ç æ–‡ä»¶ä¸€æ ·ï¼Œè¿™é‡ŒåŒ…å«ä»£ç çš„æ¨¡å‹æ–‡ä»¶ï¼Œgit clone æ—¶è¦å®‰è£…Git LFSï¼ˆGit Large File Storageï¼‰
2. Transformersï¼šè¿™æ˜¯ä¸€ä¸ªè‡ªç„¶è¯­è¨€å¤„ç†çš„åº“ï¼Œæ”¯æŒå¤šç§ç¼–ç¨‹è¯­è¨€ï¼ˆå¦‚Pythonã€JavaScriptã€Swiftç­‰ï¼‰å’Œæ¡†æ¶ï¼ˆå¦‚PyTorchã€TensorFlowç­‰ï¼‰ï¼Œå¹¶æä¾›äº†ç®€å•æ˜“ç”¨çš„APIï¼Œè®©ä½ å¯ä»¥å¿«é€Ÿåœ°åŠ è½½ã€è®­ç»ƒå’Œéƒ¨ç½²æ¨¡å‹ã€‚å¸®æˆ‘ä»¬è·Ÿè¸ªæµè¡Œçš„æ–°æ¨¡å‹ï¼Œå¹¶ä¸”**æä¾›ç»Ÿä¸€çš„ä»£ç é£æ ¼æ¥ä½¿ç”¨BERTã€XLNetå’ŒGPTç­‰ç­‰å„ç§ä¸åŒçš„æ¨¡å‹**ã€‚åªæœ‰configurationï¼Œmodelså’Œtokenizerä¸‰ä¸ªä¸»è¦ç±»ï¼ŒåŸºäºä¸Šé¢çš„ä¸‰ä¸ªç±»ï¼Œæä¾›æ›´ä¸Šå±‚çš„pipelineå’ŒTrainer/TFTrainerï¼Œä»è€Œç”¨æ›´å°‘çš„ä»£ç å®ç°æ¨¡å‹çš„é¢„æµ‹å’Œå¾®è°ƒã€‚
    1. pipeline åœ¨åº•å±‚æ˜¯ç”± AutoModel å’Œ AutoTokenizer ç±»æ¥å®ç°çš„ã€‚AutoClassï¼ˆå³åƒ AutoModel å’Œ AutoTokenizer è¿™æ ·çš„é€šç”¨ç±»ï¼‰æ˜¯åŠ è½½æ¨¡å‹çš„å¿«æ·æ–¹å¼ï¼Œå®ƒå¯ä»¥ä»å…¶åç§°æˆ–è·¯å¾„ä¸­è‡ªåŠ¨æ£€ç´¢é¢„è®­ç»ƒæ¨¡å‹ã€‚
    2.  
    ```python
    from transformers import MODEL_NAME # å¯¼å…¥æ¨¡å‹
    model = MODEL_NAME.from_pretrained('MODEL_NAME') # å®ä¾‹åŒ–æ¨¡å‹ï¼Œå…¶ä¸­ MODEL_NAME æ˜¯æ¨¡å‹çš„åç§°æˆ–è·¯å¾„ã€‚
    inputs = xx             # å‡†å¤‡è¾“å…¥æ•°æ®ï¼Œè½¬æ¢ä¸ºæ¨¡å‹æ”¯æŒçš„æ ¼å¼ã€‚ï¼ˆå¦‚ tokenizer åçš„æ–‡æœ¬ã€å›¾åƒç­‰ï¼‰
    outputs = model(inputs) # è°ƒç”¨æ¨¡å‹å¹¶è·å¾—è¾“å‡º

    model.save_pretrained('PATH')   # å°†æ¨¡å‹ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„
    ```
    3. åœ¨Linuxä¸‹ï¼Œæ¨¡å‹é»˜è®¤ä¼šç¼“å­˜åˆ°`~/.cache/huggingface/transformers/`ã€‚æ‰€æœ‰çš„æ¨¡å‹éƒ½å¯ä»¥é€šè¿‡ç»Ÿä¸€çš„from_pretrained()å‡½æ•°æ¥å®ç°åŠ è½½ï¼Œtransformersä¼šå¤„ç†ä¸‹è½½ã€ç¼“å­˜å’Œå…¶å®ƒæ‰€æœ‰åŠ è½½æ¨¡å‹ç›¸å…³çš„ç»†èŠ‚ã€‚è€Œæ‰€æœ‰è¿™äº›æ¨¡å‹éƒ½ç»Ÿä¸€åœ¨Hugging Face Modelsç®¡ç†ã€‚
    4. æœ€åˆæˆ‘è®¤ä¸ºæ‚¨éœ€è¦ä¸ºæ¯ä¸ªæ¨¡å‹ç³»åˆ—ä½¿ç”¨ç‰¹å®šçš„Transformerså’ŒTokenizerï¼ˆä¾‹å¦‚ï¼Œå¦‚æœæ‚¨ä½¿ç”¨T5æ¨¡å‹ç³»åˆ—ï¼Œåˆ™å¯¹åº”T5Tokenizerå’Œ T5ForConditionalGenerationï¼‰ï¼Œå¯¹äºæ‰€æœ‰é¢„è®­ç»ƒæ¨¡å‹ï¼Œæ‚¨å¯ä»¥å£°æ˜ä¸€ä¸ªç®€å•çš„è¯­å¥ï¼š
        ```python
        from transformers import AutoTokenizer, AutoModelForCausalLM
        tokenizer = AutoTokenizer.from_pretrained("databricks/dolly-v2-3b")
        model = AutoModelForCausalLM.from_pretrained("databricks/dolly-v2-3b")
        ```
3. Inference APIï¼šè¿™æ˜¯ä¸€ä¸ªæœåŠ¡ï¼Œè®©ä½ å¯ä»¥ç›´æ¥ä»Hugging Faceçš„åŸºç¡€è®¾æ–½ä¸Šè¿è¡Œå¤§è§„æ¨¡çš„NLPæ¨¡å‹ï¼Œå¹¶åœ¨æ¯«ç§’çº§åˆ«å¾—åˆ°å“åº”ã€‚
4. Datasetsï¼šè¿™æ˜¯ä¸€ä¸ªæ•°æ®é›†çš„åº“ï¼Œè®©ä½ å¯ä»¥è·å–ã€åŠ è½½å’Œå¤„ç†è¶…è¿‡1400ä¸ªå…¬å¼€å¯ç”¨çš„æ•°æ®é›†ã€‚Datasetsæ”¯æŒå¤šç§æ•°æ®ç±»å‹ï¼ˆå¦‚æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ç­‰ï¼‰å’Œæ ¼å¼ï¼ˆå¦‚JSONã€CSVç­‰ï¼‰ï¼Œå¹¶æä¾›äº†é«˜æ•ˆä¸”ç»Ÿä¸€çš„APIï¼Œè®©ä½ å¯ä»¥å¿«é€Ÿåœ°åŠ è½½ã€ç¼“å­˜å’Œè½¬æ¢æ•°æ®ã€‚PSï¼š ä¸€å¼€å§‹çœ‹ä»£ç çš„æ—¶å€™ï¼Œæ€»ä»¥ä¸ºæ˜¯pytorch dataset

ä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼ˆä¸€èˆ¬æœ‰å‡ ä¸ªGï¼‰æœ‰å¤šç§æ–¹å¼
1. åˆ°huggingface å®˜ç½‘æ‰‹åŠ¨é€šè¿‡æ–‡ä»¶é“¾æ¥ä¸‹è½½
    1. å¯ä»¥ä½¿ç”¨ `export HF_ENDPOINT=https://hf-mirror.com` å›½å†…åŠ é€Ÿä¸€ä¸‹
2. Git LFS ä¸‹è½½ã€‚`git clone https://huggingface.co/THUDM/chatglm-6b` PSï¼šæ³¨æ„ä¸æ˜¯github åœ°å€
3. Hugging Face Hub ä¸‹è½½ã€‚
    ```python
    from huggingface_hub import snapshot_download
    snapshot_download(repo_id="bert-base-chinese")
    ```
4. `huggingface-cli  download  --resume-download --cache-dir ./cache/ --local-dir ./starcoder  bigcode/starcoder`
4. ä½¿ç”¨transformers åº“ï¼Œä½†è¿™ç§æ–¹å¼é€Ÿåº¦æ…¢ï¼Œä¸”ç»å¸¸ä¸­æ–­ã€‚
    ```
    from transformers import AutoTokenizer, AutoModel
    tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True,mirror="tuna")
    model = AutoModel.from_pretrained("THUDM/chatglm2-6b",trust_remote_code=True, mirror="tuna")
    ```
    ä¸‹è½½åæ–‡ä»¶ä¼šå‡ºç°åœ¨ `~/.cache`ç›®å½•ä¸‹
    ```
    ~/.cache
        /torch/sentence_transformers
            /moka-ai_m3e-base
                /config.json
                /pytorch_model.bin
        /huggingface/hub
            /models--THUDM--chatglm2-6b
                /blobs
                /snapshots
                    /b1502f4f75c71499a3d566b14463edd62620ce9f   # æŸä¸ªç‰ˆæœ¬çš„æ–‡ä»¶å†…å®¹
                        /config.json
                        /pytorch_model.xx.bin
    ```

### LangChainä½¿ç”¨ HuggingFace æ¨¡å‹

å¦‚æœåªæ˜¯è°ƒç”¨æ¨¡å‹æœåŠ¡ï¼Œé‚£ç›´æ¥ä½¿ç”¨HuggingFaceåº“å³å¯ï¼Œä½†è‹¥æ˜¯æƒ³å’ŒLangChainç»“åˆï¼Œè¿˜æ˜¯è¦é€‚é…æˆLangChain.LLM æ‰èƒ½ä¸LangChain å…¶å®ƒæ¨¡å—ååŒï¼ŒHuggingFace æä¾›äº†LangChain.LLM çš„å®ç°ã€‚
1. ä½¿ç”¨åœ¨çº¿æ¨¡å‹
    ```python
    import os
    from langchain import PromptTemplate, HuggingFaceHub, LLMChain
    os.environ['HUGGINGFACEHUB_API_TOKEN'] = 'xx'
    template = """Question: {question}
    Answer: Let's think step by step."""
    prompt = PromptTemplate(template=template, input_variables=["question"])
    llm = HuggingFaceHub(repo_id="google/flan-t5-xl", model_kwargs={"temperature":0, "max_length":64})
    llm_chain = LLMChain(prompt=prompt, llm=llm)
    question = "What NFL team won the Super Bowl in the year Justin Beiber was born?"
    print(llm_chain.run(question))
    ```
2. å°† Hugging Face æ¨¡å‹ç›´æ¥æ‹‰åˆ°æœ¬åœ°ä½¿ç”¨ï¼ˆæœ‰äº›æ¨¡å‹æ— æ³•åœ¨ Hugging Face è¿è¡Œï¼‰
    ```python
    from langchain import PromptTemplate, LLMChain
    from langchain.llms import HuggingFacePipeline
    from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM
    model_id = 'google/flan-t5-large'
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_id)
    pipe = pipeline("text2text-generation",model=model,tokenizer=tokenizer, max_length=100)
    local_llm = HuggingFacePipeline(pipeline=pipe)
    print(local_llm('What is the capital of France? '))
    
    template = """Question: {question} Answer: Let's think step by step."""
    prompt = PromptTemplate(template=template, input_variables=["question"])
    llm_chain = LLMChain(prompt=prompt, llm=local_llm)
    question = "What is the capital of England?"
    print(llm_chain.run(question))
    ```

## æ¨¡å‹æœåŠ¡

ä½¿ç”¨å¤§æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬åœ¨huggingfaceæˆ–modelscope çœ‹åˆ°çš„ä»£ç ç±»ä¼¼ä¸‹é¢ï¼Œå¾ˆæ˜æ˜¾ä¸èƒ½ç›´æ¥å‘ç”¨æˆ·æä¾›æœåŠ¡ã€‚ 
```python
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True)
model = AutoModel.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True).half().cuda()
model = model.eval()
response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
print(response)
ä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚
response, history = model.chat(tokenizer, "æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ", history=history)
print(response)
æ™šä¸Šç¡ä¸ç€å¯èƒ½ä¼šè®©ä½ æ„Ÿåˆ°ç„¦è™‘æˆ–ä¸èˆ’æœ,ä½†ä»¥ä¸‹æ˜¯ä¸€äº›å¯ä»¥å¸®åŠ©ä½ å…¥ç¡çš„æ–¹æ³•:...
```

ä¸€èˆ¬æœ‰å‡ ä¸ªéœ€æ±‚
1. ç»Ÿä¸€apiï¼Œè¿™æ ·åˆ‡æ¢æ¨¡å‹æ—¶ä¸Šæ¸¸åº”ç”¨æ— æ„Ÿï¼Œæœ€å¥½æ˜¯ OpenAI-compatibleï¼Œå…¶api è¢«ä¸»è¦ä¸Šæ¸¸æ¡†æ¶ï¼ˆæ¯”å¦‚langchainï¼‰å…¼å®¹
    1. æ”¯æŒæµå¼è¾“å‡ºå’Œæ™®é€šè¾“å‡º
2. æ”¯æŒå¤šå®ä¾‹ï¼Œè¿›è€Œæ”¯æŒç°åº¦å‘å¸ƒç­‰
3. æ”¯æŒé€šç”¨çš„åŠ é€Ÿåº“æ¯”å¦‚vllmç­‰

### ç®€å•å°è£…

[ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) åœ¨github æœ‰ä¸€ä¸ªä»“åº“ï¼Œä¸€èˆ¬åŒ…å«
1. æ¨¡å‹ä»‹ç» README.md
2. æ¨¡å‹çš„å¯¹å¤–æ¥å£ api.py/cli_demo.py/web_demo.pyã€‚ è‡ªå·±ä½¿ç”¨ fastapi åŸºäºpythonåº“ç›´æ¥å¯¹å¤–æä¾›RESTful APIs.

ä»¥api.py ä¸ºä¾‹
```python
from fastapi import FastAPI, Request
from transformers import AutoTokenizer, AutoModel
import uvicorn, json, datetime
import torch

app = FastAPI()

@app.post("/")
async def create_item(request: Request):
    global model, tokenizer
    json_post_raw = await request.json()
    json_post = json.dumps(json_post_raw)
    json_post_list = json.loads(json_post)
    prompt = json_post_list.get('prompt')
    history = json_post_list.get('history')
    max_length = json_post_list.get('max_length')
    top_p = json_post_list.get('top_p')
    temperature = json_post_list.get('temperature')
    response, history = model.chat(tokenizer,prompt, history=history,
                                   max_length=max_length if max_length else 2048,
                                   top_p=top_p if top_p else 0.7,
                                   temperature=temperature if temperature else 0.95)
    now = datetime.datetime.now()
    time = now.strftime("%Y-%m-%d %H:%M:%S")
    answer = {"response": response,"history": history,"status": 200,"time": time}
    log = "[" + time + "] " + '", prompt:"' + prompt + '", response:"' + repr(response) + '"'
    print(log)
    torch_gc()
    return answer

if __name__ == '__main__':
    tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)
    model = AutoModel.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True).half().cuda()
    model.eval()
    uvicorn.run(app, host='0.0.0.0', port=8000, workers=1)
```

### FastChat

[ä¸€æ–‡å…¥é—¨æœ€çƒ­çš„LLMåº”ç”¨å¼€å‘æ¡†æ¶LangChain](https://mp.weixin.qq.com/s/bYzNNL3F0998Do2Jl0PQtw)

[FastChat](https://github.com/lm-sys/FastChat)æ˜¯ä¸€ä¸ªç”¨äºè®­ç»ƒã€æœåŠ¡å’Œè¯„ä¼°åŸºäºèŠå¤©æœºå™¨äººçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„å¼€æ”¾å¹³å°ã€‚The core features include:
1. The training and evaluation code for state-of-the-art models (e.g., Vicuna).
2. A distributed multi-model serving system with web UI and OpenAI-compatible RESTful APIs.


```sh
# å‘½ä»¤è¡Œæ–¹å¼ä¸llm äº¤äº’
python3 -m fastchat.serve.cli --model-path lmsys/vicuna-7b-v1.3
# webuiæ–¹å¼ä¸llmäº¤äº’ï¼Œæ­¤æ—¶éœ€å¯åŠ¨3ä¸ªç»„ä»¶ web servers ==> controller ==> model workers
python3 -m fastchat.serve.controller
python3 -m fastchat.serve.model_worker --model-path lmsys/vicuna-7b-v1.3
python3 -m fastchat.serve.gradio_web_server
# æä¾›OpenAI-compatible RESTful APIs  openai_api_server ==> controller ==> model workers
python3 -m fastchat.serve.controller
python3 -m fastchat.serve.model_worker --model-path lmsys/vicuna-7b-v1.3
python3 -m fastchat.serve.openai_api_server --host localhost --port 8000
```

### FastChatæºç åˆ†æ

ä½¿ç”¨ModelWorker åŠ è½½model æä¾›http æ¥å£ 

```python
app = FastAPI()
@app.post("/worker_generate")
async def api_generate(request: Request):
    params = await request.json()
    await acquire_worker_semaphore()
    output = worker.generate_gate(params)
    release_worker_semaphore()
    return JSONResponse(output)
if __name__ == "__main__":
    ...
    worker = ModelWorker(...,args.model_path,)
    uvicorn.run(app, host=args.host, port=args.port, log_level="info")
```
ModelWorkerå®ç°
```python
BaseModelWorker
     init_heart_beat
         # å°†modelWorker idæ³¨å†Œåˆ°controllerï¼Œå¹¶ä¿æŒå¿ƒè·³ã€‚å‡é€šè¿‡httpæ¥å£

# åŠ è½½æ¨¡å‹ï¼Œè°ƒç”¨æ¨¡å‹ï¼ˆåº•å±‚éƒ½æ˜¯è°ƒç”¨æµå¼æ¥å£ï¼‰
ModelWorker
     def __init__():
          self.model, self.tokenizer = load_model(model_path, device=device,...)
            # load_model å¯¹åº”ä¸€ä¸ªä¸“é—¨çš„ ModelAdapter æŠ½è±¡ï¼Œç”¨æ¥é€‚é…æ¨¡å‹çš„åŠ è½½
            adapter = get_model_adapter(model_path)
            model, tokenizer = adapter.load_model(model_path, kwargs)
     generate_stream_gate(self, params) 
     generate_gate(self, params)    # æ ¹æ®å‚æ•°è¿”å›è¾“å‡ºï¼Œè°ƒç”¨generate_stream_gate
        for x in self.generate_stream_gate(params):
            pass
        return json.loads(x[:-1].decode())
```
api => ModelWorker.generate_gate ==> ModelWorker.generate_stream_gate ==> ModelWorker.model.stream_generate
```python
generate_stream_gate
    get_generate_stream_function(model: torch.nn.Module, model_path: str)
       # æ ¹æ®æ¨¡å‹ä¸åŒé€‰æ‹©å¯¹åº”çš„å‡½æ•° 
       generate_stream_chatglm
            prompt = params["prompt"]
            inputs = tokenizer([prompt], return_tensors="pt").to(model.device)
            for total_ids in model.stream_generate(**inputs, **gen_kwargs):
                  response = tokenizer.decode(output_ids)
                  response = process_response(response)
```


### FastChat, How to support a new model?

1. FastChat uses the Conversation class to handle prompt templates and BaseModelAdapter class to handle model loading.
2. Implement a conversation template for the new model at `fastchat/conversation.py`. You can follow existing examples and use register_conv_template to add a new one. Please also add a link to the official reference code if possible. PSï¼š æ¯•ç«Ÿfastcaht æœåŠ¡chat åœºæ™¯å˜›ï¼Œå¯¹è¯è¯·æ±‚ä¼ å…¥çš„æ—¶å€™ ä¸€èˆ¬æ˜¯ `prompt = "\n###user:å¤©ä¸ºä»€ä¹ˆè¿™ä¹ˆè“ï¼Ÿ\n###"`ï¼Œè¦æŠŠè¿™ä¸ªè¿˜åŸä¸º `history = [{"role": "user", "content": "å¤©ä¸ºä»€ä¹ˆè¿™ä¹ˆè“ï¼Ÿ"}]`ï¼Œä¸åŒçš„æ¨¡å‹ å¯¹roleçš„ç§°å‘¼ä¸åŒã€‚
3. Implement a model adapter for the new model at `fastchat/model/model_adapter.py`. You can follow existing examples and use register_model_adapter to add a new one. PSï¼šä¸åŒçš„æ¨¡å‹åŠ è½½æ—¶æœ‰ä¸€äº›ç‰¹å®šçš„å‚æ•°ï¼Œæ¯”å¦‚ chatglm çš„trust_remote_code å‚æ•°ï¼Œ`model = AutoModel.from_pretrained(model_path, trust_remote_code=True, **from_pretrained_kwargs)`
4. ModelWorker ä¸»è¦é€»è¾‘æ˜¯æ‰§è¡Œ `generate_stream(model,tokenizer,params)` ï¼Œå¾ˆå¸¸è§„çš„ `input_ids = tokenizer(prompt); output_ids = model(input_ids,xx)`ã€‚ å¦‚æœæ¨¡å‹çš„generate é€»è¾‘æœ‰ä¸€äº›ç‰¹åˆ«çš„å¤„ç†ï¼Œåˆ™éœ€è¦è‡ªå®šä¹‰generate_stream_xxï¼Œå¹¶åŠ å…¥get_generate_stream_function é€»è¾‘ï¼ˆæ ¹æ®æ¨¡å‹åç­‰ è·¯ç”±åˆ°ä¸åŒçš„generate_stream_xxï¼‰
5. (Optional) add the model name to the "Supported models" section above and add more information in `fastchat/model/model_registry.py.`

å¦‚ä½•ç†è§£FastChat éƒ½å¹²äº†ä»€ä¹ˆï¼Ÿæœ¬è´¨æ˜¯å¯¹ä¸‹é¢çš„ åŸå§‹çš„å¤§æ¨¡å‹æ¨ç†ä»£ç è¿›è¡ŒæŠ½è±¡ï¼ˆæ¨¡å‹åŠ è½½ã€æ¨¡å‹æ¨ç†=tokenizer+modelï¼‰å’Œå°è£…

```python
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True)
model = AutoModel.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True).half().cuda()
model = model.eval()
response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
print(response)
ä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚
response, history = model.chat(tokenizer, "æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ", history=history)
print(response)
æ™šä¸Šç¡ä¸ç€å¯èƒ½ä¼šè®©ä½ æ„Ÿåˆ°ç„¦è™‘æˆ–ä¸èˆ’æœ,ä½†ä»¥ä¸‹æ˜¯ä¸€äº›å¯ä»¥å¸®åŠ©ä½ å…¥ç¡çš„æ–¹æ³•:...
```

## LangChain

LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model, but will also be:
1. Data-aware: connect a language model to other sources of data
2. Agentic: allow a language model to interact with its environment

å¦‚æœæ˜¯ä¸€ä¸ªç®€å•çš„åº”ç”¨ï¼Œæ¯”å¦‚å†™è¯—æœºå™¨äººï¼Œæˆ–è€…æœ‰ token æ•°é‡é™åˆ¶çš„æ€»ç»“å™¨ï¼Œå¼€å‘è€…å®Œå…¨å¯ä»¥åªä¾èµ– Promptã€‚å½“ä¸€ä¸ªåº”ç”¨ç¨å¾®å¤æ‚ç‚¹ï¼Œå•çº¯ä¾èµ– Prompting å·²ç»ä¸å¤Ÿäº†ï¼Œè¿™æ—¶å€™éœ€è¦**å°† LLM ä¸å…¶ä»–ä¿¡æ¯æºæˆ–è€… LLM ç»™è¿æ¥èµ·æ¥**ï¼ˆPSï¼šé‡ç‚¹ä¸æ˜¯æ¨¡å‹æœåŠ¡æœ¬èº«ï¼‰ï¼Œæ¯”å¦‚è°ƒç”¨æœç´¢ API æˆ–è€…æ˜¯å¤–éƒ¨çš„æ•°æ®åº“ç­‰ã€‚LangChain çš„ä¸»è¦ä»·å€¼åœ¨äºï¼š
1. ç»„ä»¶ï¼š**ç”¨äºå¤„ç†è¯­è¨€æ¨¡å‹çš„æŠ½è±¡ï¼Œä»¥åŠæ¯ä¸ªæŠ½è±¡çš„ä¸€ç³»åˆ—å®ç°**ã€‚æ— è®ºæ‚¨æ˜¯å¦ä½¿ç”¨ LangChain æ¡†æ¶çš„å…¶ä»–éƒ¨åˆ†ï¼Œç»„ä»¶éƒ½æ˜¯æ¨¡å—åŒ–ä¸”æ˜“äºä½¿ç”¨çš„ã€‚PSï¼š æ—¢å¯ä»¥å•ç‹¬ç”¨ï¼Œä¹Ÿå¯ä»¥ç»„åˆç”¨ã€‚
2. ç°æˆçš„é“¾å¼ç»„è£…ï¼šç”¨äºå®Œæˆç‰¹å®šé«˜çº§ä»»åŠ¡çš„ç»“æ„åŒ–ç»„ä»¶ç»„è£…ã€‚ä¸€äº›ä¾‹å­åŒ…æ‹¬ï¼š
    1. å°† LLM ä¸æç¤ºæ¨¡æ¿ç»“åˆã€‚
    2. é€šè¿‡å°†ç¬¬ä¸€ä¸ª LLM çš„è¾“å‡ºä½œä¸ºç¬¬äºŒä¸ª LLM çš„è¾“å…¥ï¼ŒæŒ‰é¡ºåºç»„åˆå¤šä¸ª LLMã€‚
    3. å°† LLM ä¸å¤–éƒ¨æ•°æ®ç»“åˆï¼Œä¾‹å¦‚ç”¨äºé—®ç­”ç³»ç»Ÿã€‚
    4. å°† LLM ä¸é•¿æœŸè®°å¿†ç»“åˆï¼Œä¾‹å¦‚ç”¨äºèŠå¤©å†å²è®°å½•ã€‚

### åŸºç¡€åŠŸèƒ½

[LangChain ä¸­æ–‡å…¥é—¨æ•™ç¨‹](https://github.com/liaokongVFX/LangChain-Chinese-Getting-Started-Guide)ï¼š
1. Modelï¼Œä¸»è¦æ¶µç›–å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä¸ºå„ç§ä¸åŒåŸºç¡€æ¨¡å‹**æä¾›ç»Ÿä¸€æ¥å£**ï¼Œç„¶åæˆ‘ä»¬å¯ä»¥è‡ªç”±çš„åˆ‡æ¢ä¸åŒçš„æ¨¡å‹ã€‚ç›¸å…³ä»£ç è¾ƒå°‘ï¼Œå¤§éƒ¨åˆ†ä¸»è¦æ˜¯è°ƒç”¨å¤–éƒ¨èµ„æºï¼Œå¦‚ OPENAI æˆ–è€… Huggingface ç­‰æ¨¡å‹/APIã€‚
    1. æ™®é€šLLMï¼šæ¥æ”¶æ–‡æœ¬å­—ç¬¦ä¸²ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›æ–‡æœ¬å­—ç¬¦ä¸²ä½œä¸ºè¾“å‡º
    2. èŠå¤©æ¨¡å‹ï¼šå°†èŠå¤©æ¶ˆæ¯åˆ—è¡¨ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›ä¸€ä¸ªèŠå¤©æ¶ˆæ¯ã€‚æ”¯æŒæµæ¨¡å¼ï¼ˆå°±æ˜¯ä¸€ä¸ªå­—ä¸€ä¸ªå­—çš„è¿”å›ï¼Œç±»ä¼¼æ‰“å­—æ•ˆæœï¼‰ã€‚
2. Promptï¼Œæ”¯æŒå„ç§è‡ªå®šä¹‰æ¨¡æ¿
3. æ‹¥æœ‰å¤§é‡çš„æ–‡æ¡£åŠ è½½å™¨ï¼Œä»æŒ‡å®šæºè¿›è¡ŒåŠ è½½æ•°æ®çš„ï¼Œæ¯”å¦‚ Emailã€Markdownã€PDFã€Youtube ...å½“ä½¿ç”¨loaderåŠ è½½å™¨è¯»å–åˆ°æ•°æ®æºåï¼Œæ•°æ®æºéœ€è¦è½¬æ¢æˆ Document å¯¹è±¡åï¼Œåç»­æ‰èƒ½è¿›è¡Œä½¿ç”¨ã€‚
4. å¯¹ç´¢å¼•çš„æ”¯æŒã€‚å¯¹ç”¨æˆ·ç§åŸŸæ–‡æœ¬ã€å›¾ç‰‡ã€PDFç­‰å„ç±»æ–‡æ¡£è¿›è¡Œå­˜å‚¨å’Œæ£€ç´¢ã€‚ä¸ºäº†ç´¢å¼•ï¼Œä¾¿ä¸å¾—ä¸ç‰µæ¶‰ä»¥ä¸‹è¿™äº›èƒ½åŠ›
    1. åœ¨LangChainä¸­ï¼Œæ‰€æœ‰çš„æ•°æ®æºéƒ½å¯ä»¥è®¤ä¸ºæ˜¯Documentï¼Œä»»ä½•çš„æ•°æ®åº“ã€ç½‘ç»œã€å†…å­˜ç­‰ç­‰éƒ½å¯ä»¥çœ‹æˆæ˜¯ä¸€ä¸ªDocstoreã€‚
    1. æ–‡æ¡£åˆ†å‰²å™¨/Text Splittersï¼Œä¸ºä»€ä¹ˆéœ€è¦åˆ†å‰²æ–‡æœ¬ï¼Ÿå› ä¸ºæˆ‘ä»¬æ¯æ¬¡ä¸ç®¡æ˜¯åšæŠŠæ–‡æœ¬å½“ä½œ prompt å‘ç»™ openai api ï¼Œè¿˜æ˜¯è¿˜æ˜¯ä½¿ç”¨ openai api embedding åŠŸèƒ½éƒ½æ˜¯æœ‰å­—ç¬¦é™åˆ¶çš„ã€‚æ¯”å¦‚æˆ‘ä»¬å°†ä¸€ä»½300é¡µçš„ pdf å‘ç»™ openai apiï¼Œè®©å®ƒè¿›è¡Œæ€»ç»“ï¼Œå®ƒè‚¯å®šä¼šæŠ¥è¶…è¿‡æœ€å¤§ Token é”™ã€‚æ‰€ä»¥è¿™é‡Œå°±éœ€è¦ä½¿ç”¨æ–‡æœ¬åˆ†å‰²å™¨å»åˆ†å‰²æˆ‘ä»¬ loader è¿›æ¥çš„ Documentã€‚
    2. å‘é‡åŒ–ï¼Œæ•°æ®ç›¸å…³æ€§æœç´¢å…¶å®æ˜¯å‘é‡è¿ç®—ã€‚ä»¥ï¼Œä¸ç®¡æˆ‘ä»¬æ˜¯ä½¿ç”¨ openai api embedding åŠŸèƒ½è¿˜æ˜¯ç›´æ¥é€šè¿‡å‘é‡æ•°æ®åº“ç›´æ¥æŸ¥è¯¢ï¼Œéƒ½éœ€è¦å°†æˆ‘ä»¬çš„åŠ è½½è¿›æ¥çš„æ•°æ® Document è¿›è¡Œå‘é‡åŒ–ï¼Œæ‰èƒ½è¿›è¡Œå‘é‡è¿ç®—æœç´¢ã€‚
    3. å¯¹æ¥å‘é‡å­˜å‚¨ä¸æœç´¢ï¼Œå‘é‡åŒ–å­˜å‚¨æ¥å£VectorStoreï¼Œ æ¯”å¦‚ Chromaã€Pineconeã€Qdrand
5. Chainsï¼Œç›¸å½“äº pipelineï¼ŒåŒ…æ‹¬ä¸€ç³»åˆ—å¯¹å„ç§ç»„ä»¶çš„è°ƒç”¨ï¼Œ**æ¯ä¸€ä¸ªä»Promptåˆ°Answerçš„è¿‡ç¨‹ï¼Œéƒ½è¢«æ ‡å‡†åŒ–ä¸ºä¸åŒç±»å‹çš„LLMChain**ã€‚Chainå¯ä»¥ç›¸äº’åµŒå¥—å¹¶ä¸²è¡Œæ‰§è¡Œï¼Œé€šè¿‡è¿™ä¸€å±‚ï¼Œè®©LLMçš„èƒ½åŠ›é“¾æ¥åˆ°å„è¡Œå„ä¸šã€‚ å†…åµŒäº†memoryã€cacheã€callbackç­‰ç»„ä»¶ã€‚
    1. LLMChain
    2. å„ç§å·¥å…·Chain
    3. LangChainHub

å¦‚æœè¯´Promptså’ŒAnsweræ˜¯æ°´å’ŒåœŸå£¤ï¼Œæœ‰äº†CoTçš„ç†è®ºæŒ‡å¯¼åŠ ä¸ŠChainsçš„æ¶æ„å°±å¥½åƒå¼€æ¸ é€ æ²³ï¼Œå°†åŸæœ¬ç”±AIéšæ„å‘æ•£è€Œä¸å¯æ§çš„æ¨ç†è¿‡ç¨‹å›ºåŒ–åˆ°äº†Chainå’ŒChainçš„è¿æ¥ä¸Šï¼Œè®©ä¸€åˆ‡éƒ½å›å½’åˆ°æˆ‘ä»¬è®¤çŸ¥ä¸­æµç¨‹åº”æœ‰çš„æ ·å­ã€‚

![](/public/upload/machine/langchain_chains.jpg)

Modelï¼Œä¸»è¦æ¶µç›–å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ 
```python
from langchain.llms import OpenAI
#Here we are using text-ada-001 but you can change it 
llm = OpenAI(model_name="text-ada-001", n=2, best_of=2)
#Ask anything
llm("Tell me a joke")
# è¾“å‡º
'\n\nWhy did the chicken cross the road?\n\nTo get to the other side.'
```
Promptï¼Œæˆ‘ä»¬å¯ä»¥æä¾›æç¤ºæ¨¡æ¿ä½œä¸ºè¾“å…¥ã€‚æ¨¡æ¿æŒ‡çš„æ˜¯æˆ‘ä»¬å¸Œæœ›è·å¾—ç­”æ¡ˆçš„å…·ä½“æ ¼å¼æˆ–è“å›¾ã€‚ä½¿ç”¨æç¤ºè¯æ¨¡æ¿çš„æ„ä¹‰åœ¨äºï¼Œå¾ˆå¤šæç¤ºè¯éƒ½ä¼šå¾ˆé•¿ï¼Œé‡Œé¢ä¼šæœ‰å¾ˆå¤šç»†èŠ‚ï¼Œä½†**åªæœ‰ä¸€éƒ¨åˆ†å†…å®¹æ˜¯åŠ¨æ€çš„**ï¼Œæ‰€ä»¥åº”è¯¥å°½å¯èƒ½åˆ©ç”¨æ¨¡æ¿æ¥å¤ç”¨æç¤ºè¯ï¼Œæ¨¡æ¿ä¸­å¯ä»¥ç”¨ {} æ¥å¼•ç”¨å˜é‡ã€‚LangChain æä¾›äº†é¢„å…ˆè®¾è®¡å¥½çš„æç¤ºæ¨¡æ¿ï¼Œå¯ä»¥ç”¨äºç”Ÿæˆä¸åŒç±»å‹ä»»åŠ¡çš„æç¤ºã€‚ç„¶è€Œï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œé¢„è®¾çš„æ¨¡æ¿å¯èƒ½æ— æ³•æ»¡è¶³ä½ çš„éœ€æ±‚ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è‡ªå®šä¹‰çš„æç¤ºæ¨¡æ¿ã€‚PSï¼šä¸Šå±‚ç”¨æˆ·åªéœ€è¦è¾“å…¥å…³é”®è¯å³å¯ã€‚

```python
from langchain import PromptTemplate
# This template will act as a blue print for prompt
template = """
I want you to act as a naming consultant for new companies.
What is a good name for a company that makes {product}?
"""
prompt = PromptTemplate(input_variables=["product"], template=template,)
prompt.format(product="colorful socks")
# -> I want you to act as a naming consultant for new companies.
# -> What is a good name for a company that makes colorful socks?
```

||Completionsæ¥å£|Chat æ¥å£|
|---|---|---|
|æ¨¡å‹|OpenAI|ChatOpenAI|
|| llm.predict(xx)|chat.predict_messages([SystemMessage(context=xx),HumanMessage(content=xx),AIMessage(content=xx)])|
|PromptTemplate|prompt = PromptTemplate.from_template(xx)<br>text = prompt.format(xx)|chat_prompt = ChatPromptTemplate.from_template([SystemMessagePromptTemplate.from_template(xx),HumanMessagePromptTemplate.from_template(xx)])<br>messages = chat_prompt.format_messages(xx)|

LLMChain ï¼Œé€šè¿‡é“¾å¼è°ƒç”¨çš„æ–¹å¼ï¼ŒæŠŠä¸€ä¸ªéœ€è¦è¯¢é—® AI å¤šè½®æ‰èƒ½è§£å†³çš„é—®é¢˜å°è£…èµ·æ¥ï¼Œåœ¨è¿™ä¸ªé“¾å¼åºåˆ—ä¸­ï¼Œæ¯ä¸ªé“¾å¼éƒ½æœ‰ä¸€ä¸ªè¾“å…¥å’Œä¸€ä¸ªè¾“å‡ºï¼Œ**ä¸€ä¸ªæ­¥éª¤çš„è¾“å‡ºä½œä¸ºä¸‹ä¸€ä¸ªæ­¥éª¤çš„è¾“å…¥**ã€‚æŠŠä¸€ä¸ªé€šè¿‡è‡ªç„¶è¯­è¨€å¤šè½®è°ƒç”¨æ‰èƒ½è§£å†³çš„é—®é¢˜ï¼Œå˜æˆäº†ä¸€ä¸ªå‡½æ•°è°ƒç”¨ã€‚æ‰€æœ‰ Chain å¯¹è±¡å…±äº«çš„callå’Œ run æ–¹æ³•å¤–

```python
chain = LLMChain(llm = llm, prompt = prompt)
chain("what is rice?")
```

æƒ³è¦é€šè¿‡å¤§è¯­è¨€æ¨¡å‹ï¼Œå®Œæˆä¸€ä¸ªå¤æ‚çš„ä»»åŠ¡ï¼Œå¾€å¾€éœ€è¦æˆ‘ä»¬å¤šæ¬¡å‘ AI æé—®ï¼Œå¹¶ä¸”å‰é¢æé—®çš„ç­”æ¡ˆï¼Œå¯èƒ½æ˜¯åé¢é—®é¢˜è¾“å…¥çš„ä¸€éƒ¨åˆ†ã€‚LangChain é€šè¿‡å°†å¤šä¸ª LLMChain ç»„åˆæˆä¸€ä¸ª SequantialChain å¹¶é¡ºåºæ‰§è¡Œï¼Œå¤§å¤§ç®€åŒ–äº†è¿™ç±»ä»»åŠ¡çš„å¼€å‘å·¥ä½œã€‚Langchain çš„é“¾å¼è°ƒç”¨å¹¶ä¸å±€é™äºä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹çš„æ¥å£ã€‚
1. LLMMathChain èƒ½å¤Ÿé€šè¿‡ Python è§£é‡Šå™¨å˜æˆä¸€ä¸ªè®¡ç®—å™¨ï¼Œè®© AI èƒ½å¤Ÿå‡†ç¡®åœ°è¿›è¡Œæ•°å­¦è¿ç®—ã€‚
2. é€šè¿‡ RequestsChainï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥è°ƒç”¨å¤–éƒ¨ APIï¼Œç„¶åå†è®© AI ä»è¿”å›çš„ç»“æœé‡Œæå–æˆ‘ä»¬å…³å¿ƒçš„å†…å®¹ã€‚
3. TransformChain èƒ½å¤Ÿè®©æˆ‘ä»¬æ ¹æ®è‡ªå·±çš„è¦æ±‚å¯¹æ•°æ®è¿›è¡Œå¤„ç†å’Œè½¬åŒ–ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠ AI è¿”å›çš„è‡ªç„¶è¯­è¨€çš„ç»“æœè¿›ä¸€æ­¥è½¬æ¢æˆç»“æ„åŒ–çš„æ•°æ®ï¼Œæ–¹ä¾¿å…¶ä»–ç¨‹åºå»å¤„ç†ã€‚
4. VectorDBQA èƒ½å¤Ÿå®Œæˆå’Œ llama-index ç›¸ä¼¼çš„äº‹æƒ…ï¼Œåªè¦é¢„å…ˆåšå¥½å†…éƒ¨æ•°æ®èµ„æ–™çš„ Embedding å’Œç´¢å¼•ï¼Œé€šè¿‡å¯¹ LLMChain è¿›è¡Œä¸€æ¬¡è°ƒç”¨ï¼Œæˆ‘ä»¬å°±å¯ä»¥ç›´æ¥è·å–å›ç­”çš„ç»“æœã€‚
5. Langchain é‡Œæœ‰ SQLDatabaseChain å¯ä»¥ç›´æ¥è®©æˆ‘ä»¬å†™éœ€æ±‚è®¿é—®æ•°æ®åº“ã€‚
è¿™äº›èƒ½åŠ›å¤§å¤§å¢å¼ºäº† AI çš„å®ç”¨æ€§ï¼Œè§£å†³äº†å‡ ä¸ªä¹‹å‰å¤§è¯­è¨€æ¨¡å‹å¤„ç†å¾—ä¸å¥½çš„é—®é¢˜ï¼ŒåŒ…æ‹¬æ•°å­¦è®¡ç®—èƒ½åŠ›ã€å®æ—¶æ•°æ®èƒ½åŠ›ã€å’Œç°æœ‰ç¨‹åºç»“åˆçš„èƒ½åŠ›ï¼Œä»¥åŠæœç´¢å±äºè‡ªå·±çš„èµ„æ–™åº“çš„èƒ½åŠ›ã€‚ä½ å®Œå…¨å¯ä»¥å®šä¹‰è‡ªå·±éœ€è¦çš„ LLMChainï¼Œé€šè¿‡ç¨‹åºæ¥å®Œæˆå„ç§ä»»åŠ¡ï¼Œç„¶ååˆç†åœ°ç»„åˆä¸åŒç±»å‹çš„ LLMChain å¯¹è±¡ï¼Œæ¥å®ç°è¿ ChatGPT éƒ½åšä¸åˆ°çš„äº‹æƒ…ã€‚


### Memory

åœ¨ LangChain ä¸­ï¼Œé“¾å¼å’Œä»£ç†**é»˜è®¤ä»¥æ— çŠ¶æ€æ¨¡å¼è¿è¡Œï¼Œå³å®ƒä»¬ç‹¬ç«‹å¤„ç†æ¯ä¸ªä¼ å…¥çš„æŸ¥è¯¢**ã€‚ç„¶è€Œï¼Œåœ¨æŸäº›åº”ç”¨ç¨‹åºï¼ˆå¦‚èŠå¤©æœºå™¨äººï¼‰ä¸­ï¼Œä¿ç•™å…ˆå‰çš„äº¤äº’è®°å½•å¯¹äºçŸ­æœŸå’Œé•¿æœŸéƒ½éå¸¸é‡è¦ã€‚è¿™æ—¶å°±éœ€è¦å¼•å…¥ â€œå†…å­˜â€ çš„æ¦‚å¿µï¼Œåˆ™éœ€è¦æˆ‘ä»¬è‡ªè¡Œç»´æŠ¤èŠå¤©è®°å½•ï¼Œå³æ¯æ¬¡æŠŠèŠå¤©è®°å½•å‘ç»™ gptã€‚PSï¼šä¸€èˆ¬æƒ…å†µä¸‹ï¼Œè®°å¿†æ¨¡å‹æ›´å¤šçš„æ˜¯åº”ç”¨äºä¼šè¯/chatåœºæ™¯ï¼ŒBaseMemoryä¸‹ä¸€å±‚åªæœ‰ä¸€ä¸ªå­ç±» BaseChatMemoryã€‚
1. æˆ‘ä»¬å¯ä»¥é€šè¿‡ BufferWindowMemory è®°ä½è¿‡å»å‡ è½®çš„å¯¹è¯ï¼Œé€šè¿‡ SummaryMemory æ¦‚æ‹¬å¯¹è¯çš„å†å²å¹¶è®°ä¸‹æ¥ã€‚ä¹Ÿå¯ä»¥å°†ä¸¤è€…ç»“åˆï¼Œä½¿ç”¨ BufferSummaryMemory æ¥ç»´æŠ¤ä¸€ä¸ªå¯¹æ•´ä½“å¯¹è¯åšäº†å°ç»“ï¼ŒåŒæ—¶åˆè®°ä½æœ€è¿‘å‡ è½®å¯¹è¯çš„â€œè®°å¿†â€ã€‚
2. å¯ä»¥ä½¿ç”¨ EntityMemoryï¼Œå®ƒä¼šå¸®åŠ©æˆ‘ä»¬è®°ä½æ•´ä¸ªå¯¹è¯é‡Œé¢çš„â€œå‘½åå®ä½“â€ï¼ˆEntityï¼‰ï¼Œä¿ç•™å®é™…åœ¨å¯¹è¯ä¸­æˆ‘ä»¬æœ€å…³å¿ƒçš„ä¿¡æ¯ã€‚

```python
from langchain.memory import ChatMessageHistory
from langchain.chat_models import ChatOpenAI

chat = ChatOpenAI(temperature=0)
history = ChatMessageHistory() # åˆå§‹åŒ– MessageHistory å¯¹è±¡
history.add_ai_message("ä½ å¥½ï¼") # ç»™ MessageHistory å¯¹è±¡æ·»åŠ å¯¹è¯å†…å®¹
history.add_user_message("ä¸­å›½çš„é¦–éƒ½æ˜¯å“ªé‡Œï¼Ÿ")
ai_response = chat(history.messages) # æ‰§è¡Œå¯¹è¯
print(ai_response)
```

### Agent

Turn your LLMs into reasoning engines. the core idea of agents is to use an LLM to choose a sequence of actions to take. in chains, a sequence of actions is hardcoded(in code). In agents a language model is used as a reasoning engine to determine which actions to take and in which order. 

ä»â€œå‘Šè¯‰è®¡ç®—æœºè¦åšä»€ä¹ˆâ€çš„ç¼–ç¨‹èŒƒå¼å‘â€œå‘Šè¯‰è®¡ç®—æœºæˆ‘ä»¬æƒ³è¦ä»€ä¹ˆâ€çš„èŒƒå¼çš„è½¬å˜ã€‚äººç±»ä¹‹æ‰€ä»¥åœ¨åœ°çƒä¸Šæ˜¾å¾—ç‹¬ç‰¹ï¼Œä¸€ä¸ªé‡è¦åŸå› æ˜¯æˆ‘ä»¬æ›´æ“…é•¿ä½¿ç”¨å·¥å…·ã€‚æ— è®ºæ˜¯ç½‘ç»œä¸Šçš„è¿˜æ˜¯ç°å®ä¸–ç•Œé‡Œé¢çš„å®ä½“æœºå™¨äººï¼Œ**åªè¦ç»™å¤§æ¨¡å‹è¶³å¤Ÿçš„ã€Œè¯´æ˜ä¹¦ã€**ï¼Œå¤§æ¨¡å‹å°±å¯ä»¥æŠŠå®ƒçº³å…¥åˆ°è§£å†³æ–¹æ¡ˆå·¥å…·ç®±ï¼Œç”¨æ¥å›ç­”æˆ–è€…æ‰§è¡Œä½ ç”¨è‡ªç„¶è¯­è¨€æå‡ºçš„é—®é¢˜ or ä»»åŠ¡ã€‚

æŸäº›åº”ç”¨å¯èƒ½éœ€è¦ä¸ä»…é¢„å®šçš„ LLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰/å…¶ä»–å·¥å…·è°ƒç”¨é¡ºåºï¼Œè¿˜å¯èƒ½éœ€è¦æ ¹æ®ç”¨æˆ·çš„è¾“å…¥ç¡®å®šä¸ç¡®å®šçš„è°ƒç”¨é¡ºåºã€‚è¿™ç§æƒ…å†µä¸‹æ¶‰åŠåˆ°çš„åºåˆ—åŒ…æ‹¬ä¸€ä¸ª â€œä»£ç†ï¼ˆAgentï¼‰â€ï¼Œè¯¥ä»£ç†å¯ä»¥è®¿é—®å¤šç§å·¥å…·ã€‚**æ ¹æ®ç”¨æˆ·çš„è¾“å…¥ï¼Œä»£ç†å¯èƒ½å†³å®šæ˜¯å¦è°ƒç”¨è¿™äº›å·¥å…·**ï¼Œå¹¶ç¡®å®šè°ƒç”¨æ—¶çš„è¾“å…¥ã€‚å¦‚æœæˆ‘ä»¬çœŸçš„æƒ³è¦åšä¸€ä¸ªèƒ½è·‘åœ¨ç”Ÿäº§ç¯å¢ƒä¸Šçš„ AI èŠå¤©æœºå™¨äººï¼Œæˆ‘ä»¬éœ€è¦çš„ä¸åªä¸€ä¸ªå•é¡¹æŠ€èƒ½ï¼Œå¯¹äºæœ‰å¾ˆå¤šä¸ªä¸åŒçš„â€œå•é¡¹æŠ€èƒ½â€ï¼ŒAI è¦èƒ½å¤Ÿè‡ªå·±åˆ¤æ–­ä»€ä¹ˆæ—¶å€™è¯¥ç”¨ä»€ä¹ˆæ ·çš„æŠ€èƒ½ï¼ˆæ„å›¾è¯†åˆ«é—®é¢˜ï¼‰ã€‚é€šè¿‡â€œå…ˆè®© AI åšä¸ªé€‰æ‹©é¢˜â€çš„æ–¹å¼ï¼ŒLangchain è®© AI è‡ªåŠ¨ä¸ºæˆ‘ä»¬é€‰æ‹©åˆé€‚çš„ Tool å»è°ƒç”¨ã€‚æˆ‘ä»¬å¯ä»¥æŠŠå›ç­”ä¸åŒç±»å‹é—®é¢˜çš„ LLMChain å°è£…æˆä¸åŒçš„ Toolï¼Œä¹Ÿå¯ä»¥ç›´æ¥è®© Tool å»è°ƒç”¨ç‰¹å®šèƒ½åŠ›çš„LLMChain ç­‰å·¥å…·ã€‚æ¯”å¦‚

```python
from langchain.agents import load_tools
from langchain.agents import initialize_agent
from langchain.llms import OpenAI
from langchain.agents import AgentType
llm = OpenAI(temperature=0,max_tokens=2048)      # åŠ è½½ OpenAI æ¨¡å‹
tools = load_tools(["serpapi"])                  # åŠ è½½ serpapi å·¥å…·
agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True) # å·¥å…·åŠ è½½åéƒ½éœ€è¦åˆå§‹åŒ–ï¼Œverbose å‚æ•°ä¸º Trueï¼Œä¼šæ‰“å°å…¨éƒ¨çš„æ‰§è¡Œè¯¦æƒ…
agent.run("What's the date today? What great events have taken place today in history?") # è¿è¡Œ agent
```

### å®æˆ˜

1. å®Œæˆä¸€æ¬¡é—®ç­”
    ```python
    from langchain.llms import OpenAI
    llm = OpenAI(model_name="text-davinci-003",max_tokens=1024)
    llm("æ€ä¹ˆè¯„ä»·äººå·¥æ™ºèƒ½")
    ```
2. é€šè¿‡ Google æœç´¢å¹¶è¿”å›ç­”æ¡ˆã€‚ ç”¨agent
3. å¯¹è¶…é•¿æ–‡æœ¬è¿›è¡Œæ€»ç»“ã€‚ æˆ‘ä»¬é€šå¸¸çš„åšæ³•å°±æ˜¯ç›´æ¥å‘ç»™ api è®©ä»–æ€»ç»“ã€‚ä½†æ˜¯å¦‚æœæ–‡æœ¬è¶…è¿‡äº† api æœ€å¤§çš„ token é™åˆ¶å°±ä¼šæŠ¥é”™ã€‚è¿™æ—¶ï¼Œæˆ‘ä»¬ä¸€èˆ¬ä¼šè¿›è¡Œå¯¹æ–‡ç« è¿›è¡Œåˆ†æ®µï¼Œæ¯”å¦‚é€šè¿‡ tiktoken è®¡ç®—å¹¶åˆ†å‰²ï¼Œç„¶åå°†å„æ®µå‘é€ç»™ api è¿›è¡Œæ€»ç»“ï¼Œæœ€åå°†å„æ®µçš„æ€»ç»“å†è¿›è¡Œä¸€ä¸ªå…¨éƒ¨çš„æ€»ç»“ã€‚
    ```python
    loader = UnstructuredFileLoader("/content/sample_data/data/lg_test.txt")     # å¯¼å…¥æ–‡æœ¬
    document = loader.load()  # å°†æ–‡æœ¬è½¬æˆ Document å¯¹è±¡
    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500,chunk_overlap = 0)     # åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
    split_documents = text_splitter.split_documents(document)  # åˆ‡åˆ†æ–‡æœ¬
    llm = OpenAI(model_name="text-davinci-003", max_tokens=1500)  # åŠ è½½ llm æ¨¡å‹
    chain = load_summarize_chain(llm, chain_type="refine", verbose=True) # åˆ›å»ºæ€»ç»“é“¾
    chain.run(split_documents[:5]) # æ‰§è¡Œæ€»ç»“é“¾ï¼Œï¼ˆä¸ºäº†å¿«é€Ÿæ¼”ç¤ºï¼Œåªæ€»ç»“å‰5æ®µï¼‰
    ```
4. æ„å»ºæœ¬åœ°çŸ¥è¯†åº“é—®ç­”æœºå™¨äºº
    ```python
    loader = DirectoryLoader('/content/sample_data/data/', glob='**/*.txt') # åŠ è½½æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰txtç±»å‹çš„æ–‡ä»¶
    documents = loader.load() # å°†æ•°æ®è½¬æˆ document å¯¹è±¡ï¼Œæ¯ä¸ªæ–‡ä»¶ä¼šä½œä¸ºä¸€ä¸ª document
    text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0) # åˆå§‹åŒ–åŠ è½½å™¨
    split_docs = text_splitter.split_documents(documents)  # åˆ‡å‰²åŠ è½½çš„ document
    embeddings = OpenAIEmbeddings() # åˆå§‹åŒ– openai çš„ embeddings å¯¹è±¡
    docsearch = Chroma.from_documents(split_docs, embeddings) # å°† document é€šè¿‡ openai çš„ embeddings å¯¹è±¡è®¡ç®— embedding å‘é‡ä¿¡æ¯å¹¶ä¸´æ—¶å­˜å…¥ Chroma å‘é‡æ•°æ®åº“ï¼Œç”¨äºåç»­åŒ¹é…æŸ¥è¯¢
    qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type="stuff", retriever=docsearch.as_retriever(), return_source_documents=True) # åˆ›å»ºé—®ç­”å¯¹è±¡
    result = qa({"query": "ç§‘å¤§è®¯é£ä»Šå¹´ç¬¬ä¸€å­£åº¦æ”¶å…¥æ˜¯å¤šå°‘ï¼Ÿ"}) # è¿›è¡Œé—®ç­”
    print(result)
    ```

## å‘é‡æ•°æ®åº“

å½“æˆ‘ä»¬æŠŠé€šè¿‡æ¨¡å‹æˆ–è€… AI åº”ç”¨å¤„ç†å¥½çš„æ•°æ®å–‚ç»™å®ƒä¹‹åï¼ˆâ€œä¸€å †ç‰¹å¾å‘é‡â€ï¼‰ï¼Œå®ƒä¼šæ ¹æ®ä¸€äº›å›ºå®šçš„å¥—è·¯ï¼Œä¾‹å¦‚åƒä¼ ç»Ÿæ•°æ®åº“è¿›è¡ŒæŸ¥è¯¢ä¼˜åŒ–åŠ é€Ÿé‚£æ ·ï¼Œä¸ºè¿™äº›æ•°æ®å»ºç«‹ç´¢å¼•ã€‚é¿å…æˆ‘ä»¬è¿›è¡Œæ•°æ®æŸ¥è¯¢çš„æ—¶å€™ï¼Œéœ€è¦ç¬¨æ‹™çš„åœ¨æµ·é‡æ•°æ®ä¸­è¿›è¡Œã€‚

### æœ¬åœ°

faiss åŸç”Ÿä½¿ç”¨
```python
# å‡†å¤‡æ•°æ®
model = SentenceTransformer('uer/sbert-base-chinese-nli')
sentences = ["ä½åœ¨å››å·æ™®é‡Œæ€€ç‰¹è¡—çš„æœæ–¯åˆ©å…ˆç”ŸåŠå¤«äººéå¸¸éª„å‚²åœ°å®£ç§°è‡ªå·±æ˜¯ååˆ†æ­£å¸¸çš„äºº",
             "æœæ–¯åˆ©å…ˆç”Ÿæ˜¯ä¸€å®¶å«ä½œæ ¼æœ—å®æ–¯çš„é’»æœºå·¥å‚çš„è€æ¿", "å“ˆåˆ©çœ‹ç€å¥¹èŒ«ç„¶åœ°ä½ä¸‹å¤´æ‘¸äº†æ‘¸é¢å¤´ä¸Šé—ªç”µå½¢çš„ä¼¤ç–¤",
             "åä¹å¹´æ¥å“ˆåˆ©çš„ä¼¤ç–¤å†ä¹Ÿæ²¡æœ‰ç–¼è¿‡"]
sentence_embeddings = model.encode(sentences)
# å»ºç«‹ç´¢å¼•
dimension = sentence_embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(sentence_embeddings)

# æ£€ç´¢
topK = 2
search = model.encode(["å“ˆåˆ©æ³¢ç‰¹çŒ›ç„¶ç¡é†’"])  # å°†è¦æœç´¢çš„å†…å®¹â€œå“ˆåˆ©æ³¢ç‰¹çŒ›ç„¶ç¡é†’â€ç¼–ç ä¸ºå‘é‡
D, I = index.search(search, topK)         # DæŒ‡çš„æ˜¯â€œæ•°æ®ç½®ä¿¡åº¦/å¯ä¿¡åº¦â€ I æŒ‡çš„æ˜¯æˆ‘ä»¬ä¹‹å‰æ•°æ®å‡†å¤‡æ—¶çŒå…¥çš„æ–‡æœ¬æ•°æ®çš„å…·ä½“è¡Œæ•°ã€‚
print(I)
print([x for x in sentences if sentences.index(x) in I[0]])
```
faiss ä¸LangChain é›†åˆï¼Œä¸»è¦æ˜¯ä¸  LangChain çš„ documentå’Œ Embeddings ç»“åˆã€‚ faiss æœ¬èº«åªå­˜å‚¨ æ–‡æœ¬å‘é‡åŒ–åçš„å‘é‡ï¼ˆindex.faissæ–‡ä»¶ï¼‰ï¼Œä½†æ˜¯vector dbå¯¹å¤–ä½¿ç”¨ï¼Œä¸€å®šæ˜¯æ–‡æœ¬æŸ¥æ–‡æœ¬ï¼Œæ‰€ä»¥è¦è®°å½• æ–‡æœ¬å—ä¸å‘é‡å…³ç³»ï¼ˆindex.pklæ–‡ä»¶ï¼‰ã€‚æ­¤å¤–ï¼Œéœ€æ”¯æŒæ–°å¢å’Œåˆ é™¤æ–‡ä»¶ï¼ˆåŒ…å«å¤šä¸ªæ–‡æœ¬å—ï¼‰ï¼Œæ‰€ä»¥ä¹Ÿè¦æ”¯æŒæŒ‰æ–‡ä»¶åˆ é™¤ æ–‡æœ¬å—å¯¹åº”çš„å‘é‡ã€‚ 

```python
from langchain.document_loaders import TextLoader
# å½•å…¥documents åˆ°faiss
loader = TextLoader("xx.txt")  # åŠ è½½æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰txtç±»å‹çš„æ–‡ä»¶
documents = loader.load() # å°†æ•°æ®è½¬æˆ document å¯¹è±¡ï¼Œæ¯ä¸ªæ–‡ä»¶ä¼šä½œä¸ºä¸€ä¸ª document
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0) 
docs = text_splitter.split_documents(documents)  # åˆ‡å‰²åŠ è½½çš„ document

embeddings = OpenAIEmbeddings() # åˆå§‹åŒ– openai çš„ embeddings å¯¹è±¡
db = FAISS.from_documents(docs, embeddings) # å°† document é€šè¿‡ openai çš„ embeddings å¯¹è±¡è®¡ç®— embedding å‘é‡ä¿¡æ¯å¹¶ä¸´æ—¶å­˜å…¥ faiss å‘é‡æ•°æ®åº“ï¼Œç”¨äºåç»­åŒ¹é…æŸ¥è¯¢

query = "What did the president say about Ketanji Brown Jackson"
docs = db.similarity_search(query)
print(docs[0].page_content)
```

ç®€å•çš„æºç åˆ†æ

```python
# æ ¹æ®æ–‡æ¡£å†…å®¹æ„å»º langchain.vectorstores.Faiss
vectorstore.base.from_documents(cls: Type[VST],documents: List[Document], embedding: Embeddings,    **kwargs: Any,) -> VST:
    """Return VectorStore initialized from documents and embeddings."""
    texts = [d.page_content for d in documents]
    metadatas = [d.metadata for d in documents]
    return cls.from_texts(texts, embedding, metadatas=metadatas, **kwargs)
        # Embeds documents.
        embeddings = embedding.embed_documents(texts)
        cls.__from(texts,embeddings,embedding, metadatas=metadatas,ids=ids,**kwargs,)
            # Initializes the FAISS database
            faiss = dependable_faiss_import()
            index = faiss.IndexFlatL2(len(embeddings[0]))
            vector = np.array(embeddings, dtype=np.float32)
            index.add(vector)
            # å»ºç«‹id ä¸text çš„å…³è”
            documents = []
            if ids is None:
                ids = [str(uuid.uuid4()) for _ in texts]
            for i, text in enumerate(texts):
                metadata = metadatas[i] if metadatas else {}
                documents.append(Document(page_content=text, metadata=metadata))
            index_to_id = dict(enumerate(ids))
            # Creates an in memory docstore
            docstore = InMemoryDocstore(dict(zip(index_to_id.values(), documents)))
            return cls(embedding.embed_query,index,docstore,index_to_id,normalize_L2=normalize_L2,**kwargs,) 
save_local:
    faiss = dependable_faiss_import()
    faiss.write_index(self.index, str(path / "{index_name}.faiss".format(index_name=index_name)))
    with open(path / "{index_name}.pkl".format(index_name=index_name), "wb") as f:
        pickle.dump((self.docstore, self.index_to_docstore_id), f)   
```



### åœ¨çº¿

Pinecone æ˜¯ä¸€ä¸ªåœ¨çº¿çš„å‘é‡æ•°æ®åº“ã€‚æ‰€ä»¥ï¼Œæˆ‘å¯ä»¥ç¬¬ä¸€æ­¥ä¾æ—§æ˜¯æ³¨å†Œï¼Œç„¶åæ‹¿åˆ°å¯¹åº”çš„ api keyã€‚

```python
from langchain.vectorstores import Pinecone
# ä»è¿œç¨‹æœåŠ¡åŠ è½½æ•°æ®
docsearch = Pinecone.from_existing_index(index_name, embeddings)

# å½•å…¥documents æŒä¹…åŒ–æ•°æ®åˆ°pinecone
# åˆå§‹åŒ– pinecone
pinecone.init(api_key="ä½ çš„api key",environment="ä½ çš„Environment")
loader = DirectoryLoader('/content/sample_data/data/', glob='**/*.txt')
documents = loader.load() # å°†æ•°æ®è½¬æˆ document å¯¹è±¡ï¼Œæ¯ä¸ªæ–‡ä»¶ä¼šä½œä¸ºä¸€ä¸ª document
text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0)
split_docs = text_splitter.split_documents(documents) # åˆ‡å‰²åŠ è½½çš„ document
docsearch = Pinecone.from_texts([t.page_content for t in split_docs], embeddings, index_name=index_name) # æŒä¹…åŒ–æ•°æ®åˆ°pinecone
```

[ LangChain + GPTCache =å…¼å…·ä½æˆæœ¬ä¸é«˜æ€§èƒ½çš„ LLM](https://mp.weixin.qq.com/s/kC6GB9JaT-WApxU2o3QfdA) æœªè¯»ã€‚

## æ¨ç†

1. vLLMæ˜¯ä¸€ä¸ªå¼€æºçš„å¤§æ¨¡å‹æ¨ç†åŠ é€Ÿæ¡†æ¶ï¼Œé€šè¿‡PagedAttentioné«˜æ•ˆåœ°ç®¡ç†attentionä¸­ç¼“å­˜çš„å¼ é‡ï¼Œå®ç°äº†æ¯”HuggingFace Transformersé«˜14-24å€çš„ååé‡ã€‚
2. NVIDIA FasterTransformer (FT) æ˜¯ä¸€ä¸ªç”¨äºå®ç°åŸºäºTransformerçš„ç¥ç»ç½‘ç»œæ¨ç†çš„åŠ é€Ÿå¼•æ“ã€‚å®ƒåŒ…å«Transformerå—çš„é«˜åº¦ä¼˜åŒ–ç‰ˆæœ¬çš„å®ç°ï¼Œå…¶ä¸­åŒ…å«ç¼–ç å™¨å’Œè§£ç å™¨éƒ¨åˆ†ã€‚ä½¿ç”¨æ­¤æ¨¡å—ï¼Œæ‚¨å¯ä»¥è¿è¡Œç¼–ç å™¨-è§£ç å™¨æ¶æ„æ¨¡å‹ï¼ˆå¦‚ï¼šT5ï¼‰ã€ä»…ç¼–ç å™¨æ¶æ„æ¨¡å‹ï¼ˆå¦‚ï¼šBERTï¼‰å’Œä»…è§£ç å™¨æ¶æ„æ¨¡å‹ï¼ˆå¦‚ï¼šGPTï¼‰çš„æ¨ç†ã€‚FTæ¡†æ¶æ˜¯ç”¨C++/CUDAç¼–å†™çš„ï¼Œä¾èµ–äºé«˜åº¦ä¼˜åŒ–çš„ cuBLASã€cuBLASLt å’Œ cuSPARSELt åº“ï¼Œè¿™ä½¿æ‚¨å¯ä»¥åœ¨ GPU ä¸Šè¿›è¡Œå¿«é€Ÿçš„ Transformer æ¨ç†ã€‚ä¸ NVIDIA TensorRT ç­‰å…¶ä»–ç¼–è¯‘å™¨ç›¸æ¯”ï¼ŒFT çš„æœ€å¤§ç‰¹ç‚¹æ˜¯å®ƒæ”¯æŒä»¥åˆ†å¸ƒå¼æ–¹å¼è¿›è¡Œ Transformer å¤§æ¨¡å‹æ¨ç†ã€‚åœ¨åº•å±‚ï¼ŒèŠ‚ç‚¹é—´æˆ–èŠ‚ç‚¹å†…é€šä¿¡ä¾èµ–äº MPI ã€ NVIDIA NCCLã€Glooç­‰ã€‚å› æ­¤ï¼Œä½¿ç”¨FasterTransformerï¼Œæ‚¨å¯ä»¥åœ¨å¤šä¸ª GPU ä¸Šä»¥å¼ é‡å¹¶è¡Œè¿è¡Œå¤§å‹Transformerï¼Œä»¥å‡å°‘è®¡ç®—å»¶è¿Ÿã€‚åŒæ—¶ï¼ŒTP å’Œ PP å¯ä»¥ç»“åˆåœ¨ä¸€èµ·ï¼Œåœ¨å¤š GPU èŠ‚ç‚¹ç¯å¢ƒä¸­è¿è¡Œå…·æœ‰æ•°åäº¿ã€æ•°ä¸‡äº¿ä¸ªå‚æ•°çš„å¤§å‹ Transformer æ¨¡å‹ã€‚
3. DeepSpeed-MII æ˜¯ DeepSpeed çš„ä¸€ä¸ªæ–°çš„å¼€æº Python åº“ï¼Œæ—¨åœ¨ä½¿æ¨¡å‹ä¸ä»…ä½å»¶è¿Ÿå’Œä½æˆæœ¬æ¨ç†ï¼Œè€Œä¸”è¿˜æ˜“äºè®¿é—®ã€‚

## å…¶å®ƒ

[LangFlowâ€”â€”ä¸€æ¬¾å¯è½»æ¾å®éªŒå’ŒåŸå‹åŒ– LangChainæµæ°´çº¿çš„AIé¡¹ç›®](https://mp.weixin.qq.com/s/omHZ_IqjISphmdGz3tiMnQ)