---

layout: post
title: å¤§æ¨¡å‹æ¨ç†
category: æ¶æ„
tags: MachineLearning
keywords: large model

---

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

## ç®€ä»‹

* TOC
{:toc}


## æ€è·¯

[å¤§æ¨¡å‹æ¨ç†åŠ é€ŸæŠ€æœ¯æ¦‚è¦](https://mp.weixin.qq.com/s/kr5-QFhPXrUb7omTvJ-rDw)ç›®å‰å¤§æ¨¡å‹æ¨ç†åŠ é€ŸæŠ€æœ¯æ ˆå¤§ä½“å¯ä»¥åˆ†æˆä¸‰å±‚ï¼ˆä»ä½åˆ°é«˜ï¼‰ï¼š
1. çº¿æ€§ä»£æ•°è®¡ç®—åº“ï¼ŒcuBLASã€Eigenã€Intel MKLã€ARM Compute Libraryç­‰ï¼Œå…¶ä¸­å®šä¹‰äº†çŸ©é˜µä¹˜æ³•ã€çŸ©é˜µå’Œå‘é‡ä¹˜æ³•ç­‰æ•°åä¸ªæ ‡å‡†å‡½æ•°ã€‚çº¿æ€§ä»£æ•°å±‚çš„åŠ é€Ÿä¸»è¦ä¾èµ–ä»¥ä¸‹ä¼˜åŒ–ï¼š
    1. GPUå¤šæ ¸è®¡ç®—èƒ½åŠ›ï¼šé€šè¿‡è°ƒç”¨CUDAã€OpenCLç­‰APIï¼Œæ¥åˆ©ç”¨GPUçš„å¹¶è¡Œèƒ½åŠ›ã€‚
    2. CPU SIMDå’Œå¤šæ ¸ ï¼šå•æŒ‡ä»¤å¤šæ•°æ®SIMDåœ¨x86ä¸Šæœ‰SSExå’ŒAVXç­‰æŒ‡ä»¤ï¼Œåœ¨ARMä¸Šæœ‰NEONå’ŒSVEï¼Œéƒ½å¹¿æ³›è¢«ä½¿ç”¨ï¼Œä¹Ÿæœ‰çš„åº“é€šè¿‡OpenMPå†å åŠ å¤šæ ¸èƒ½åŠ›ã€‚
    3. Tilingåˆ†å—ï¼šçŸ©é˜µä¹˜æ³•GEMMä½œä¸ºæœºå™¨å­¦ä¹ å…³é”®æ“ä½œï¼Œå¯ä»¥é€šè¿‡Tilingçš„æ–¹æ³•ï¼Œå¤§å¹…å‡å°‘å¯¹äºå†…å­˜å¸¦å®½çš„éœ€æ±‚ï¼Œæé«˜é€Ÿåº¦ã€‚
    4. Autotuningè‡ªåŠ¨è°ƒä¼˜ï¼šé€šè¿‡å‚æ•°ç©ºé—´æœç´¢ï¼Œå¯ä»¥åœ¨å¤šä¸ªåˆ†å—åŠæ³•å’Œæ“ä½œæ ¸ä¹‹é—´è‡ªåŠ¨ä¼˜é€‰é€‚åˆæœ¬æœºçš„ä¼˜åŒ–æ–¹æ¡ˆã€‚
2. æ¨¡å‹æ¨ç†å¼•æ“ï¼ŒTensorRTã€TensorFlowServingã€TVMç­‰ã€‚ å’Œçº¿æ€§ä»£æ•°å±‚çš„ä¼˜åŒ–ä¸åŒï¼Œæ‰§è¡Œå¼•æ“èƒ½å¤Ÿçœ‹åˆ°æ•´ä¸ªç¥ç»ç½‘ç»œçš„æ¶æ„ï¼Œä¹Ÿèƒ½å¤ŸåŒæ—¶å¤„ç†å¤šä¸ªæ¥è‡ªå®¢æˆ·ç«¯çš„è¯·æ±‚ï¼Œæ‰€ä»¥å¯ä»¥ä½¿ç”¨æ¶‰åŠå¤šä¸ªç®—å­ã€æ•´ä¸ªæ¨¡å‹ï¼Œä»¥åŠå¤šä¸ªè¯·æ±‚é—´çš„ä¼˜åŒ–æ¥æé«˜æ‰§è¡Œæ•ˆç‡ã€‚æ‰§è¡Œå¼•æ“ä¸€èˆ¬æœ‰è¿™äº›åŠæ³•å°†æ¨¡å‹æ¨ç†è¿›ä¸€æ­¥åŠ é€Ÿï¼š
    1. Operator Fusion ç®—å­èåˆï¼šå› ä¸ºå†…å­˜å¸¦å®½å¾€å¾€æ˜¯ä¸€å¤§ç“¶é¢ˆï¼Œæ‰€ä»¥ç®€å•å°†å¤šä¸ªç›¸é‚»çš„ç®—å­æ‰¾å‡†æœºä¼šåˆå¹¶èµ·æ¥è®¡ç®—ï¼Œå°±å¯ä»¥å‡å°‘å¯¹æ•°æ®çš„æ‰«æè€Œå¤§å¹…æå‡æ€§èƒ½ï¼Œæ‰€ä»¥Fusionæ˜¯ç®—å­é—´ä¼˜åŒ–çš„é‡è¦æ­¥éª¤ï¼Œå¯ä»¥æ‰‹å·¥è¿›è¡Œï¼Œä¹Ÿå¯ä»¥ç”±æ‰§è¡Œå¼•æ“è‡ªåŠ¨è¿›è¡Œã€‚
    2. Quantization é‡åŒ–ï¼šéšç€GPUå¯¹æ•°æ®ç»“æ„æ”¯æŒçš„å¤šå…ƒåŒ–ï¼Œå½“å‰æ¨ç†çš„åŸºçº¿æ•°æ®ç±»å‹å·²ç»æ˜¯FP16ï¼Œæ¯”å‡ å¹´å‰çš„FP32æé«˜äº†ä¸å°‘é€Ÿåº¦ã€‚å³ä¾¿å¦‚æ­¤ï¼Œå°†æ¨¡å‹é‡åŒ–ä¸ºINT8è¿›è¡Œæ¨ç†ï¼Œä¾ç„¶å¯ä»¥æé«˜è¾ƒå¤šé€Ÿåº¦ï¼Œè€Œåœ¨æ‰‹æœºå¹³å°ä¸Šï¼Œé‡åŒ–æ¨ç†èƒ½è¿›ä¸€æ­¥é™ä½èƒ½è€—ã€‚
    3. Distribution åˆ†å¸ƒå¼ï¼šä½¿ç”¨å¤šå¡æ¨ç†ï¼Œä»¥åŠé€šä¿¡åŠ é€Ÿï¼Œæ¥æå‡èƒ½æ¨ç†çš„æ¨¡å‹è§„æ¨¡å’Œé€Ÿåº¦ã€‚
    4. Batching æ‰¹é‡åŒ–ï¼šå°†å¤šä¸ªè¯·æ±‚åˆå¹¶å¤„ç†ï¼Œæ˜¯æé«˜æ€§èƒ½çš„å¦å¤–ä¸€ä¸ªå…³é”®åŠæ³•ï¼Œè¿™ä¸ªèƒ½å¤§å¹…æé«˜æ€§èƒ½çš„åŸå› ä¸»è¦æœ‰ä¸¤ä¸ªï¼š1. åˆå¹¶è¯·æ±‚å¯ä»¥å¢å¤§ä»£æ•°è¿ç®—çš„çŸ©é˜µè§„æ¨¡ï¼Œè€Œä¸‹å±‚ä»£æ•°åº“å¤„ç†è¶Šå¤§çš„çŸ©é˜µè§„æ¨¡ï¼Œç›¸å¯¹æ€§èƒ½è¶Šé«˜ã€‚2. åˆå¹¶è¯·æ±‚å¯ä»¥å‡å°‘å¯¹é™æ€çš„æ¨¡å‹å‚æ•°çŸ©é˜µçš„æ‰«ææ¬¡æ•°ï¼Œå‡å°‘å†…å­˜å¸¦å®½æ¶ˆè€—ã€‚
3. å¤§æ¨¡å‹è°ƒåº¦å¼•æ“ï¼ŒvLLMã€TensorRT-LLMï¼ˆåŸFasterTransformerï¼‰ã€llama.cppç­‰ã€‚å¤§æ¨¡å‹è°ƒåº¦å¼•æ“æ˜¯2022å¹´å¼€å§‹æ–°å‡ºç°çš„ä¸€å±‚æŠ½è±¡ã€‚ä¸ºä»€ä¹ˆæœ‰äº†æ‰§è¡Œå¼•æ“è¿˜éœ€è¦å¤§æ¨¡å‹è°ƒåº¦å¼•æ“ï¼Ÿä¸»è¦æ˜¯å› ä¸ºå¤§å®¶å¸Œæœ›è¿›ä¸€æ­¥ä¼˜åŒ–æ¨ç†æ€§èƒ½ï¼Œè€Œå¤§æ¨¡å‹æ¶æ„ç›¸å¯¹å›ºå®šï¼ˆTransformeræ¶æ„åŠå˜å½¢ï¼‰ï¼Œé€šè¿‡ä¸“é—¨é’ˆå¯¹å¤§æ¨¡å‹è€Œä¸æ˜¯æ›´é€šç”¨çš„ç¥ç»ç½‘ç»œè¿›è¡Œæ¨ç†ä¼˜åŒ–ï¼Œå°±å¯ä»¥åˆ©ç”¨å¤§æ¨¡å‹æ¶æ„çš„ç‰¹ç‚¹å’Œç®—æ³•ç‰¹æ€§ï¼Œæ¥è¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚
    1. KV Cacheï¼šè¿™æ˜¯fairseqç­‰ç³»ç»Ÿå¾ˆæ—©å°±å¼€å§‹æœ‰çš„åŸºç¡€æ–¹æ³•ï¼Œå°±æ˜¯å°†transformer attentionè®¡ç®—ä¸­çš„Keyå’ŒValueå¼ é‡é›†åˆç¼“å­˜ä¸‹æ¥ï¼Œé¿å…æ¯è¾“å‡ºä¸€ä¸ªtokenéƒ½é‡å¤è®¡ç®—ã€‚
    2. Iteration-level scheduling è¿­ä»£å±‚è°ƒåº¦ï¼šè¿™æ˜¯2022å¹´Orcaå¼•å…¥çš„æ–¹æ³•ï¼ˆå‚è€ƒæ–‡çŒ®1ï¼‰ï¼Œæ¨ç†å¼•æ“é»˜è®¤éƒ½æ˜¯æŒ‰è¯·æ±‚æ‰¹é‡åŒ–ï¼Œè€ŒLLMæ¨ç†éœ€è¦å¤šæ¬¡è¿­ä»£è¿›è¡Œè‡ªå›å½’è®¡ç®—ï¼Œæ‰€ä»¥æŒ‰â€œè¿­ä»£â€ä¸ºå•ä½è¿›è¡Œæ‰¹é‡åŒ–ï¼Œå¯ä»¥æé«˜å¹¶è¡Œåº¦å’Œæ€§èƒ½ã€‚
    3. PagedAttention åˆ†é¡µæ³¨æ„åŠ›: è¿™æ˜¯ä»Šå¹´vLLMå¼•å…¥çš„æ–¹æ³•ï¼ˆå‚è€ƒæ–‡çŒ®2ï¼‰ï¼ŒèƒŒåæ´å¯Ÿæ˜¯ä¸Šé¢æåˆ°çš„KV cacheå ç”¨å¤§é‡GPUå†…å­˜ï¼Œä¸€ä¸ª13Bæ¨¡å‹æ¯ä¸ªè¾“å‡ºtokenå¯¹åº”çš„KVå¼ é‡ï¼Œéœ€è¦800KBï¼Œè€Œæœ€é•¿è¾“å‡ºé•¿åº¦2048ä¸ªtokençš„è¯ï¼Œä¸€ä¸ªè¯·æ±‚å°±éœ€è¦1.6GBæ˜¾å­˜ã€‚å› æ­¤vLLMå¼•å…¥ç±»ä¼¼æ“ä½œç³»ç»Ÿä¸­çš„åˆ†é¡µæœºåˆ¶ï¼Œå¤§å¹…å‡å°‘äº†KV cacheçš„ç¢ç‰‡åŒ–ï¼Œæé«˜æ€§èƒ½ã€‚
    4. GPTQé‡åŒ–ã€‚æœ‰ä¸€æ‰¹ç ”ç©¶ä¸“æ³¨äºå¯»æ‰¾æ›´ä¼˜çš„é‡åŒ–æ–¹æ³•ï¼Œllama.cppæ”¯æŒè¿‘æœŸå‘è¡¨çš„GPTQï¼ˆå‚è€ƒæ–‡çŒ®3ï¼‰ï¼Œé»˜è®¤å°†æ¨¡å‹é‡åŒ–åˆ°4æ¯”ç‰¹ï¼Œå¤§å¹…æå‡æ€§èƒ½ä¸”å‡†ç¡®ç‡ä¸‹é™å¾ˆå°ã€‚
    5. Fused kernelsç­‰å„ç±»æ‰‹å·¥ä¼˜åŒ–ï¼šå¾ˆå¤šæ—¶å€™ï¼Œæ‰‹æ‰“ä¼˜åŒ–éƒ½æ˜¯å°‘ä¸äº†çš„åŠæ³•ï¼Œllama.cppçŸ­æ—¶é—´ç§¯ç´¯å¤§é‡ç”¨æˆ·ï¼Œå°±æ˜¯å› ä¸ºé¡¹ç›®ä½œè€…ä¸æ€•éº»çƒ¦ï¼Œå¿«é€Ÿç§¯ç´¯äº†å¤§é‡æ‰‹å·¥å°ä¼˜åŒ–ï¼Œé›†è…‹æˆè£˜ï¼Œå½¢æˆé¢†å…ˆçš„ç»¼åˆæ€§èƒ½ã€‚

![](/public/upload/machine/vllm_arch.jpg)

1. vLLMæ˜¯ä¸€ä¸ªå¼€æºçš„å¤§æ¨¡å‹æ¨ç†åŠ é€Ÿæ¡†æ¶ï¼Œé€šè¿‡PagedAttentioné«˜æ•ˆåœ°ç®¡ç†attentionä¸­ç¼“å­˜çš„å¼ é‡ï¼Œå®ç°äº†æ¯”HuggingFace Transformersé«˜14-24å€çš„ååé‡ã€‚
2. NVIDIA FasterTransformer (FT) æ˜¯ä¸€ä¸ªç”¨äºå®ç°åŸºäºTransformerçš„ç¥ç»ç½‘ç»œæ¨ç†çš„åŠ é€Ÿå¼•æ“ã€‚å®ƒåŒ…å«Transformerå—çš„é«˜åº¦ä¼˜åŒ–ç‰ˆæœ¬çš„å®ç°ï¼Œå…¶ä¸­åŒ…å«ç¼–ç å™¨å’Œè§£ç å™¨éƒ¨åˆ†ã€‚ä½¿ç”¨æ­¤æ¨¡å—ï¼Œæ‚¨å¯ä»¥è¿è¡Œç¼–ç å™¨-è§£ç å™¨æ¶æ„æ¨¡å‹ï¼ˆå¦‚ï¼šT5ï¼‰ã€ä»…ç¼–ç å™¨æ¶æ„æ¨¡å‹ï¼ˆå¦‚ï¼šBERTï¼‰å’Œä»…è§£ç å™¨æ¶æ„æ¨¡å‹ï¼ˆå¦‚ï¼šGPTï¼‰çš„æ¨ç†ã€‚FTæ¡†æ¶æ˜¯ç”¨C++/CUDAç¼–å†™çš„ï¼Œä¾èµ–äºé«˜åº¦ä¼˜åŒ–çš„ cuBLASã€cuBLASLt å’Œ cuSPARSELt åº“ï¼Œè¿™ä½¿æ‚¨å¯ä»¥åœ¨ GPU ä¸Šè¿›è¡Œå¿«é€Ÿçš„ Transformer æ¨ç†ã€‚ä¸ NVIDIA TensorRT ç­‰å…¶ä»–ç¼–è¯‘å™¨ç›¸æ¯”ï¼ŒFT çš„æœ€å¤§ç‰¹ç‚¹æ˜¯å®ƒæ”¯æŒä»¥åˆ†å¸ƒå¼æ–¹å¼è¿›è¡Œ Transformer å¤§æ¨¡å‹æ¨ç†ã€‚åœ¨åº•å±‚ï¼ŒèŠ‚ç‚¹é—´æˆ–èŠ‚ç‚¹å†…é€šä¿¡ä¾èµ–äº MPI ã€ NVIDIA NCCLã€Glooç­‰ã€‚å› æ­¤ï¼Œä½¿ç”¨FasterTransformerï¼Œæ‚¨å¯ä»¥åœ¨å¤šä¸ª GPU ä¸Šä»¥å¼ é‡å¹¶è¡Œè¿è¡Œå¤§å‹Transformerï¼Œä»¥å‡å°‘è®¡ç®—å»¶è¿Ÿã€‚åŒæ—¶ï¼ŒTP å’Œ PP å¯ä»¥ç»“åˆåœ¨ä¸€èµ·ï¼Œåœ¨å¤š GPU èŠ‚ç‚¹ç¯å¢ƒä¸­è¿è¡Œå…·æœ‰æ•°åäº¿ã€æ•°ä¸‡äº¿ä¸ªå‚æ•°çš„å¤§å‹ Transformer æ¨¡å‹ã€‚
3. DeepSpeed-MII æ˜¯ DeepSpeed çš„ä¸€ä¸ªæ–°çš„å¼€æº Python åº“ï¼Œæ—¨åœ¨ä½¿æ¨¡å‹ä¸ä»…ä½å»¶è¿Ÿå’Œä½æˆæœ¬æ¨ç†ï¼Œè€Œä¸”è¿˜æ˜“äºè®¿é—®ã€‚

å½“å‰çš„ç”Ÿæˆå¼å¤§æ¨¡å‹çš„æ¨ç†å¯ä»¥åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šContext é˜¶æ®µå’Œ Generation é˜¶æ®µã€‚Context é˜¶æ®µæ˜¯æ‰¹é‡è®¡ç®—è¾“å…¥çš„ Promptï¼Œå±äºè®¡ç®—å¯†é›†å‹ã€‚Generation é˜¶æ®µæ˜¯é€å­—ç”Ÿæˆä¸‹ä¸€ä¸ª Tokenï¼Œå±äºè®¿å­˜å¯†é›†å‹ï¼Œè™½ç„¶æ¯ä¸€è½® Generation çš„è®¡ç®—é‡å°äº Context é˜¶æ®µï¼Œä½†æ˜¯è®¿å­˜é‡ç›¸å½“ã€‚å¤§æ¨¡å‹æ¨ç†ä¸»è¦é¢ä¸´ä¸‰ä¸ªæŒ‘æˆ˜ï¼šè¾“å…¥è¾“å‡ºå˜é•¿ã€è®¡ç®—è§„æ¨¡å¤§ã€æ˜¾å­˜å ç”¨å¤§ï¼Œé’ˆå¯¹è¿™äº›æŒ‘æˆ˜å½“å‰æœ‰å¤šç§ä¼˜åŒ–æ‰‹æ®µè¿›è¡Œä¼˜åŒ–ï¼š
1. æœåŠ¡å±‚é¢ï¼Œæ‰“ç ´ä¹‹å‰çš„ Batch åªèƒ½åŒæ—¶è¿”å›ç»“æœçš„é™åˆ¶ï¼Œå…è®¸éƒ¨åˆ†è¯·æ±‚ç»“æŸåæ’å…¥æ–°çš„è¯·æ±‚ã€‚
2. è®¡ç®—æ–¹é¢ï¼Œä¹Ÿæœ‰ä¸€äº›ç®—å­èåˆï¼ŒKV Cache è¿™æ ·çš„æ— æŸåŠ é€Ÿæ–¹æ¡ˆï¼Œä¹Ÿæœ‰æ¨¡å‹é‡åŒ–åŠ é€Ÿæ–¹æ¡ˆï¼Œæ¯”å¦‚ Smooth Quant é‡åŒ–æ–¹æ¡ˆå°†æ¿€æ´»å’Œæƒé‡çš„åˆ†å¸ƒè¿›è¡Œå¹³è¡¡æ¥é™ä½æ¨¡å‹ç²¾åº¦æŸå¤±ã€‚
3. æ˜¾å­˜æ–¹é¢ï¼ŒGeneration è®¡ç®—çš„è®¿å­˜å¯†é›†å‹å¯ä»¥é€šè¿‡ Flash Attention ä¼˜åŒ–è®¿å­˜ï¼Œä¹Ÿå¯ä»¥é€šè¿‡ Paged Attention æ–¹æ³•ä¼˜åŒ–æ¨ç†è¿‡ç¨‹æ˜¾å­˜å ç”¨ä»è€Œæ”¯æŒæ›´å¤§çš„ååã€‚
    1. å¯¹äºè¾ƒçŸ­çš„æ–‡æœ¬è¾“å…¥ (è¯å…ƒæ•°å°äº 1024)ï¼Œæ¨ç†çš„å†…å­˜éœ€æ±‚å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºæ¨¡å‹æƒé‡çš„å¤§å°ã€‚


### å½±å“å› ç´ 

[è¯­è¨€å¤§æ¨¡å‹æ¨ç†æ€§èƒ½å·¥ç¨‹ï¼šæœ€ä½³å®è·µ](https://mp.weixin.qq.com/s/mniKrBWkDE1tWWb2wQBDCA)
æˆ‘ä»¬åº”è¯¥å¦‚ä½•å‡†ç¡®è¡¡é‡æ¨¡å‹çš„æ¨ç†é€Ÿåº¦å‘¢ï¼Ÿé¦–ä¸ªè¯å…ƒç”Ÿæˆæ—¶é—´ï¼ˆTime To First Tokenï¼Œç®€ç§°TTFTï¼‰ï¼›å•ä¸ªè¾“å‡ºè¯å…ƒçš„ç”Ÿæˆæ—¶é—´ï¼›æ—¶å»¶ï¼šæ¨¡å‹ä¸ºç”¨æˆ·ç”Ÿæˆå®Œæ•´å“åº”æ‰€éœ€çš„æ€»æ—¶é—´ï¼›ååé‡ï¼šæ¨ç†æœåŠ¡å™¨åœ¨æ‰€æœ‰ç”¨æˆ·å’Œè¯·æ±‚ä¸­æ¯ç§’å¯ç”Ÿæˆçš„è¾“å‡ºè¯å…ƒæ•°ã€‚
ä»¥ä¸‹é€šç”¨æŠ€æœ¯å¯ç”¨äºä¼˜åŒ–è¯­è¨€å¤§æ¨¡å‹çš„æ¨ç†ï¼š
1. ç®—å­èåˆï¼šå°†ç›¸é‚»çš„ä¸åŒç®—å­åˆå¹¶åœ¨ä¸€èµ·é€šå¸¸å¯ä»¥è·å¾—æ›´çŸ­çš„æ—¶å»¶ã€‚
2. é‡åŒ–ï¼šå¯¹æ¿€æ´»å€¼å’Œæƒé‡è¿›è¡Œå‹ç¼©ï¼Œä»¥ä½¿ç”¨æ›´å°‘çš„æ¯”ç‰¹æ•°ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæ‰€æœ‰é‡åŒ–æŠ€æœ¯çš„å·¥ä½œåŸç†å¦‚ä¸‹: $Y=X*W$ å˜æˆ $Y=X* dequantize(W); quantize(W)$ï¼Œå½“è¾“å…¥å‘é‡èµ°è¿‡æ¨¡å‹è®¡ç®—å›¾æ—¶ï¼Œæ‰€æœ‰æƒé‡çŸ©é˜µéƒ½ä¼šä¾æ¬¡æ‰§è¡Œåé‡åŒ–å’Œé‡é‡åŒ–æ“ä½œã€‚å› æ­¤ï¼Œä½¿ç”¨æƒé‡é‡åŒ–æ—¶ï¼Œæ¨ç†æ—¶é—´é€šå¸¸ ä¸ä¼š å‡å°‘ï¼Œåè€Œä¼šå¢åŠ ã€‚
3. å‹ç¼©ï¼šç¨€ç–æ€§æˆ–è’¸é¦ã€‚
4. å¹¶è¡ŒåŒ–ï¼šåœ¨å¤šä¸ªè®¾å¤‡é—´è¿›è¡Œå¼ é‡å¹¶è¡Œï¼Œæˆ–è€…é’ˆå¯¹è¾ƒå¤§çš„æ¨¡å‹è¿›è¡Œæµæ°´çº¿å¹¶è¡Œã€‚
é™¤ä¸Šè¿°æ–¹æ³•ä»¥å¤–ï¼Œè¿˜æœ‰è®¸å¤šé’ˆå¯¹Transformerçš„é‡è¦ä¼˜åŒ–æŠ€æœ¯ï¼Œå¦‚KVï¼ˆé”®-å€¼ï¼‰ç¼“å­˜ã€‚[Transformeræ¨ç†æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯å¾ˆé‡è¦çš„ä¸€ä¸ªå°±æ˜¯K V cacheï¼Œèƒ½å¦é€šä¿—åˆ†æï¼Œå¯ä»¥ç»“åˆä»£ç ? - çœ‹å›¾å­¦çš„å›ç­” - çŸ¥ä¹](https://www.zhihu.com/question/596900067/answer/3257946543)

åœ¨LLMä¸­ï¼Œè®¡ç®—ä¸»è¦ç”±çŸ©é˜µä¹˜æ³•è®¡ç®—ä¸»å¯¼ï¼›è¿™äº›ç»´åº¦è¾ƒå°çš„è®¡ç®—åœ¨å¤§å¤šæ•°ç¡¬ä»¶ä¸Šé€šå¸¸å—å†…å­˜å¸¦å®½çš„é™åˆ¶ã€‚åœ¨ä»¥è‡ªå›å½’æ–¹å¼ç”Ÿæˆè¯å…ƒæ—¶ï¼Œæ¿€æ´»çŸ©é˜µçš„ç»´åº¦ä¹‹ä¸€ï¼ˆç”±æ‰¹å¤§å°å’Œåºåˆ—ä¸­çš„è¯å…ƒæ•°å®šä¹‰ï¼‰åœ¨å°å‹æ‰¹å¤§å°ä¸Šè¾ƒå°ã€‚å› æ­¤ï¼Œé€Ÿåº¦ç”±æˆ‘ä»¬å°†æ¨¡å‹å‚æ•°ä»GPUå†…å­˜åŠ è½½åˆ°æœ¬åœ°ç¼“å­˜/å¯„å­˜å™¨ä¸­çš„é€Ÿåº¦å†³å®šï¼Œè€Œä¸æ˜¯ç”±è®¡ç®—åŠ è½½æ•°æ®çš„é€Ÿåº¦å†³å®šã€‚ç›¸æ¯”å³°å€¼è®¡ç®—æ€§èƒ½ï¼Œæ¨ç†ç¡¬ä»¶ä¸­å¯ç”¨å’Œå¯å®ç°çš„å†…å­˜å¸¦å®½èƒ½å¤Ÿæ›´å¥½åœ°é¢„æµ‹è¯å…ƒçš„ç”Ÿæˆé€Ÿåº¦ã€‚

å¯¹äºæœåŠ¡æˆæœ¬æ¥è¯´ï¼Œæ¨ç†ç¡¬ä»¶çš„åˆ©ç”¨ç‡éå¸¸é‡è¦ã€‚ç”±äºGPUçš„ä»·æ ¼ååˆ†é«˜æ˜‚ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦å°½å¯èƒ½åœ°è®©å®ƒä»¬å®Œæˆæ›´å¤šå·¥ä½œã€‚å…±äº«æ¨ç†æœåŠ¡é€šè¿‡å°†å¤šä¸ªç”¨æˆ·çš„å·¥ä½œè´Ÿè½½ç»„åˆåœ¨ä¸€èµ·ï¼Œå¡«è¡¥å„è‡ªçš„å·®è·ï¼Œå¹¶å°†é‡å çš„è¯·æ±‚è¿›è¡Œæ‰¹å¤„ç†ï¼Œä»¥é™ä½æˆæœ¬ã€‚å¯¹äºLLaMA2-70Bç­‰å¤§å‹æ¨¡å‹ï¼Œåªæœ‰åœ¨è¾ƒå¤§çš„æ‰¹å¤§å°ä¸‹æ‰èƒ½å®ç°æ›´å¥½çš„æ€§ä»·æ¯”ã€‚æ‹¥æœ‰èƒ½å¤Ÿä»¥è¾ƒå¤§æ‰¹å¤§å°è¿è¡Œçš„æ¨ç†æœåŠ¡ç³»ç»Ÿå¯¹äºæˆæœ¬æ•ˆç‡è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œè¾ƒå¤§çš„æ‰¹å¤§å°æ„å‘³ç€è¾ƒå¤§çš„KVç¼“å­˜ï¼Œè¿™åè¿‡æ¥åˆå¢åŠ äº†éƒ¨ç½²æ¨¡å‹æ‰€éœ€çš„GPUæ•°é‡ã€‚æˆ‘ä»¬éœ€è¦åœ¨è¿™ä¸¤è€…ä¹‹é—´åšå¼ˆï¼Œè¿›è¡Œå–èˆï¼Œå…±äº«æœåŠ¡è¿è¥å•†éœ€è¦æƒè¡¡æˆæœ¬ï¼Œä¼˜åŒ–ç³»ç»Ÿã€‚

### åœ¨çº¿æ¨ç†

[æ­ç§˜å¤§è¯­è¨€æ¨¡å‹å®è·µï¼šåˆ†å¸ƒå¼æ¨ç†çš„å·¥ç¨‹åŒ–è½åœ°æ‰æ˜¯å…³é”®ï¼](https://mp.weixin.qq.com/s/QeDmD-XlvkkJ7LMNJEynHg)ä¸ä»¥å¾€çš„æ¨¡å‹ä¸åŒï¼Œå•å¼  GPU å¡çš„æ˜¾å­˜å¯èƒ½ä¸è¶³ä»¥æ”¯æ’‘å¤§è¯­è¨€æ¨¡å‹ã€‚å› æ­¤ï¼Œéœ€è¦ä½¿ç”¨æ¨¡å‹å¹¶è¡ŒæŠ€æœ¯ï¼Œå°†å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œåˆ‡åˆ†åï¼Œåœ¨å¤šå¼  GPU å¡ä¸Šè¿›è¡Œæ¨ç†ã€‚æˆ‘ä»¬ä½¿ç”¨ DeepSpeed Inference æ¥éƒ¨ç½²å¤§è¯­è¨€æ¨¡å‹åˆ†å¸ƒå¼æ¨ç†æœåŠ¡ã€‚DeepSpeed Inference æ˜¯ Microsoft æä¾›çš„åˆ†å¸ƒå¼æ¨ç†è§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿå¾ˆå¥½çš„æ”¯æŒ transformer ç±»å‹çš„å¤§è¯­è¨€æ¨¡å‹ã€‚ã€‚DeepSpeed Inference æä¾›äº†æ¨¡å‹å¹¶è¡Œèƒ½åŠ›ï¼Œåœ¨å¤š GPU ä¸Šå¯¹å¤§æ¨¡å‹å¹¶è¡Œæ¨ç†ã€‚é€šè¿‡å¼ é‡å¹¶è¡ŒæŠ€æœ¯åŒæ—¶åˆ©ç”¨å¤šä¸ª GPUï¼Œæé«˜æ¨ç†æ€§èƒ½ã€‚DeepSpeed è¿˜æä¾›äº†ä¼˜åŒ–è¿‡çš„æ¨ç†å®šåˆ¶å†…æ ¸æ¥æé«˜ GPU èµ„æºåˆ©ç”¨ç‡ï¼Œé™ä½æ¨ç†å»¶è¿Ÿã€‚

æœ‰äº†å¤§æ¨¡å‹åˆ†å¸ƒå¼æ¨ç†æ–¹æ¡ˆï¼Œç„¶è€Œæƒ³è¦åœ¨ Kubernetes é›†ç¾¤ä¸­é«˜æ•ˆéƒ¨ç½²å¤§æ¨¡å‹æ¨ç†æœåŠ¡ï¼Œè¿˜å­˜åœ¨å¾ˆå¤šå·¥ç¨‹åŒ–æŒ‘æˆ˜ï¼Œæ¯”å¦‚å¤§è§„æ¨¡çš„ GPU ç­‰å¼‚æ„èµ„æºå¦‚ä½•é«˜æ•ˆåœ°ç®¡ç†è¿ç»´å’Œè‡ªåŠ¨è°ƒåº¦ï¼Ÿå¦‚ä½•å¿«é€Ÿéƒ¨ç½²æ¨ç†æœåŠ¡ï¼ŒæœåŠ¡ä¸Šçº¿åå¦‚ä½•ä¿è¯èµ„æºèƒ½å¤Ÿåº”å¯¹æ³¢åŠ¨çš„è®¿é—®é‡ï¼Ÿä»¥åŠæ²¡æœ‰é€‚åˆçš„å·¥å…·è¿›è¡Œæ¨ç†æœåŠ¡æ—¶å»¶ã€ååã€GPU åˆ©ç”¨ç‡ã€æ˜¾å­˜å ç”¨ç­‰å…³é”®æŒ‡æ ‡ç›‘æ§ï¼Œæ²¡æœ‰åˆç†çš„æ¨¡å‹åˆ‡åˆ†æ–¹æ¡ˆï¼Œæ¨¡å‹ç‰ˆæœ¬ç®¡ç†ç­‰ã€‚

[å¤§æ¨¡å‹çš„å¥½ä¼™ä¼´ï¼Œæµ…ææ¨ç†åŠ é€Ÿå¼•æ“FasterTransformer](https://mp.weixin.qq.com/s/Gkf_zIYWs4u7AJrJLDVq_Q) æœªç»†è¯»
FasterTransformer æ˜¯çœŸå¯¹äº Transofrmer ç±»å‹æ¨¡å‹ï¼ˆä¹ŸåŒ…æ‹¬ encoder-onlyã€decoder-onlyï¼‰çš„æ¨ç†åŠ é€Ÿæ–¹æ¡ˆï¼Œå…¶æä¾›äº† Kernel Fuseã€Memory reuseã€kv cacheã€é‡åŒ–ç­‰å¤šç§ä¼˜åŒ–æ–¹æ¡ˆï¼ŒåŒæ—¶ä¹Ÿæä¾›äº† Tensor Parallel å’Œ Pipeline Parallel ä¸¤ç§åˆ†å¸ƒå¼æ¨ç†æ–¹æ¡ˆã€‚

### åˆ†å¸ƒå¼æ¨ç†

1. åœ¨æå‡æ¨¡å‹æ˜¾å­˜ä½¿ç”¨æ•ˆç‡æ–¹é¢ï¼ŒFlash Attention å’Œ Paged Attention æ˜¯ä¸¤ç§å¸¸ç”¨çš„æ–¹æ³•ã€‚åœ¨è¾“å…¥åºåˆ—ä¸­ï¼Œæ¨¡å‹ä¼šæ ¹æ®æ¯ä¸ªè¯çš„é‡è¦æ€§æ¥åˆ†é…æ˜¾å­˜ã€‚å¯¹äºé‡è¦æ€§è¾ƒé«˜çš„è¯ï¼Œæ¨¡å‹ä¼šåˆ†é…æ›´å¤šçš„æ˜¾å­˜ç©ºé—´æ¥å­˜å‚¨å…¶ä¿¡æ¯ï¼›è€Œå¯¹äºé‡è¦æ€§è¾ƒä½çš„è¯ï¼Œæ¨¡å‹åˆ™ä¼šåˆ†é…è¾ƒå°‘çš„æ˜¾å­˜ç©ºé—´ã€‚
2. é‡åŒ–ã€‚é‡åŒ–è¿‡ç¨‹ä¸»è¦æ¶‰åŠä¸¤ä¸ªæ–¹é¢ï¼šå‚æ•°ç¯èŠ‚çš„å°å‹åŒ–å’Œé™ä½æ•°æ®ç±»å‹ã€‚é€šè¿‡è¿™ä¸€æ­¥éª¤ï¼Œæˆ‘ä»¬èƒ½å¤Ÿä½¿å¾—æ¨¡å‹åŠ è½½çš„å‚æ•°æ›´å°ï¼Œä»åŸæœ¬çš„ FP32 é™ä½åˆ° FP16ï¼Œä»è€Œæé«˜æ¨ç†æ€§èƒ½ã€‚åœ¨é‡åŒ–è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬è¿˜ä¼šé‡‡ç”¨æ··åˆç²¾åº¦é‡åŒ–æŠ€æœ¯ã€‚è¿™ç§æŠ€æœ¯èƒ½å¤Ÿåœ¨ä¿è¯æ¨¡å‹å‡†ç¡®æ€§çš„å‰æä¸‹ï¼Œå°†å¼‚å¸¸å€¼ä¿ç•™ç²¾åº¦ï¼Œå¹¶åœ¨æ··åˆç²¾åº¦åˆ†å—çŸ©é˜µæœ€åå†åŠ å›å»ã€‚
3. æ¨¡å‹ç¨€ç–åŒ–ã€‚æ¨¡å‹ç¨€ç–åŒ–æ˜¯ä¸€ç§é‡è¦çš„ä¼˜åŒ–æ–¹æ³•ã€‚å®ƒçš„ä¸»è¦ç›®çš„æ˜¯å‡å°‘æ¨¡å‹å‚æ•°çš„æ•°é‡ï¼Œä»è€Œé™ä½æ¨¡å‹çš„å¤æ‚åº¦ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œè®¡ç®—æ•ˆç‡ã€‚æ¨¡å‹ç¨€ç–åŒ–çš„ä¸»è¦æ–¹æ³•æœ‰å‰ªæã€é‡åŒ–ã€ä½ç§©è¿‘ä¼¼ç­‰ã€‚å‰ªææ˜¯ä¸€ç§ç›´æ¥åˆ é™¤æ¨¡å‹ä¸­éƒ¨åˆ†å‚æ•°çš„æ–¹æ³•ï¼Œå®ƒå¯ä»¥æœ‰æ•ˆåœ°å‡å°‘æ¨¡å‹çš„è§„æ¨¡ï¼Œä½†éœ€è¦æ³¨æ„ä¸èƒ½è¿‡åº¦å‰ªæï¼Œä»¥å…å½±å“æ¨¡å‹çš„æ€§èƒ½ã€‚ä½ç§©è¿‘ä¼¼åˆ™æ˜¯é€šè¿‡å°†æ¨¡å‹è½¬æ¢ä¸ºä½ç§©çŸ©é˜µï¼Œæ¥å‡å°‘æ¨¡å‹çš„å‚æ•°æ•°é‡ã€‚

## æ¨¡å‹æœåŠ¡æ¡†æ¶

ä½¿ç”¨å¤§æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬åœ¨huggingfaceæˆ–modelscope çœ‹åˆ°çš„ä»£ç ç±»ä¼¼ä¸‹é¢ï¼Œå¾ˆæ˜æ˜¾ä¸èƒ½ç›´æ¥å‘ç”¨æˆ·æä¾›æœåŠ¡ã€‚ 
```python
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True)
model = AutoModel.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True).half().cuda()
model = model.eval()
response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
print(response)
ä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚
response, history = model.chat(tokenizer, "æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ", history=history)
print(response)
æ™šä¸Šç¡ä¸ç€å¯èƒ½ä¼šè®©ä½ æ„Ÿåˆ°ç„¦è™‘æˆ–ä¸èˆ’æœ,ä½†ä»¥ä¸‹æ˜¯ä¸€äº›å¯ä»¥å¸®åŠ©ä½ å…¥ç¡çš„æ–¹æ³•:...
```

ä¸€èˆ¬æœ‰å‡ ä¸ªéœ€æ±‚
1. ç»Ÿä¸€apiï¼Œè¿™æ ·åˆ‡æ¢æ¨¡å‹æ—¶ä¸Šæ¸¸åº”ç”¨æ— æ„Ÿï¼Œæœ€å¥½æ˜¯ OpenAI-compatibleï¼Œå…¶api è¢«ä¸»è¦ä¸Šæ¸¸æ¡†æ¶ï¼ˆæ¯”å¦‚langchainï¼‰å…¼å®¹
    1. æ”¯æŒæµå¼è¾“å‡ºå’Œæ™®é€šè¾“å‡º
2. æ”¯æŒå¤šå®ä¾‹ï¼Œè¿›è€Œæ”¯æŒç°åº¦å‘å¸ƒç­‰
3. æ”¯æŒé€šç”¨çš„åŠ é€Ÿåº“æ¯”å¦‚vllmç­‰

### ç®€å•å°è£…

[ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) åœ¨github æœ‰ä¸€ä¸ªä»“åº“ï¼Œä¸€èˆ¬åŒ…å«
1. æ¨¡å‹ä»‹ç» README.md
2. æ¨¡å‹çš„å¯¹å¤–æ¥å£ api.py/cli_demo.py/web_demo.pyã€‚ è‡ªå·±ä½¿ç”¨ fastapi åŸºäºpythonåº“ç›´æ¥å¯¹å¤–æä¾›RESTful APIs.

ä»¥api.py ä¸ºä¾‹
```python
from fastapi import FastAPI, Request
from transformers import AutoTokenizer, AutoModel
import uvicorn, json, datetime
import torch

app = FastAPI()

@app.post("/")
async def create_item(request: Request):
    global model, tokenizer
    json_post_raw = await request.json()
    json_post = json.dumps(json_post_raw)
    json_post_list = json.loads(json_post)
    prompt = json_post_list.get('prompt')
    history = json_post_list.get('history')
    max_length = json_post_list.get('max_length')
    top_p = json_post_list.get('top_p')
    temperature = json_post_list.get('temperature')
    response, history = model.chat(tokenizer,prompt, history=history,
                                   max_length=max_length if max_length else 2048,
                                   top_p=top_p if top_p else 0.7,
                                   temperature=temperature if temperature else 0.95)
    now = datetime.datetime.now()
    time = now.strftime("%Y-%m-%d %H:%M:%S")
    answer = {"response": response,"history": history,"status": 200,"time": time}
    log = "[" + time + "] " + '", prompt:"' + prompt + '", response:"' + repr(response) + '"'
    print(log)
    torch_gc()
    return answer

if __name__ == '__main__':
    tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)
    model = AutoModel.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True).half().cuda()
    model.eval()
    uvicorn.run(app, host='0.0.0.0', port=8000, workers=1)
```

### FastChat

[ä¸€æ–‡å…¥é—¨æœ€çƒ­çš„LLMåº”ç”¨å¼€å‘æ¡†æ¶LangChain](https://mp.weixin.qq.com/s/bYzNNL3F0998Do2Jl0PQtw)

FastChatåŠŸèƒ½è¦†ç›–è®­ç»ƒï¼Œæ¨ç†ï¼Œè¯„ä¼°çš„å…¨è¿‡ç¨‹ã€‚è®¾è®¡ç›®æ ‡éå¸¸æ˜ç¡®ï¼Œå°±æ˜¯åœ¨æ€§èƒ½ã€åŠŸèƒ½åŠé£æ ¼ä¸Šå…¨é¢å¯¹æ ‡OpenAI ChatGPTï¼Œä»¥æˆä¸ºChatGPTçš„å¼€æºå¹³æ›¿ã€‚åœ¨ç”Ÿæ€é›†æˆä¸Šï¼Œç”±äºå®ƒå®Œå…¨å…¼å®¹OpenAIçš„é£æ ¼ï¼ŒåŸºäºChatGPTçš„langchainåº”ç”¨ï¼Œå¯ä»¥æ— ç¼åœ°ä½¿ç”¨FastChatæ›¿ä»£ã€‚ æ¨ç†ä¾§ç±»ä¼¼å·¥å…·Xinference/OpenLLM/RayLLM

[FastChat](https://github.com/lm-sys/FastChat)æ˜¯ä¸€ä¸ªç”¨äºè®­ç»ƒã€æœåŠ¡å’Œè¯„ä¼°åŸºäºèŠå¤©æœºå™¨äººçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„å¼€æ”¾å¹³å°ã€‚The core features include:
1. The training and evaluation code for state-of-the-art models (e.g., Vicuna).
2. A distributed multi-model serving system with web UI and OpenAI-compatible RESTful APIs.


```sh
# å‘½ä»¤è¡Œæ–¹å¼ä¸llm äº¤äº’
python3 -m fastchat.serve.cli --model-path lmsys/vicuna-7b-v1.3
# webuiæ–¹å¼ä¸llmäº¤äº’ï¼Œæ­¤æ—¶éœ€å¯åŠ¨3ä¸ªç»„ä»¶ web servers ==> controller ==> model workers
python3 -m fastchat.serve.controller
python3 -m fastchat.serve.model_worker --model-path lmsys/vicuna-7b-v1.3
python3 -m fastchat.serve.gradio_web_server
# æä¾›OpenAI-compatible RESTful APIs  openai_api_server ==> controller ==> model workers
python3 -m fastchat.serve.controller
python3 -m fastchat.serve.model_worker --model-path lmsys/vicuna-7b-v1.3
python3 -m fastchat.serve.openai_api_server --host localhost --port 8000
```

è®¾è®¡æ€è·¯
1. å› ä¸ºè¦æ”¯æŒä¸åŒçš„llm åº“æˆ–åŠ é€Ÿåº“ï¼Œæ¯”å¦‚Transformerã€vllmç­‰ï¼Œå› æ­¤æ¨ç†ä¾§å¿…é¡»æœ‰ä¸€ä¸ªç»Ÿä¸€çš„LLM æŠ½è±¡ï¼Œåœ¨Fastchaté‡Œæ˜¯XXModelWorkerï¼Œåœ¨xinference é‡Œæ˜¯XXLLM
2. å°†python llm åº“ apiåŒ–ï¼Œä¸€ä¸ªapi è¦æœ‰ä¸€ä¸ªapi handler å‡½æ•°ï¼Œä¸€èˆ¬æŠ½è±¡ä¸ºä¸€ä¸ªå¯¹è±¡ ä½œä¸ºapi handlerçš„è½½ä½“ï¼Œè¿™ä¸ªå¯¹è±¡æŒæœ‰ä¸Šé¢çš„XxLLM æ‰§è¡Œchat/generate æ–¹æ³•ï¼Œæœ‰æ—¶å€™è¿˜è¦æ”¯æŒ/å°è£…åˆ†å¸ƒå¼ã€å¼‚æ­¥ç­‰ç»†èŠ‚ã€‚åœ¨Fastchaté‡Œæ˜¯ModelWorkerï¼Œåœ¨xinference é‡Œæ˜¯WorkerActor
3. ä¸åŒçš„llm è¿˜æœ‰å¾ˆå¤šå·®åˆ«çš„ï¼ˆæ¯”å¦‚åŠ è½½ load_modelã€è¿è¡Œchat/generateã€æ¨¡å‹é…ç½®è½¬æ¢ï¼‰ï¼Œä¹Ÿæœ‰å¾ˆå¤šå…±æ€§ï¼Œæ‰€ä»¥æ¨¡å‹è®¾è®¡çš„åˆ†å±‚æŠ½è±¡å¾ˆé‡è¦ï¼ŒFastchat çš„æ€è·¯æ˜¯ æä¾›äº†ä¸€ä¸ªModelAdapterï¼ˆä¸»è¦å·®å¼‚åŒ–äº†åŠ è½½ï¼‰ å’Œä¸€ä¸ª generate_stream_gate å‡½æ•°æˆå‘˜ï¼ˆå·®å¼‚åŒ–ç”Ÿæˆï¼‰ï¼Œinferenceçš„æ€è·¯æ˜¯ä¸€ä¸ªæ¨¡å‹ä¸€ä¸ªXXLLM
  1. è¿™é‡Œçš„æ¨¡å‹é…ç½®è½¬æ¢è¯´çš„æ˜¯ï¼Œæ¯”å¦‚ä¸€ä¸ªchat message åŒ…å«role å’Œcontent ä¸¤ä¸ªéƒ¨åˆ†ï¼Œrole=system/user/assistant å„å®¶å„æœ‰å·®å¼‚ï¼Œä½†å› ä¸ºå¯¹å¤–æä¾›çš„æ¥å£ä¸€èˆ¬æ˜¯openai é£æ ¼ï¼Œæ‰€ä»¥æœ‰ä¸€ä¸ªè½¬æ¢çš„è¿‡ç¨‹ã€‚

### FastChatæºç åˆ†æ

ä½¿ç”¨ModelWorker åŠ è½½model æä¾›http æ¥å£ 

```python
app = FastAPI()
@app.post("/worker_generate")
async def api_generate(request: Request):
    params = await request.json()
    await acquire_worker_semaphore()
    output = worker.generate_gate(params)
    release_worker_semaphore()
    return JSONResponse(output)
if __name__ == "__main__":
    ...
    worker = ModelWorker(...,args.model_path,)
    uvicorn.run(app, host=args.host, port=args.port, log_level="info")
```
ModelWorkerå®ç°
```python
BaseModelWorker
     init_heart_beat
         # å°†modelWorker idæ³¨å†Œåˆ°controllerï¼Œå¹¶ä¿æŒå¿ƒè·³ã€‚å‡é€šè¿‡httpæ¥å£

# åŠ è½½æ¨¡å‹ï¼Œè°ƒç”¨æ¨¡å‹ï¼ˆåº•å±‚éƒ½æ˜¯è°ƒç”¨æµå¼æ¥å£ï¼‰
ModelWorker
     def __init__():
          self.model, self.tokenizer = load_model(model_path, device=device,...)
            # load_model å¯¹åº”ä¸€ä¸ªä¸“é—¨çš„ ModelAdapter æŠ½è±¡ï¼Œç”¨æ¥é€‚é…æ¨¡å‹çš„åŠ è½½
            adapter = get_model_adapter(model_path)
            model, tokenizer = adapter.load_model(model_path, kwargs)
     generate_stream_gate(self, params) 
     generate_gate(self, params)    # æ ¹æ®å‚æ•°è¿”å›è¾“å‡ºï¼Œè°ƒç”¨generate_stream_gate
        for x in self.generate_stream_gate(params):
            pass
        return json.loads(x[:-1].decode())
```
api => ModelWorker.generate_gate ==> ModelWorker.generate_stream_gate ==> ModelWorker.model.stream_generate
```python
generate_stream_gate
    get_generate_stream_function(model: torch.nn.Module, model_path: str)
       # æ ¹æ®æ¨¡å‹ä¸åŒé€‰æ‹©å¯¹åº”çš„å‡½æ•° 
       generate_stream_chatglm
            prompt = params["prompt"]
            inputs = tokenizer([prompt], return_tensors="pt").to(model.device)
            for total_ids in model.stream_generate(**inputs, **gen_kwargs):
                  response = tokenizer.decode(output_ids)
                  response = process_response(response)
```


### FastChat, How to support a new model?

1. FastChat uses the Conversation class to handle prompt templates and BaseModelAdapter class to handle model loading.
2. Implement a conversation template for the new model at `fastchat/conversation.py`. You can follow existing examples and use register_conv_template to add a new one. Please also add a link to the official reference code if possible. PSï¼š æ¯•ç«Ÿfastcaht æœåŠ¡chat åœºæ™¯å˜›ï¼Œå¯¹è¯è¯·æ±‚ä¼ å…¥çš„æ—¶å€™ ä¸€èˆ¬æ˜¯ `prompt = "\n###user:å¤©ä¸ºä»€ä¹ˆè¿™ä¹ˆè“ï¼Ÿ\n###"`ï¼Œè¦æŠŠè¿™ä¸ªè¿˜åŸä¸º `history = [{"role": "user", "content": "å¤©ä¸ºä»€ä¹ˆè¿™ä¹ˆè“ï¼Ÿ"}]`ï¼Œä¸åŒçš„æ¨¡å‹ å¯¹roleçš„ç§°å‘¼ä¸åŒã€‚
3. Implement a model adapter for the new model at `fastchat/model/model_adapter.py`. You can follow existing examples and use register_model_adapter to add a new one. PSï¼šä¸åŒçš„æ¨¡å‹åŠ è½½æ—¶æœ‰ä¸€äº›ç‰¹å®šçš„å‚æ•°ï¼Œæ¯”å¦‚ chatglm çš„trust_remote_code å‚æ•°ï¼Œ`model = AutoModel.from_pretrained(model_path, trust_remote_code=True, **from_pretrained_kwargs)`
4. ModelWorker ä¸»è¦é€»è¾‘æ˜¯æ‰§è¡Œ `generate_stream(model,tokenizer,params)` ï¼Œå¾ˆå¸¸è§„çš„ `input_ids = tokenizer(prompt); output_ids = model(input_ids,xx)`ã€‚ å¦‚æœæ¨¡å‹çš„generate é€»è¾‘æœ‰ä¸€äº›ç‰¹åˆ«çš„å¤„ç†ï¼Œåˆ™éœ€è¦è‡ªå®šä¹‰generate_stream_xxï¼Œå¹¶åŠ å…¥get_generate_stream_function é€»è¾‘ï¼ˆæ ¹æ®æ¨¡å‹åç­‰ è·¯ç”±åˆ°ä¸åŒçš„generate_stream_xxï¼‰
5. (Optional) add the model name to the "Supported models" section above and add more information in `fastchat/model/model_registry.py.`

å¦‚ä½•ç†è§£FastChat éƒ½å¹²äº†ä»€ä¹ˆï¼Ÿæœ¬è´¨æ˜¯å¯¹ä¸‹é¢çš„ åŸå§‹çš„å¤§æ¨¡å‹æ¨ç†ä»£ç è¿›è¡ŒæŠ½è±¡ï¼ˆæ¨¡å‹åŠ è½½ã€æ¨¡å‹æ¨ç†=tokenizer+modelï¼‰å’Œå°è£…ï¼Œå¯¹å¤–æä¾›rest apiã€‚

```python
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True)
model = AutoModel.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True).half().cuda()
model = model.eval()
response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
print(response)
ä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚
response, history = model.chat(tokenizer, "æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ", history=history)
print(response)
æ™šä¸Šç¡ä¸ç€å¯èƒ½ä¼šè®©ä½ æ„Ÿåˆ°ç„¦è™‘æˆ–ä¸èˆ’æœ,ä½†ä»¥ä¸‹æ˜¯ä¸€äº›å¯ä»¥å¸®åŠ©ä½ å…¥ç¡çš„æ–¹æ³•:...
```