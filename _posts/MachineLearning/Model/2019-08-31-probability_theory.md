---

layout: post
title: 概率论
category: 架构
tags: MachineLearning
keywords:  概率论

---

## 简介

* TOC
{:toc}

机器学习模型本质上就是用模型表达的概率分布去刻画真实世界的概率分布。

[详解最大似然估计、最大后验概率估计及贝叶斯公式](https://mp.weixin.qq.com/s/_YND-c4wAwsz2IWyTzrqeA) 未读。

《成为AI产品经理》在概率统计中，我们最需要掌握的就是概率的分布。举个例子，我们在做一个预测用户评分的时候，这个分数可能是购买倾向，也可能是信用评分。按照经验，这个评分结果应该是符合正态分布的。这个时候，如果算法同学的模型预测出来的结果不符合正态分布，我们就必须对这个结果进行质疑，让他们给出合理的解释。从这个例子中我们知道，概率分布是我们用来评估特征数据和模型结果的武器。那产品经理怎么才能利用好这个武器呢？首先，我们要掌握常用的概率分布的类型。其次，我们还要知道业务场景下的特征数据和模型结果的分布，以及它们应该符合哪种分布类型。这样，产品经理就可以把概率分布应用于日常的工作中。

简单来说，概率是由因导果，统计是由果溯因。概率研究的是已知随机现象，然后分析和预测这个现象可能出现的结果，以及这些结果出现的可能性大小。统计则是在数据的基础上，研究如何收集、分析和解释数据，通过观测到的数据，反过来推断数据背后的模型或规律。可以用一个简单的例子来区分它们：假设我们有一个骰子，我们想知道掷出某个点数的概率。 
1. 概率：如果我们知道这个骰子是均匀的（每个面出现的概率相等），那么掷出任何一个点数的概率都是 1/6。这就是概率研究的问题，已知骰子的性质，预测结果的可能性。
2. 统计：如果我们不知道这个骰子是否均匀，我们需要多次投掷骰子，记录每个点数出现的次数，然后根据这些数据来推断骰子是否均匀，或者估计每个点数出现的概率。这就是统计研究的问题，通过数据反推模型的性质。
不过在更细的层面上，概率论中也有「反演」「逆概率」之类的问题（例如贝叶斯法则），而统计学中也会基于某些先验模型进行推断，二者并非截然割裂。

## 概率论

![](/public/upload/machine/probability_theory.png)

[为什么我说概率论是大学最不能翘的一门数学课](https://zhuanlan.zhihu.com/p/36920233)

假设你在一个教室里，同时周围有30名学生，游戏的规则是这样的：一开始，大家都是鸡蛋，你们需要和周围同类型的物种进行石头剪子布来升级，鸡蛋找鸡蛋划石头剪子布，胜利者就变成了小鸡。接着小鸡找小鸡划，胜利者就变成了凤凰，然后，凤凰和凤凰划，胜利者就变成了人，变成人的同学就是最后的赢家，同时，每次石头剪子布输的那一方都会降一级，比如从凤凰降级到小鸡，小鸡降级到鸡蛋，游戏限时十分钟，所以，现在问你在这个游戏中如何取胜（此处认真思考二十秒）

如果一个人在这十分钟内只进行一轮石头剪子布（我们现在定义由鸡蛋到人的三次石头剪子布为一轮，如果提前输掉则视作此轮结束）那么这个人变成人的概率为1/8，而如果它进行两轮，那么我们先不管他是在第一轮还是第二轮中变成了人，只要最后变成人了就OK，所以我们只用讨论它两轮都输的概率，就是（1-1/8）^2，而变成人的概率为1-（1-1/8）^2=15/64>1/8，依次类推，可知道如果进行n轮，则他变成人的概率为1-（1-1/8）^n,当n趋于无穷时，这个概率将趋于 1，所以这个游戏取胜的秘诀就是不停的和周围的人进行石头剪子布，频次越高，也就越有机会变成人。

对于大多数玩家而言，是不会仔细思考这个游戏背后的数学模型的。想对上述游戏进行一次完整而严谨的建模也将是一个十分复杂的过程。

[概率：了解不确定性](https://songshuhui.net/archives/93539)在1654年的一天早上，法国数学家布莱兹·帕斯卡收到了他的朋友贡博的一封来信：两位贵族A与B正在进行一场赌局，赌注是每人500法郎，两人轮流掷硬币，得到正面则A得一分，反面则B得一分，每一局两人得分的机会相等，谁先得到6分谁就得到1000法郎。两人激战正酣，比分达到2比4之际，B突然有事需要终止赌局。赌注应该如何分配才最公平。

对于某个非常简单的随机事件，比如说掷硬币，我们知道每种结果出现可能性的大小，这样的事件被称为“基本事件”。我们可以多次重复这些基本事件，假定它们发生的可能性不会改变，而且这些重复没有相互影响。如果我们将这些基本事件以合适的形式组合起来，就能得到一个更为复杂而有趣的系统。许多概率问题实际上就是对这些随机系统的各种性质的研究。

### 概率与自然语言处理

概率的定义就是随机事件发生的可能性的度量，而信息则是减少随机不定性的东西，这两者生而就是有着千丝万缕的联系的，所以我们在研究自然语言处理（本质是通信）这类信息时，也必然要引入概率的知识

### 贝叶斯公式的另类解读

```
P(AB) = P(A|B) * P(B) = P(B|A) * P(A)
```

$$P(A|B) = \frac{P(B|A) * P(A)}{P(B)} = \frac{P(B|A)}{P(B)} * P(A)$$

`P(A|B)`  可以视为 `P(A)` 的增强，也就是后验概率是先验概率的增强。**观测者观测历史数据得出预测假设P(A)，然后新的信息`P(B)`出现，观测者修正预测为`P(A|B)`**。`P(B|A)/P(B)` 被称为调整因子。

定义`P(A|B)`是P`(B|A)`的逆概率。贝叶斯最早的目的就是研究一个概率和他的逆概率之间的关系。

在实际的场景下， A 和 B 通常代表了一个结果和原因， `P(结果|原因)`好算，`P(原因|结果)`知果索因就很麻烦，**不然侦探片就没那么好看了**。

人们已经能够计算”正向概率“，如“一个袋子N个白球M个黑球，你伸手进去摸一把，摸出黑球的概率多大”。而一个自然而然的问题反过来：如果事先不知道袋子中黑白球比例，而是闭着眼睛摸出好几个球，观察这些取出来的球的颜色之后，那么我们可以对袋子中黑白球的比例做出怎样的推测？如果再摸出几个球，是否要对刚才的推测进行校正？

贝叶斯公式看着没什么感觉，但在贝叶斯分类中就很有用武之地了。比如我们将水果的形状、颜色、纹理、重量、握感、口感全部数值化。可以得到一个苹果、橙子是黄颜色的概率分别是多少，基于贝叶斯公式就可以反推一个黄色的、圆形水果是苹果、橙子的概率。前者是训练数据，后者是计算机根据贝叶斯分类算法做出的判断。

[怎么简单理解贝叶斯公式?半瓶晃荡加水中的回答 ](https://www.zhihu.com/question/51448623/answer/306116102)最重要的思维并非是逻辑思维，是直觉式思维。在学习过程中，必须把知识和直觉思维联系起来，才能真正掌握这个知识。直觉性思维的本质，是通过观察少量结果，反向推导出事物的因果联系。

### 概率论与数理统计

||概率论|数理统计|
|---|---|---|
|方法|根据已知的分布来分析随机变量的特征与规律|根据得到的观察结果对原始分布做出推断<br>具体的说是假定样本符合某一分布，估计分布对应概率函数的参数值|
|例子|已知摇奖规律判断一注号码中奖的可能性|根据之前多次中奖不中奖的号码以一定的精确性推断摇奖的规律|
|例子|已知人群身高符合正态分布，预测一个陌生人的身高|已知一个群体的身高， 且假设符合正态分布，推测正态分布的参数值|

不管是概率论，还是数理统计，分布对应的概率函数都是已知的。

对于`P(x|θ)`输入有两个：x表示某一个具体的数据；θ表示模型的参数。

1. 如果θ是已知确定的，x是变量，这个函数叫做概率函数(probability function)，它描述对于不同的样本点x，其出现概率是多少。
2. 如果x是已知确定的，θ是变量，这个函数叫做似然函数(likelihood function), 它描述对于不同的模型参数，出现x这个样本点的概率是多少，也可以用`P(x;θ)` 来表示

[详解最大似然估计（MLE）、最大后验概率估计（MAP），以及贝叶斯公式的理解](https://blog.csdn.net/u011508640/article/details/72815981)

1. 最大似然估计是求参数θ, 使似然函数`P(data|θ)`最大。换个说法，求使似然函数`P(data|θ)`最大的θ
2. 最大后验概率估计则是想求θ使`P(data|θ)P(θ)`最大。换个说法，求使`P(data|θ)P(θ)`最大的θ

假设有一个造币厂生产某种硬币，现在我们拿到了一枚这种硬币，想试试这硬币是不是均匀的。即想知道抛这枚硬币，正反面出现的概率（记为θ）各是多少？于是我们拿这枚硬币抛了10次，得到的数据（data）是：反正正正正反正正正反。我们可以假设抛硬币模型是**二项分布**，正面概率θ是模型参数，如何根据 data 求θ？

### 最大似然估计MLE

那么，出现实验结果data（即反正正正正反正正正反）的似然函数是多少呢？

$$P(data|θ)=(1−θ)×θ×θ×θ×θ×(1−θ)×θ×θ×θ×(1−θ)=θ^7(1−θ)^3=f(θ)$$

注意，这是个只关于θ的函数。而最大似然估计，顾名思义，就是要最大化这个函数。对应使得 f(data,θ) 导数为0 的点， 在θ=0.7时，似然函数取得最大值。

### 最大后验概率估计MAP

一些人可能会说，硬币一般都是均匀的啊！ 就算你做实验发现结果是“反正正正正反正正正反”，我也不信θ=0.7。如果一枚硬币抛10次，10次均为正面，根据最大似然估计，那么这枚硬币的概率应该为1。这也未免太武断了。

MAP其实是在最大化

$$P(θ|data)=\frac{P(data|θ)P(θ)}{P(data)}$$

不过因为data是确定的（即投出的“反正正正正反正正正反”），P(data)是一个已知值，所以去掉了分母P(data)。对于投硬币的例子来看，我们认为（”先验地知道“）θ取0.5的概率很大，取其他值的概率小一些。我们用一个高斯分布来具体描述我们掌握的这个先验知识，例如假设P(θ)为均值0.5，方差0.1的高斯函数

![](/public/upload/machine/gaussian_function.png)

$P(data|\theta)P(\theta)=\theta^7(1−\theta)^3P(\theta)$，则$P(data|\theta)P(\theta)$的函数图像为：

![](/public/upload/machine/gaussian_function_move.png)

在θ=0.558时函数取得了最大值。那要怎样才能说服一个贝叶斯派相信θ=0.7呢？你得多做点实验。如果做了1000次实验，其中700次都是正面向上，则在θ=0.696处，`P(data|θ)P(θ)`取得最大值。

[最大后验估计](https://zhuanlan.zhihu.com/p/32616870)此外再提两点：

1. 如果先验认为这个硬币是概率是**均匀分布**的，被称为无信息先验( non-informative prior )，通俗的说就是“让数据自己说话”，此时贝叶斯方法等同于频率方法。
2. 随着数据的增加，先验的作用越来越弱，数据的作用越来越强，参数的分布会向着最大似然估计靠拢。而且可以证明，最大后验估计的结果是先验和最大似然估计的凸组合。
