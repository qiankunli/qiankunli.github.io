---

layout: post
title: LLMé¢„è®­ç»ƒ
category: æŠ€æœ¯
tags: MachineLearning
keywords: llm pretrain

---

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$']], // æ”¯æŒ $å’Œ$$ ä½œä¸ºè¡Œå†…å…¬å¼åˆ†éš”ç¬¦
      displayMath: [['$$', '$$']], // å—çº§å…¬å¼åˆ†éš”ç¬¦
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script async src="/public/js/mathjax/es5/tex-mml-chtml.js"></script>

* TOC
{:toc}


## ç®€ä»‹


[LLama3 405B æŠ€æœ¯è§£è¯»](https://mp.weixin.qq.com/s/51h70Zg-bfbvnQWr6UH1ZQ)å¤§æ¨¡å‹ä¹‹æ‰€ä»¥èƒ½åŠ›ä»åœ¨å¿«é€Ÿæå‡ï¼Œä¸»è¦é©±åŠ¨åŠ›æœ‰ä¸‰ä¸ªï¼š
1. é¦–å…ˆå°±æ˜¯ä¸æ–­æ‰©å¤§æ¨¡å‹å’Œæ•°æ®è§„æ¨¡ï¼ˆScaling Lawï¼‰ã€‚
2. ä¸€ä¸ªæ˜¯è¶Šæ¥è¶Šå¼ºè°ƒæ•°æ®è´¨é‡çš„ä½œç”¨ï¼Œå„ç§æ•°æ®ç­›é€‰æ–¹æ³•å’Œå·¥å…·è¶Šæ¥è¶Šå¤šï¼Œä¿è¯è´¨é‡æ˜¯ç¬¬ä¸€ä½çš„
3. ä¸æ–­å¢åŠ æ•°å­¦ã€é€»è¾‘ã€ä»£ç è¿™ç§èƒ½å¤Ÿæå‡å¤§æ¨¡å‹ç†æ€§èƒ½åŠ›çš„æ•°æ®é…æ¯”æ¯”ä¾‹ï¼ŒåŒ…æ‹¬åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼ˆå¢åŠ é¢„è®­ç»ƒæ•°æ®æ­¤ç±»æ•°æ®æ¯”ä¾‹ï¼Œä¸”åœ¨é¢„è®­ç»ƒåé¢é˜¶æ®µæ¥ä¸Šé‡‡æ ·æ­¤ç±»æ•°æ®ï¼Œå°±æ˜¯è¯´**åŒæ ·æ•°æ®å¤šæ‰§è¡Œå‡ éï¼Œä»¥å¢åŠ å…¶å¯¹æ¨¡å‹å‚æ•°å½±å“çš„æƒé‡**ï¼‰å’ŒPost-Trainingé˜¶æ®µï¼ˆå¢åŠ æ­¤ç±»æ•°æ®å æ¯”ï¼ŒLlama3çš„ç»è¿‡instructçš„æ¨¡å‹æ¯”ä»…åšé¢„è®­ç»ƒæ¨¡å‹ç›¸æ¯”ï¼Œå„ç§å°ºå¯¸çš„æ•ˆæœæå‡éƒ½å¾ˆå¤§ï¼‰çš†æ˜¯å¦‚æ­¤ã€‚
ç›®å‰çœ‹ï¼Œåœ¨é€šç”¨æ•°æ®å¿«è¢«ç”¨å®Œæƒ…å†µä¸‹ï¼Œç¬¬ä¸‰ä¸ªå› ç´ ä¼šæˆä¸ºä¹‹åå¤§æ¨¡å‹è¿›æ­¥çš„ä¸»å¯¼åŠ›é‡ï¼ŒåŒ…æ‹¬ä½¿ç”¨æ•°å­¦ã€é€»è¾‘ã€ä»£ç åˆæˆæ•°æ®åœ¨Post-Trainingé˜¶æ®µçš„åº”ç”¨ï¼Œç›®å‰æŠ€æœ¯ä¹Ÿè¶Šæ¥è¶Šæˆç†Ÿï¼Œå…¶è´¨é‡å’Œæ•°é‡ä¼šæ˜¯å†³å®šæœªæ¥å¤§æ¨¡å‹æ•ˆæœå·®å¼‚çš„æœ€å…³é”®å› ç´ ã€‚PSï¼šåˆæˆæ•°æ®å…¶å®æ˜¯æ¨¡å‹è’¸é¦çš„ä¸€ç§å˜ä½“ï¼Œåˆæˆæ•°æ®æ˜¯æ›´å¤§çš„æ¨¡å‹è¾“å‡ºæ•°æ®ä½œä¸ºTeacherï¼Œå°ç‚¹çš„æ¨¡å‹ä½œä¸ºStudentä»ä¸­å­¦ä¹ çŸ¥è¯†ï¼Œç”¨Aå¤§æ¨¡å‹åˆæˆçš„æ•°æ®å¯¹Aå¤§æ¨¡å‹æœ¬èº«æ²¡æœ‰æé«˜æ•ˆæœï¼Œæ‰€ä»¥å…¶å®æœ¬è´¨ä¸Šæ˜¯ä¸€ç§æ¨¡å‹è’¸é¦

è‡ªç ” pretrain æ¨¡å‹çš„æ„ä¹‰åˆæœ‰å“ªäº›å‘¢ï¼Ÿ
1. å„å…¬å¸ä»…ä»…æ˜¯å¼€æºäº†æ¨¡å‹å‚æ•°ï¼Œä½†å¹¶æ²¡æœ‰å¼€æºè®­ç»ƒæ¡†æ¶ã€è®­ç»ƒæ•°æ®ç­‰æ›´æ ¸å¿ƒçš„å†…å®¹ï¼Œå…¶å®æœ¬è´¨ä¸Šè¿˜æ˜¯é—­æºã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¯ä¸€ä¸ª qwen æ¨¡å‹çš„ä½¿ç”¨è€…éƒ½æ— æ³•ä¸ºä¸‹ä¸€ç‰ˆ qwen æ¨¡å‹çš„è¿­ä»£åšå‡ºè´¡çŒ®ï¼Œqwen å›¢é˜Ÿä¹Ÿä»…ä»…æ˜¯æ”¶è·äº†å£ç¢‘ï¼Œç”šè‡³å› ä¸ºè‡ªå·±çš„æ¨¡å‹å·²ç»å¼€æºå¯ä»¥è‡ªè¡Œéƒ¨ç½²ï¼Œä¹°ä»–ä»¬æœåŠ¡çš„å®¢æˆ·å¯èƒ½éƒ½ä¼šå˜å°‘ã€‚å› æ­¤ï¼Œåœ¨ llm çœŸæ­£èµ°å‘å…¨é¢å¼€æºä¹‹å‰ï¼ˆå¤§å‚å…¬å¼€è®­ç»ƒä»£ç ã€é…æ¯”æ•°æ®ï¼Œä»»ä½•äººéƒ½å¯ä»¥é€šè¿‡æ CR æ¥å¸®åŠ©å¤§å‚ä¼˜åŒ–è®­ç»ƒæ•ˆç‡ã€ç‚¼ä¸¹æŠ€å·§ï¼‰ï¼ŒæŒæ¡ pretrain çš„æŠ€æœ¯èƒ½åŠ›ä¾ç„¶æ˜¯æœ‰æ„ä¹‰çš„ï¼›
2. é€šç”¨æ¨¡å‹çš„å˜ç°èƒ½åŠ›è¿œä¸å¦‚ domain æ¨¡å‹ï¼Œcontinue-pretrain çš„éœ€æ±‚åœ¨æ—¥ç›Šå¢é•¿ï¼Œè€Œ continue-pretrain çš„æŠ€æœ¯æ ˆå’Œ pretrain çš„æŠ€æœ¯æ ˆå¹¶æ²¡æœ‰æœ¬è´¨åŒºåˆ«ï¼›
3. ä¸æ˜¯è‡ªå·±åšçš„ pretrainï¼Œå¿…ç„¶æ— æ³•å¾—çŸ¥è¿™ä¸ªæ¨¡å‹åœ¨ pretrain é˜¶æ®µåˆ°åº•å–‚äº†ä»€ä¹ˆæ•°æ®ã€‚å„ç§æ•°æ®çš„ç²¾ç¡®é…æ¯”ã€å„ç§ knowledge çš„æŒæ¡ç¨‹åº¦ï¼Œå¹¶ä¸æ˜¯é è¯„ä¼°èƒ½å‡†ç¡®è¡¡é‡çš„ã€‚è€Œå¦‚æœä¸çŸ¥é“è¿™äº›æ•°æ®ç»†èŠ‚ï¼Œé‚£ä¹ˆ alignment é˜¶æ®µå°±æ— æ³•å¯¹ç—‡ä¸‹è¯ï¼Œå°±æ— æ³•æœ€å¤§é™åº¦çš„å¼€å‘æ¨¡å‹æ½œåŠ›ã€‚ä¸¾ä¸ªç®€å•çš„ä¾‹å­ï¼Œ**ä½ åœ¨ sft é˜¶æ®µï¼Œè®©ä¸€ä¸ªæ²¡è®­è¿‡å”è¯—å®‹è¯çš„é€šç”¨æ¨¡å‹å­¦ä¹ ä½œè¯—ï¼Œå®ƒä¸å‡ºå¹»è§‰è°å‡ºå¹»è§‰ï¼Ÿ**
4. ä½¿ç”¨å¼€æºæ¨¡å‹çš„è¯ï¼Œtokenizer ä¸å¯æ§ï¼Œè¿›è€Œå¯¼è‡´è§£ç é€Ÿåº¦ä¸å¯æ§ã€‚è¿™é‡Œä¹Ÿä¸¾ä¸ªä¾‹å­ï¼Œå¦‚æœæˆ‘ä»¬ç”¨ llama æ¨¡å‹æ¥åšæ„å›¾è¯†åˆ«ä»»åŠ¡ï¼Œæœ‰ä¸ªæ„å›¾å« ai.listen.musicï¼Œä¼šè¢«æ˜ å°„æˆ 5 ä¸ª tokenï¼Œä½†å¦‚æœä½¿ç”¨è‡ªå·±è®­ç»ƒçš„å¤§æ¨¡å‹ï¼Œä¾¿ä¼šåœ¨ä¸€å¼€å§‹å°±è®¾ç½®æˆ 1 ä¸ª tokenï¼Œæå¤§èŠ‚çœäº†ç”Ÿæˆé€Ÿåº¦ã€‚è™½ç„¶æ‰©è¯è¡¨å·²ç»æ˜¯ä¸€ä¸ªæ¯”è¾ƒæˆç†Ÿçš„æŠ€æœ¯äº†ï¼Œä½†ä¸ä»…éœ€è¦èŠ±è´¹ç®—åŠ›æ¥æ¢å¤æ•ˆæœï¼Œè€Œä¸”ä¸ç®¡è®­å¤šå°‘æ–°è¯­æ–™ï¼Œä¹Ÿå¾ˆéš¾åšåˆ°æ¨¡å‹æ•ˆæœå®Œå…¨ä¸æ‰ç‚¹ã€‚

[ä»é›¶æ‰‹æ“ä¸­æ–‡å¤§æ¨¡å‹](https://mp.weixin.qq.com/s/kbmkdkukkvnGMCzRD2Z1mQ) æ¯”è¾ƒé€šä¿—æ˜“æ‡‚ï¼Œæ¨èçœ‹ä¸‹ã€‚

ç›®å‰æ¯”è¾ƒæˆç†Ÿï¼Œä¸€èˆ¬é‡‡ç”¨ gptæ¶æ„ï¼ŒLlama/Llama2ï¼Œåœ¨æ¨¡å‹ç»“æ„ä¸æ”¹å˜çš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹çš„ä¼˜åŒ–ä¸»è¦åœ¨äº åˆ†è¯ã€å‚æ•°é€‰æ‹©ã€è®­ç»ƒæ•°æ®ï¼ŒåŒæ—¶ä¹ŸåŒ…æ‹¬å¯¹attentionæ–¹æ³•çš„é€‰æ‹©ã€‚

## è®¡ç®—é‡

[é¢è¯•æ—¶è¢«é—®åˆ°â€œScaling Lawâ€ï¼Œæ€ä¹ˆç­”ï¼Ÿ](https://mp.weixin.qq.com/s/Q0fThU-4YP5OwFmfJM_q-Q)åœ¨å¤§æ¨¡å‹çš„ç ”å‘ä¸­ï¼Œé€šå¸¸ä¼šæœ‰ä¸‹é¢ä¸€äº›éœ€æ±‚ï¼š
1. è®¡åˆ’è®­ç»ƒä¸€ä¸ª 10B çš„æ¨¡å‹ï¼Œæƒ³çŸ¥é“è‡³å°‘éœ€è¦å¤šå¤§çš„æ•°æ®ï¼Ÿ
2. æ”¶é›†åˆ°äº† 1T çš„æ•°æ®ï¼Œæƒ³çŸ¥é“èƒ½è®­ç»ƒä¸€ä¸ªå¤šå¤§çš„æ¨¡å‹ï¼Ÿ
3. è€æ¿å‡†å¤‡ 1 ä¸ªæœˆåå¼€å‘å¸ƒä¼šï¼Œç»™çš„èµ„æºæ˜¯ 100 å¼  A100ï¼Œåº”è¯¥ç”¨å¤šå°‘æ•°æ®è®­å¤šå¤§çš„æ¨¡å‹æ•ˆæœæœ€å¥½ï¼Ÿ
4. è€æ¿å¯¹ç°åœ¨ 10B çš„æ¨¡å‹ä¸æ»¡æ„ï¼Œæƒ³çŸ¥é“æ‰©å¤§åˆ° 100B æ¨¡å‹çš„æ•ˆæœèƒ½æå‡åˆ°å¤šå°‘ï¼Ÿ
å¤§æ¨¡å‹çš„ Scaling Lawï¼ˆä¸€èˆ¬ç§°ä¸ºpretrain-time scaling lawï¼‰ æ˜¯ OpenAI åœ¨ 2020 å¹´æå‡ºçš„æ¦‚å¿µï¼Œå…·ä½“å¦‚ä¸‹ï¼š
1. å¯¹äº Decoder-only çš„æ¨¡å‹ï¼Œè®¡ç®—é‡ C(Flops), æ¨¡å‹å‚æ•°é‡ Nï¼Œæ•°æ®å¤§å° D(token æ•°)ï¼Œä¸‰è€…æ»¡è¶³ï¼š$C \approx 6ND$
2. æ¨¡å‹çš„æœ€ç»ˆæ€§èƒ½ä¸»è¦ä¸è®¡ç®—é‡ Cï¼Œæ¨¡å‹å‚æ•°é‡ N å’Œæ•°æ®å¤§å° D ä¸‰è€…ç›¸å…³ï¼Œè€Œä¸æ¨¡å‹çš„å…·ä½“ç»“æ„(å±‚æ•°/æ·±åº¦/å®½åº¦)åŸºæœ¬æ— å…³ã€‚å›ºå®šæ¨¡å‹çš„æ€»å‚æ•°é‡ï¼Œè°ƒæ•´å±‚æ•°/æ·±åº¦/å®½åº¦ï¼Œä¸åŒæ¨¡å‹çš„æ€§èƒ½å·®è·å¾ˆå°ï¼Œå¤§éƒ¨åˆ†åœ¨ 2% ä»¥å†…ã€‚
3. å¯¹äºè®¡ç®—é‡ ğ¶ï¼Œæ¨¡å‹å‚æ•°é‡ ğ‘ å’Œæ•°æ®å¤§å° ğ·ï¼Œå½“ä¸å—å…¶ä»–ä¸¤ä¸ªå› ç´ åˆ¶çº¦æ—¶ï¼Œæ¨¡å‹æ€§èƒ½ä¸æ¯ä¸ªå› ç´ éƒ½å‘ˆç°å¹‚å¾‹å…³ç³»ã€‚æ ¹æ®å¹‚å¾‹å®šå¾‹ï¼Œæ¨¡å‹çš„å‚æ•°å›ºå®šï¼Œæ— é™å †æ•°æ®å¹¶ä¸èƒ½æ— é™æå‡æ¨¡å‹çš„æ€§èƒ½ï¼Œæ¨¡å‹æœ€ç»ˆæ€§èƒ½ä¼šæ…¢æ…¢è¶‹å‘ä¸€ä¸ªå›ºå®šçš„å€¼ã€‚
4. ä¸ºäº†æå‡æ¨¡å‹æ€§èƒ½ï¼Œæ¨¡å‹å‚æ•°é‡ N å’Œæ•°æ®å¤§å° D éœ€è¦åŒæ­¥æ”¾å¤§ï¼Œä½†æ¨¡å‹å’Œæ•°æ®åˆ†åˆ«æ”¾å¤§çš„æ¯”ä¾‹è¿˜å­˜åœ¨äº‰è®®ã€‚
5. Scaling Law ä¸ä»…é€‚ç”¨äºè¯­è¨€æ¨¡å‹ï¼Œè¿˜é€‚ç”¨äºå…¶ä»–æ¨¡æ€ä»¥åŠè·¨æ¨¡æ€çš„ä»»åŠ¡ï¼š

å¯¹äº Decoder-only çš„æ¨¡å‹ï¼Œè®¡ç®—é‡C(Flops)ï¼Œæ¨¡å‹å‚æ•°é‡ N(é™¤å» Embedding éƒ¨åˆ†)ï¼Œæ•°æ®å¤§å° D(token æ•°)ï¼Œä¸‰è€…çš„å…³ç³»ä¸º: Câ‰ˆ6NDã€‚æ¨å¯¼å¦‚ä¸‹ï¼Œè®°æ¨¡å‹çš„ç»“æ„ä¸ºï¼šdecoder å±‚æ•°lï¼Œattentionéšå±‚ç»´åº¦dï¼Œattention feedforwardå±‚ç»´åº¦ $d_{ff}$ï¼Œä¸€èˆ¬æ¥è¯´ $d_{ff} = 4 * d$ã€‚
æ¨¡å‹çš„å‚æ•°é‡Nï¼ˆå¿½ç•¥emebeddingã€normå’Œbiasï¼‰ è®¡ç®—å¦‚ä¸‹ï¼Œtransformer æ¯å±‚åŒ…å« self-attention å’Œmlp ä¸¤ä¸ªéƒ¨åˆ†
1. self-attention çš„å‚æ•°ä¸º$W_q$ã€$W_k$ã€$W_v$ã€$W_o$ï¼Œæ¯ä¸ªçŸ©é˜µçš„ç»´åº¦ä¸º$R^{d*d}$ï¼Œæ•´ä½“å‚æ•°é‡ï¼š$4 * d^2$
2. mlp çš„å‚æ•°ä¸º $W_1$ï¼ˆç»´åº¦ä¸º$R^{d*d_{ff}}$ï¼‰å’Œ$W_2$ï¼ˆç»´åº¦ä¸º$R^{d_{ff}*d}$ï¼‰ï¼Œæ•´ä½“å‚æ•°é‡ï¼š$2 * d * d_ff = 2 * d * 4d = 8d^2$
3. æ‰€ä»¥æ¯å±‚å‚æ•°æ˜¯$4d^2 + 8d^2 = 12d^2$ï¼Œå…¨éƒ¨l å±‚çš„å‚æ•°é‡ä¸º $12*ld^2$ï¼Œå³$N=12*ld^2$

è®¡ç®—é‡çš„å•ä½æ˜¯FLOPSï¼Œfloat point operations å¯¹äºçŸ©é˜µA (m*n)å’ŒBï¼ˆn*pï¼‰ AB ç›¸ä¹˜çš„è®¡ç®—é‡ä¸º2mnpï¼Œä¸€æ¬¡åŠ æ³•ä¸€æ¬¡ä¹˜æ³•ã€‚å‡è®¾decoder å±‚çš„è¾“å…¥Xï¼ˆb*s*dï¼‰ï¼Œbä¸ºbatch sizeï¼Œsä¸ºåºåˆ—é•¿åº¦ï¼Œdä¸ºæ¨¡å‹ç»´åº¦ã€‚

1. å‰å‘æ¨ç†çš„è®¡ç®—é‡ï¼š
    1. self-attention éƒ¨åˆ†çš„è®¡ç®—ï¼š
        1. è¾“å…¥çº¿æ€§å±‚ï¼Œ$XW_q$ã€$XW_k$ã€$XW_v$ï¼Œè®¡ç®—é‡ä¸º $3 * b * s * d * d * 2 = 6bsd^2$
        2. attention è®¡ç®— $QK^T$ï¼Œè®¡ç®—é‡ä¸º $2 * b * s* s * d = 2bs^2d$
        3. score ä¸V çš„è®¡ç®—ï¼Œ$S_{attention}V$ï¼Œè®¡ç®—é‡ä¸º $b*2 * s * s * d = 2bs^2d$
        4. è¾“å‡ºçº¿æ€§å±‚ $X^{\prime} W_O$ï¼Œè®¡ç®—é‡ä¸º $b * 2 * s * d * d = 2bsd^2$
    2. MLP éƒ¨åˆ†çš„è®¡ç®—ï¼š
        1. å‡ç»´ $XW_1$ï¼Œè®¡ç®—é‡ä¸º $b * 2 * s * d * 4d = 8bsd^2$
        2. é™ç»´ $XW_2$ï¼Œè®¡ç®—é‡ä¸º $b * 2 * s * 4d * d = 8bsd^2$
    3. æ‰€ä»¥æ•´ä¸ªdecoderå±‚çš„è®¡ç®—é‡ä¸º $24bsd^2 + 4bs^2d$ï¼Œå…¨éƒ¨lå±‚ä¸º $C_{forward}=24bsd^2 + 4bs^2d$ã€‚
2. åå‘ä¼ æ’­è®¡ç®—é‡æ˜¯æ­£å‘çš„2å€ï¼Œæ‰€ä»¥å…¨éƒ¨è®¡ç®—é‡ä¸º $C=3*C_{forward} = 72bsd^2 + 12bs^2d$ã€‚
3. å¹³å‡æ¯ä¸ªtokençš„è®¡ç®—é‡ä¸º $C_{token}=\frac{C}{b s} = 72ld^2 + 12lsd = 6N (1+\frac{s}{6 d}) \approx 6N(s \le 6d)$
4. æ‰€ä»¥å¯¹äºåŒ…å«å…¨éƒ¨Dä¸ªtokençš„æ•°æ®é›† $C = C_{token}D \approx 6ND$

PSï¼šmlpå‚æ•°é‡å’Œè®¡ç®—é‡éƒ½ä¸è¾“attentionã€‚ $C_{token} \approx 6N$ å¯ä»¥è®¤ä¸ºæ˜¯æ­£åå‘3å€*åŠ æ³•ä¹˜æ³•2å€=6å€ã€‚å†æ‹¿åˆ°GPUçš„ç®—åŠ› * GPU ä¸ªæ•°ï¼Œå°±å¯ä»¥æ ¹æ®
$C \approx 6ND$ è®¡ç®—è®­ç»ƒçš„è€—æ—¶äº†ã€‚

## æ•°æ®å‡†å¤‡

[å¤šè¯­è¨€å¤§æ¨¡å‹æ•°æ®ç ”å‘åœ¨å¤§æ¨¡å‹æ—¶ä»£çš„å®æˆ˜å…¨è§£](https://mp.weixin.qq.com/s/eku_BWzvN_H7TPhr6xq7KQ) æœªç»†è¯»ã€‚

|è®­ç»ƒé˜¶æ®µ|	è¯´æ˜|	å¸¸è§æ•°æ®é›†|
|---|---|---|
|pre-trainï¼ˆcontinual-train)|	æ”¶é›†ã€æ•´ç†æ¸…æ´—ï¼Œåˆ†ç±»ç­›é€‰ï¼Œå¤šè¯­è¨€æ— éœ€ç‰¹åˆ«åŠ å·¥ï¼Œä¸»è¦æ˜¯æ•°æ®åˆ†å¸ƒåˆç†ï¼Œè´¨é‡ä¿éšœ	|C4(T5)ï¼ŒRedPajamaï¼ŒPileï¼ŒWudaoï¼ŒROOTS(BLOOM)|
|sft|	æŒ‡ä»¤å¾®è°ƒï¼Œæ•°æ®éœ€è¦äººå·¥åŠ å·¥ï¼Œå·²ç»æœ‰ä¸€äº›å…¬å¼€æ•°æ®é›†äº†ä¸»è¦ç›®çš„æ˜¯è®©æœºå™¨å­¦ä¼šä¸€äº›æ€è€ƒã€å›ç­”é—®é¢˜çš„æ–¹æ³•	|
|RLHF|	éœ€è¦å•ç‹¬ç»„ç»‡ï¼Œäººå·¥æ•´ç†æ•°æ®ä¸»è¦ç›®çš„æ˜¯åœ¨å›ç­”çš„helpfulã€safetyç­‰æ–¹é¢ç¬¦åˆäººç±»æ ‡å‡†|	https://github.com/anthropics/hh-rlhf/blob/master/README.md|

[å…³äºå¤§æ¨¡å‹è¯­æ–™çš„è¿·æ€](https://mp.weixin.qq.com/s/A8O5omTFd4egVIuH--ZNWg) æœªè¯»ã€‚
[èŠä¸€èŠåšPretrainçš„ç»éªŒ](https://mp.weixin.qq.com/s/pUJsZVBN_Gh2yBF3g5XhKA)pretrain å¤§æ¨¡å‹çš„ç¬¬ä¸€ä»¶äº‹ï¼šå…ˆæ‰¾ä¸ª 10T å·¦å³çš„è®­ç»ƒæ•°æ®å§ã€‚è‡³äºæ€ä¹ˆè·å–æ•°æ®ï¼Œçˆ¬ç½‘é¡µã€é€›æ·˜å®ã€è”ç³»æ•°æ®è´©å­ï¼Œç­‰ç­‰ç­‰ç­‰ã€‚ç®—æ³•åŒå­¦å¾€å¾€æä¸å®šè¿™ä¸ªäº‹æƒ…ï¼Œä½ æ•¢çˆ¬ä»–å°±æ•¢å°ä½  IPï¼Œä½ çˆ¬å¾—èµ·åŠ²ä»–ç”šè‡³è¿˜å¯ä»¥èµ·è¯‰ä½ ï¼Œæ‰€ä»¥è¿™ä¸ªå·¥ä½œæœ€å¥½è¿˜æ˜¯è®©ä¸“ä¸šçš„æ•°æ®å›¢é˜ŸåŒå­¦æ¥åšã€‚å¥½åœ¨ï¼Œä¸–ä¸Šè¿˜æ˜¯å¥½äººå¤šï¼ä»Šå¹´å†åš pretrain å·¥ä½œï¼Œç½‘ä¸Šçš„å¼€æºæ•°æ®é›†å·²ç»å¾ˆå¤šäº†ã€‚FineWebã€pileã€Skypileã€RedPajamaï¼Œå‡‘åˆç€å·®ä¸å¤šèƒ½å½“å¯åŠ¨èµ„é‡‘æ¥ç”¨ã€‚ä½†ä»å¦ä¸€ä¸ªè§’åº¦è®²ï¼Œä¸–ç•Œä¸Šæ²¡æœ‰å…è´¹çš„åˆé¤ï¼Œæ‰€æœ‰å¼€æºå‡ºæ¥çš„ä¸­æ–‡å¤§æ¨¡å‹æ•°æ®é›†ï¼Œæˆ‘ä¸è®¤ä¸ºæ˜¯ä»–ä»¬æœ€å¹²å‡€çš„æ•°æ®ï¼Œè´¨é‡å¤šå°‘éƒ½æœ‰ç‚¹é—®é¢˜ã€‚å‡†å¤‡æ•°æ®è¿˜è¦æ‡‚å¾—ä¸€ä¸ªåŸºç¡€æ¦‚å¿µï¼šæ•°æ®çš„çŸ¥è¯†å¯†åº¦æ˜¯æœ‰å·®å¼‚çš„ã€‚â€œå”è¯—ä¸‰ç™¾é¦–â€çš„çŸ¥è¯†é‡è¦è¿œè¿œå¤§äºâ€œä¸­å›½æ–°é—»ç½‘çš„ä¸‰ç™¾ç¯‡æ–°é—»â€ã€‚è€Œè¿™ç§é«˜çŸ¥è¯†å¯†åº¦çš„è®­ç»ƒæ•°æ®ï¼Œå¾€å¾€éƒ½æ˜¯éœ€è¦èŠ±é’±çš„ã€‚æœ€è¿‘ï¼Œä¸€ç§æ–°çš„æ•°æ®è¶‹åŠ¿æ˜¯â€œåˆæˆé«˜çŸ¥è¯†å¯†åº¦æ•°æ®â€ï¼ŒæŠŠå‡ åƒå­—çš„æ–°é—»æ¦‚æ‹¬æˆå‡ ç™¾å­—å–‚ç»™æ¨¡å‹ï¼Œå››èˆäº”å…¥ä¹Ÿç­‰äºè®­ç»ƒé€Ÿåº¦æé«˜äº†åå€ã€‚

![](/public/upload/machine/llm_train.jpg)

1. æ•°æ®æ¸…æ´—ã€‚
    1. ç›®å‰ï¼Œåˆ©ç”¨æ¨¡å‹å¯¹ pretrain æ•°æ®çš„è´¨é‡è¿›è¡Œæ‰“åˆ†ï¼Œå·²ç»æˆäº†æ•°æ®æ¸…æ´—å·¥ä½œçš„æ ‡é…ï¼Œllama3ã€qwen2 çš„æŠ€æœ¯æŠ¥å‘Šéƒ½æœ‰æåŠã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒåŸºæœ¬ä¸Šå¤§å®¶éƒ½è®¤åŒï¼šåŒç­‰ size ä¸‹ï¼ŒBERT ç»“æ„çš„æ¨¡å‹çš„è¡¨å¾èƒ½åŠ›æ˜¯å¼ºäº transformer-decoder æ¨¡å‹çš„ï¼Œå› æ­¤æ‰“åˆ†æ¨¡å‹æœ€å¥½è¿˜æ˜¯ä» BERT å®¶æ—ä¸­é€‰ä¸€ä¸ªæ¥è®­ï¼Œæ•ˆæœå¥½ã€é€Ÿåº¦è¿˜å¿«ã€‚è®­æ‰“åˆ†å™¨è¿™ä¸ªå·¥ä½œçš„éš¾ç‚¹æ˜¯è¦å­¦ä¼šæ”¾ä½å¿ƒæ€ï¼Œåˆ«é‚£ä¹ˆæ‰§æ‹—ï¼Œä¸è¦æ‰§ç€äºæ‰“åˆ†å™¨ 100% çš„å‡†ç¡®ç‡ï¼Œå‡‘åˆèƒ½ç”¨å°±è¡Œäº†ï¼Œæœ‰æ‰“åˆ†å™¨æ€»æ¯”æ²¡æ‰“åˆ†å™¨å¼ºï¼Œä½†ä½ è¦èŠ±ä¸€ä¸ªæœˆæ¥è®­æ‰“åˆ†å™¨ï¼Œé‚£å°±è¿˜ä¸å¦‚æ²¡æ‰“åˆ†å™¨ã€‚æ‰“åˆ†å™¨ç»“æœåªæ˜¯ä¼—å¤šæ•°æ®ç‰¹å¾ä¸­çš„ä¸€ä¸ªç‰¹å¾ï¼Œå¹¶ä¸ä¸€å®šè¦å®Œå…¨ä¾èµ–å®ƒæ¥æ´—æ•°æ®ï¼Œå¯ä»¥å’Œå…¶ä»–ç‰¹å¾ç»“åˆä½¿ç”¨ã€‚
    2. è¿™ä¹Ÿå¼•å‡ºäº†æ•°æ®æ¸…æ´—çš„å¦ä¸€ä¸ªå¤§æ€å™¨ï¼šè§„åˆ™ã€‚ä¸è¦ç§ä¸èµ·è§„åˆ™ï¼æ•°æ®é•¿åº¦æ˜¯å¦å°‘äºæŸä¸ªå€¼ï¼Œæ•°æ®ä¸­æŸä¸ª token çš„æ¯”ä¾‹è¶…è¿‡æŸä¸ªé˜ˆå€¼ï¼Œæ•°æ®çš„ zh å æ¯”ã€en å æ¯”ã€æ•°å­—å æ¯”ï¼Œæ•°æ®æ˜¯å¦æœ‰â€œhttpâ€å­—æ®µï¼Œæ•°æ®æ˜¯å¦åŒ…å«äº†â€œæ–°å† â€ã€â€œç–«æƒ…â€ç­‰ä½è´¨é‡å…³é”®è¯ï¼Œæ•°æ®æ˜¯å¦åŒ…å«æŸäº›ååŠ¨è¯æ±‡ï¼Œæ•°æ®æ˜¯å¦åŒ…å«æŸäº›é»„è‰²å­—çœ¼ï¼Œç­‰ç­‰ç­‰ç­‰ã€‚ç”¨å¯å‘å¼çš„è§„åˆ™è¿‡æ»¤æ•°æ®å¹¶ä¸ä¸¢äººï¼Œæ´—ä¸å¹²å‡€æ•°æ®æ‰ä¸¢äººã€‚ä½†åŒæ—¶ï¼Œå¿…é¡»æ³¨æ„åˆ°ï¼Œç”¨è§„åˆ™æ¸…æ´—æˆ–è€…è¿‡æ»¤æ•°æ®çš„æ—¶å€™ï¼Œä¸€å®šä¸è¦æŠŠæ•°æ®ææˆåˆ†å¸ƒæœ‰åçš„æ•°æ®ã€‚æ¯”å¦‚ã€ä½ è§‰ç€ï¼šâ€œåŒ…å«ç½‘å€çš„æ•°æ®è´¨é‡ä½ï¼Œè€Œç½‘å€çš„è‹±æ–‡å æ¯”é«˜â€ï¼Œæ‰€ä»¥ä½ æŠŠè‹±æ–‡å æ¯”é«˜çš„æ•°æ®éƒ½å»æ‰äº†ã€‚æ•´æŒºå¥½ï¼Œæ¨¡å‹æˆäº†å•è¯­æ¨¡å‹ã€‚å› æ­¤ï¼Œç”¨è§„åˆ™çš„æ—¶å€™ï¼Œä¸€å®šè¦å¤š check ä¸‹è¢«æ»¤å‡ºå»çš„æ•°æ®é•¿ä»€ä¹ˆæ ·å­ï¼Œå‹¤ vim ä¸€ä¸‹ï¼
    3. æ•°æ®è„±æ•ä¹Ÿæ˜¯æ•°æ®æ¸…æ´—ç¯èŠ‚å¿…é¡»è¦åšçš„ä¸€ä¸ªå·¥ä½œã€‚æˆ‘ä»¬è¦å°½å¯èƒ½çš„æŠŠè®­ç»ƒæ•°æ®ä¸­æ¶‰åŠåˆ°çš„äººåã€ç”µè¯å·ç ã€é‚®ç®±ç­‰å‰”é™¤å‡ºå»ï¼Œä¸€æ—¦è¢«æ¨¡å‹è¯´å‡ºæ¥ï¼Œå°±æ„æˆäº†éšç§ä¾µçŠ¯ï¼Œå…¬å¸è¢«ç½šçš„é’±è¶³å¤Ÿé›‡äººæŠŠæ•°æ®è„±æ• N éäº†ã€‚æ›´å¹¿ä¹‰çš„ï¼ŒæŠŠæ•°æ®çš„â€œè½¬è½½è‡ªâ€¦â€¦â€åˆ æ‰ï¼Œé»„è‰²ä¿¡æ¯ã€ååŠ¨ä¿¡æ¯ï¼Œreferences ç­‰å‰”é™¤å‡ºå»ï¼Œéƒ½å¯ä»¥è§†ä½œæ•°æ®è„±æ•å·¥ä½œçš„ä¸€éƒ¨åˆ†ã€‚è¿™ä¸ªå·¥ä½œå¥½åƒæ²¡ä»»ä½•å¥‡æ·«å·§æŠ€ï¼Œè€è€å®å®çš„å†™æ­£åˆ™åŒ¹é…å§ã€‚
    ![](/public/upload/machine/data_cleaning.jpg)
2. æ•°æ®å»é‡ã€‚æ•°æ®ç¯èŠ‚æœ€è€ƒç ”å·¥ç¨‹èƒ½åŠ›çš„ç¯èŠ‚åˆ°äº†ï¼šå¯¹ T çº§åˆ«çš„æ•°æ®è¿›è¡Œå»é‡ã€‚
    1. ä¸è¦å¿ƒå­˜ä»»ä½•å¹»æƒ³ï¼šèƒ½ä¸èƒ½ä¸åšæ•°æ®å»é‡ã€‚ç­”æ¡ˆè‚¯å®šæ˜¯ä¸è¡Œçš„ï¼ç½‘ä¸ŠåŸºæœ¬æ‰€æœ‰çš„å¼€æºæ•°æ®ï¼Œéƒ½æ˜¯æ¥è‡ª common crawlï¼Œä½ ä¸å»é‡å¦‚ä½•æ··åˆä½¿ç”¨å‘¢ã€‚å°±ç®—ä½ åªä½¿ç”¨å•ä¸€æ•°æ®æºæˆ–è€…è‡ªå·±çˆ¬å–æ•°æ®ï¼Œä¹Ÿåº”è¯¥æ³¨æ„åˆ°ï¼šç½‘é¡µ A å¼•ç”¨äº† ç½‘é¡µ Bï¼Œç½‘é¡µ B å¼•ç”¨äº† ç½‘é¡µ Câ€¦â€¦ï¼Œç½‘é¡µ Z åˆå¼•ç”¨äº†ç½‘é¡µ Aã€‚è¿™ç§ url å¾ªç¯è°ƒç”¨çš„ç°è±¡ï¼Œåœ¨äº’è”ç½‘å±¡è§ä¸é²œï¼Œä½ çš„è®­ç»ƒæ•°æ®é›†å¤§æ¦‚ç‡ä¼šæŠŠä¸€ä¸ªç½‘é¡µç¿»æ¥è¦†å»çš„ä½¿ç”¨ã€‚å³ä½¿èƒ½ç¡®ä¿æ˜¯ä¸åŒçš„ç½‘é¡µï¼Œä¸€ç¯‡æ–‡ç« ä¹Ÿä¼šè¢«çŸ¥ä¹ã€CSDNã€åšå®¢ã€å¾®ä¿¡å…¬ä¼—å·ã€å°çº¢ä¹¦ç­‰ä¸åŒè½¯ä»¶åå¤è½¬è½½ã€‚
    2. å»é‡å·¥ä½œå”¯ä¸€å¯ä»¥è®©æ­¥çš„åœ°æ–¹æ˜¯ï¼šæ˜¯åš sentence å»é‡è¿˜æ˜¯åš document å»é‡ï¼Œè¿™ä¸ªæˆ‘ä¹Ÿä¸å¥½æ–­å®šï¼Œæˆ‘çš„å»ºè®®æ˜¯é‡åŠ›è€Œä¸ºã€‚èƒ½åš sentence å»é‡ï¼Œè°ä¸æ„¿æ„å‘¢ï¼Ÿå¯æ˜¯æ•°æ®é‡å’Œå·¥ä½œéš¾åº¦ä¹Ÿä¼šé™¡å¢ã€‚
    3. é‚£ä¹ˆå¦‚ä½•å»é‡å‘¢ï¼Ÿé¦–å…ˆï¼Œä½ ä¸€å®šè¦æœ‰ä¸€ä¸ªå¤§æ•°æ®å¤„ç†é›†ç¾¤ï¼Œhadoop ä¹Ÿå¥½ã€spark ä¹Ÿç½¢ï¼Œåªè¦æ˜¯ä¸€ä¸ª map / reduce çš„æ¡†æ¶å°±éƒ½å¯ä»¥ã€‚è¿™ä¸ªå±äºæ±½è½¦çš„è½®å­ï¼Œæƒ³è¦é  python å†™ for å¾ªç¯å®Œæˆè¿™ä¸ªå·¥ä½œï¼Œç¡®å®æ˜¯å‹‡æ°”å¯å˜‰ã€‚ç„¶åï¼Œå°±å»å®ç°ä¸€ä¸ªç®€å•çš„ minhash ä»£ç ï¼Œæ²¡å•¥éš¾åº¦ï¼ŒChatGPT ä¸€å®šä¼šå†™ã€‚
    4. æ•°æ®å»é‡å·¥ä½œæœ‰ä¸€ä¸ªæ¯”è¾ƒé‡è¦çš„æ„è¯†ï¼šè¦å…ˆç¡®å®šéœ€è¦å¤šå°‘è®­ç»ƒæ•°æ®ï¼Œå†ç¡®å®šå»é‡çš„ç²’åº¦ã€‚å»é‡å·¥ä½œæ˜¯æ²¡æœ‰å°½å¤´çš„ï¼Œä»»ä½•æ—¶å€™ä½ éƒ½èƒ½æŠŠæ•°æ®ç»§ç»­æ´—ä¸‹å»ï¼Œæ‰€ä»¥å¿…é¡»æ˜ç¡®è‡ªå·±éœ€è¦å¤šå°‘è®­ç»ƒæ•°æ®ã€‚éœ€è¦ 10T è®­ç»ƒæ•°æ®ï¼Œå°±å¡ç›¸ä¼¼åº¦åœ¨ 80% çš„é˜ˆå€¼è¿›è¡Œå»é‡ï¼›éœ€è¦ 5T çš„è®­ç»ƒæ•°æ®ï¼Œå°±å¡ç›¸ä¼¼åº¦åœ¨ 90% çš„é˜ˆå€¼è¿›è¡Œå»é‡ï¼›ä»¥æ­¤ç±»æ¨ã€‚ç›®å‰æ²¡æœ‰ä»»å·¥ä½œèƒ½è¯æ˜ï¼Œä¸€æ¡æ•°æ®åœ¨ pretrain é˜¶æ®µè®­å¤šå°‘éå¯¹æ¨¡å‹æ˜¯æœ€å‹å¥½çš„ã€‚å› æ­¤ï¼Œå¤§èƒ†çš„æŒ‰éœ€å»é‡ï¼Œå³ä½¿å»é‡ç²’åº¦å°ï¼Œå¯¼è‡´ä¸€ç¯‡æ–‡æ¡£å‡ºç°å¤šæ¬¡ï¼Œä¹Ÿå¯ä»¥é€šè¿‡è®©ä¸¤ç¯‡ç›¸ä¼¼æ–‡æ¡£ä¹‹é—´éš”å°½é‡å¤šçš„ token æ¥é™ä½å½±å“ã€‚å»é‡çš„æ—¶å€™ï¼Œâ€œæ–°é—»â€ç±»å¯èƒ½ 70% çš„é‡å¤åº¦å°±ä¸è¦ï¼Œâ€œçŸ¥è¯†â€ç±»åˆ™å¯ä»¥ 85% çš„ç›¸ä¼¼åº¦æ‰ä¸¢å¼ƒï¼Œåœ¨ä¸¢å»é‡å¤æ–‡æ¡£çš„æ—¶å€™ï¼Œä¼˜å…ˆä¿ç•™æ•°æ®æ‰“åˆ†å™¨æ¯”è¾ƒé«˜çš„æ•°æ®ã€‚
3. æ•°æ®é…æ¯”ã€‚å¤§æ¨¡å‹å¯èƒ½åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¿‡åº¦ä¸“æ³¨äºå‚ç±»æ•°æ®ï¼Œå¯¼è‡´lossçš„æ”¶æ•›ä¸å†ä¾èµ–å…¨å±€è€Œæ˜¯ä»éƒ¨åˆ†æ•°æ®è¿›è¡Œè€ƒè™‘ã€‚
    1. æ¯ä¸€ä¸ª document è¿›è¡Œç±»åˆ«åˆ¤æ–­ï¼Œä¸ç”¨ç‰¹åˆ«ç²¾å‡†ï¼ŒæŠŠæ•°æ®åˆ’åˆ†æˆæ–°é—»ã€ç™¾ç§‘ã€ä»£ç ã€markdownã€ç­‰ç±»ç›®å³å¯ï¼Œåˆ†ç±»å™¨æ¨¡å‹ä¾ç„¶å¯ä»¥é€‰æ‹©ä½¿ç”¨ BERT å®¶æ—ã€‚
    2. å¤§éƒ¨åˆ†çš„æŠ€æœ¯æŠ¥å‘Šé‡Œï¼Œåº”è¯¥éƒ½æåŠäº†è‡ªå·±çš„æ•°æ®æ˜¯å¦‚ä½•é…æ¯”çš„ï¼ŒåŸºæœ¬ä¸Šéƒ½æ˜¯â€œçŸ¥è¯† + ä»£ç  + é€»è¾‘â€ä¸‰ä¸ªå¤§ç±»ç›®ï¼Œå…¶ä¸­çŸ¥è¯†æ•°æ®åˆ†æ–‡ä¸­æ–‡çŸ¥è¯†å’Œè‹±æ–‡çŸ¥è¯†ï¼Œé€»è¾‘æ•°æ®åˆ™å¯ä»¥è®¤ä¸ºæ˜¯ math æ•°æ®å’Œ cot æ•°æ®çš„æ··åˆä½“ã€‚æ•´ä½“ä¸Šï¼Œå¤§éƒ¨åˆ†ä¸­æ–‡æ¨¡å‹çš„é…æ¯”éƒ½åœ¨è¿™ä¸ªåŒºé—´å·¦å³ï¼šä¸­ï¼šè‹±ï¼šcode = 4:4:2ï¼ˆé€»è¾‘æ•°æ®çš„æ¯”ä¾‹æˆ‘æ²¡æœ‰å†™è¿›å»ï¼ŒåŠ å…¥å¤šå°‘å–å†³äºä½ èƒ½æ”¶é›†å¤šå°‘ï¼Œå…¶ä»–ä¸‰ç±»æ•°æ®åº”è¯¥æ˜¯è¦å¤šå°‘æœ‰å¤šå°‘çš„å­˜åœ¨ï¼‰ã€‚æˆ‘ä»¬å¯ä»¥æ ¹æ®è‡ªå·±çš„å®é™…æƒ…å†µè°ƒæ•´é…æ¯”ï¼Œä½†è‹±æ–‡çš„æ¯”ä¾‹ä¸€å®šä¸èƒ½å¤ªä½ã€‚ç›®å‰ä¸­æ–‡æ•°æ®çš„è´¨é‡ä¸å¦‚è‹±æ–‡æ•°æ®è´¨é‡åŸºæœ¬å·²ç»æˆåŠŸå…±è¯†ï¼Œå¯¼è‡´è¿™ä¸ªç°è±¡å¯èƒ½æœ‰ä¸¤ä¸ªåŸå› ï¼šä¸­æ–‡ç¡®å®æ¯”è‹±æ–‡éš¾å­¦ï¼Œè¯­è¨€ç©ºé—´çš„å¤æ‚åº¦æ›´é«˜ï¼›ä¸­æ–‡è¯­æ–™æ— è®ºæ˜¯å¹²å‡€ç¨‹åº¦è¿˜æ˜¯æ•°é‡çº§ï¼Œéƒ½æ— æ³•ä¸è‹±æ–‡è¯­æ–™ç›¸æ¯”è¾ƒã€‚
4. æ•°æ®é¡ºåºï¼Œpretrain çš„æœ¬è´¨æ˜¯ä¸€ä¸ªæ•™æ¨¡å‹å­¦çŸ¥è¯†çš„è¿‡ç¨‹ï¼Œæ—¢ç„¶æ˜¯å­¦ä¹ ï¼Œé‚£ä¹ˆçŸ¥è¯†çš„é¡ºåºå°±æ˜¾å¾—å¾ˆé‡è¦ï¼Œæ€»ä¸èƒ½å…ˆå­¦å¾®ç§¯åˆ†ï¼Œå†å­¦æ•°å­—åŠ å‡æ³•å§ã€‚è¿™ä¹Ÿå°±æ˜¯â€œè¯¾ç¨‹å­¦ä¹ â€çš„æ ¸å¿ƒæ€æƒ³ã€‚è¯¾ç¨‹å­¦ä¹ çš„å†…å®¹å¾ˆå®½æ³›ï¼Œæ— è®ºæ˜¯å…ˆå­¦éš¾çŸ¥è¯†ã€å†å­¦è„çŸ¥è¯†ï¼Œè¿˜æ˜¯å…ˆå­¦å¥½æ•°æ®ã€å†å­¦è„æ•°æ®ï¼Œéƒ½å¯ä»¥è§†ä¸ºæ˜¯è¯¾ç¨‹å­¦ä¹ ã€‚å…¶æœ¬è´¨å°±æ˜¯åœ¨é˜è¿°ä¸€ä»¶äº‹æƒ…ï¼šâ€œåŒæ · 1ä¸ªTçš„è®­ç»ƒæ•°æ®ï¼Œé€šè¿‡è°ƒæ•´è®­ç»ƒé¡ºåºå¾—åˆ°çš„ä¸åŒæ¨¡å‹ï¼Œèƒ½åŠ›æ˜¯ä¸åŒçš„ã€‚â€è¿™ä¸ªè§‚ç‚¹åŸºæœ¬å·²ç»è¢«å¾ˆå¤šå›¢é˜Ÿè®ºè¯å¤šæ¬¡äº†ï¼Œå› æ­¤è¯¾ç¨‹å­¦ä¹ ç›®å‰ä¹Ÿå¯ä»¥è®¤ä¸ºæ˜¯ pretrain çš„æ ‡é…ã€‚è™½ç„¶ next_token çš„è®­ç»ƒæ–¹æ³•ï¼ŒåŸºæœ¬ä¸å­˜åœ¨æ¨¡å‹å­¦ä¸ä¼šæŸæ¡æ•°æ®çš„æƒ…å†µã€‚ä½†ä»å¦å¤–ä¸€ä¸ªè§’åº¦æ¥åˆ†æï¼Œç¾éš¾æ€§é—å¿˜å¯èƒ½å§‹ç»ˆåœ¨å‘ç”Ÿï¼ŒA + B çš„å­¦ä¹ é¡ºåºå¯èƒ½å¯¼è‡´ A çŸ¥è¯†é—å¿˜äº† 30%ï¼ŒB + A çš„å­¦ä¹ é¡ºåºå¯èƒ½å¯¼è‡´ B çŸ¥è¯†é—å¿˜äº† 20%ï¼Œé‚£åè€…å¿˜å¾—å°‘è‡ªç„¶èƒ½åŠ›æ›´å¼ºå•Šã€‚è€Œä¸”ï¼Œå¦‚æœ B æ˜¯ä¸€ä¸ªç®€å•çš„çŸ¥è¯†ï¼Œé‚£å°±ä»£è¡¨ B åœ¨è®­ç»ƒè¯­æ–™ä¸­ä¼šå‡ºç°éå¸¸å¤šçš„æ¬¡æ•°ï¼Œå³ä½¿é—å¿˜äº†åç»­ä¹Ÿä¼šè¢«é‡æ–°æ¡èµ·æ¥ï¼Œå›°éš¾çŸ¥è¯†åœ¨å…¨éƒ¨è®­ç»ƒæ•°æ®ä¸­å‡ºç°çš„æ¬¡æ•°è‡ªç„¶ä¹Ÿä¼šå°å¾ˆå¤šã€‚ï¼ˆå…¨å±€è®­ç»ƒè¯­æ–™ä¸­ï¼Œèœ€é“éš¾å…¨æ–‡å‡ºç°çš„æ¬¡æ•°ä¸€å®šæ¯”é™å¤œæ€å…¨æ–‡å‡ºç°çš„æ¬¡æ•°å°‘ï¼‰ã€‚

æ•°æ®æµæ°´çº¿ï¼šé¦–å…ˆè¦æ˜ç¡®ä¸€ä¸ªæ¦‚å¿µï¼Œpretrain æ¨¡å‹ä¸€å®šæ˜¯åŠ¨æ€åŠ è½½æ•°æ®çš„ï¼Œè¯» 1B ã€è®­ 1Bã€å†è¯» 1B ã€å†è®­ 1Bâ€¦â€¦ åŸå› å¾ˆç®€å•ï¼Œä½ ä¸çŸ¥é“ä½ è¦è®­å¤šå°‘æ•°æ®ï¼Œå³ä½¿çŸ¥é“ä½ ä¹Ÿæ²¡é‚£ä¹ˆå¤§çš„å†…å­˜ç©ºé—´ä¸€ä¸‹å­è¯»å–å¥½å‡  T çš„æ•°æ®ã€‚å†æ˜ç¡®ä¸€ä¸ªæ¦‚å¿µï¼Œpretrain é˜¶æ®µæ¨¡å‹è·å–çš„æ˜¯ token_idï¼Œè€Œä¸æ˜¯ token æœ¬èº«ï¼Œæˆ‘ä»¬çš„ tokenizationã€concatenation æ“ä½œè‚¯å®šæ˜¯è¦æå‰åšå¥½çš„ã€‚å½“æœºå™¨è¯»å–äº†ä¸€ä¸ªæ–°æ•°æ®å—ä¹‹åï¼Œå¦‚æœä¸èƒ½ç›´æ¥å»è®­ç»ƒï¼Œè€Œæ˜¯è¿˜è¦èŠ±æ—¶é—´å»è½¬ tokenï¼Œå» concatã€å» padï¼Œè¿™ç®€ç›´æ˜¯å¯¹ GPU çš„ä¸€ç§ä¾®è¾±ã€‚æ˜ç¡®è¿™ä¸¤ä¸ªæ¦‚å¿µä¹‹åï¼Œæˆ‘ä»¬å°±åº”è¯¥çŸ¥é“ï¼Œpretrain çš„ä¸¤ä¸ªè¿›ç¨‹æ˜¯ç‹¬ç«‹çš„ï¼šâ€œæ•°æ®å¤„ç†è¿›ç¨‹â€å’Œâ€œæ¨¡å‹è®­ç»ƒè¿›ç¨‹â€ã€‚å‰è€…è¦ä¿è¯åè€…å§‹ç»ˆæœ‰æœ€æ–°çš„æ•°æ®å¯ç”¨ï¼Œé™¤äº† save_checkpoint çš„æ—¶å€™ï¼ŒGPU çš„ç©ºé—²æ˜¯ä¸€ç§æå¤§çš„æµªè´¹ã€‚pretrain é˜¶æ®µçš„æ•°æ®æ˜¯å¯ä»¥å¤ç”¨çš„ï¼Œé«˜è´¨é‡æ•°æ®è®­å¤šéå¯¹æ¨¡å‹å¹¶æ²¡æœ‰åå¤„ã€‚å› æ­¤ï¼Œæ•°æ®å¤„ç†è¿›ç¨‹åœ¨ç”Ÿäº§ part-00000.jsonl çš„åŒæ—¶ï¼Œå®ƒä¹Ÿåº”è¯¥æ ‡è®°æ¸…æ¥šæ¯ä¸€æ¡åŸå§‹çš„ document æ•°æ®è¢«ä½¿ç”¨äº†å¤šå°‘æ¬¡ï¼Œè¢«æ ‡è®°æ¬¡æ•°å¤šçš„æ•°æ®ï¼Œåç»­è¦é™ä½å®ƒå†è¢«é€‰ä¸­çš„æ¦‚ç‡ã€‚æ¯ä¸ªæ•°æ®å—ä¸è¦å¤ªå¤§ï¼Œå› ä¸ºæˆ‘ä»¬è®­ç»ƒçš„æ—¶å€™ï¼Œç»å¸¸æœ‰çƒ§å¡ã€loss ç‚¸ã€æ•°æ®é…é”™äº†ï¼Œç­‰ä¸å¯æ§çš„å¤©ç¾äººç¥¸ï¼Œæ‰€ä»¥å›é€€åˆ°ä¸Šä¸ªæ•°æ®å—è¿›è¡Œç»­è®­æ˜¯ä¸€ä¸ªå¾ˆé¢‘ç¹çš„æ“ä½œã€‚è¾ƒå¤§çš„æ•°æ®å—è‡ªç„¶ä¼šå¯¼è‡´æ¨¡å‹ç‰ˆæœ¬å›é€€æ—¶æŸå¤±çš„ç®—åŠ›ä¹Ÿè¾ƒå¤šã€‚è¿™é‡Œï¼Œæˆ‘æ¨èæ¯ä¸ªæ•°æ®å—éƒ½ä»¥ B ä¸ºå•ä½ï¼Œæ­£å¥½æ˜¯ 1Bã€2Bã€4B ç­‰ã€‚

äºŒæ¬¡é¢„è®­ç»ƒ
1. ä¸»è¦æ•°æ®æ ¼å¼ï¼šæ–‡æœ¬æ®µ
2. é€‚ç”¨ç±»å‹ï¼šé€šç”¨æ”¯æŒæˆ–æ— æ³•æœ‰æ•ˆæ•´ç†æˆQAçŸ¥è¯†çš„
```
# txtæ ¼å¼
Machine learning (ML) is a field devoted to understanding and building methods that let machines "learn" â€“ that is, methods that leverage data to improve computer performance on some set of tasks.
# jsonæ ¼å¼
{
    "completion": "ä¸‹åŸ”ï¼Œæ˜¯å°æ¹¾å®œå…°å¿å¤´åŸé•‡çš„ä¸€ä¸ªä¼ ç»Ÿåœ°åŸŸåç§°ï¼Œä½äºè¯¥é•‡å—éƒ¨ã€‚ç›¸è¾ƒäºä»Šæ—¥è¡Œæ”¿åŒºï¼Œå…¶èŒƒå›´å¤§è‡´ä¸ºåŒ…æ‹¬é¡¶åŸ”é‡Œã€ä¸‹åŸ”é‡Œã€‚\nå†å²\nå°æ¹¾æ¸…æ²»æœ«æœŸï¼Œä¸‹åŸ”åœ°åŒºä¸ºä¸€è¡—åº„ï¼Œç§°ä¸ºã€Œä¸‹åŸ”åº„ã€ï¼Œéš¶å±äºå¤´å›´å ¡ã€‚è¯¥åº„ä¸œä¸ä¸‰æŠ±ç«¹åº„ä¸ºé‚»ï¼Œå—ä¸å¤§å¡­åº„ä¸ºé‚»ï¼Œè¥¿å—è¾¹ä¸€å°æ®µä¸äºŒå›´åº„ä¸ºé‚»ï¼Œè¥¿è¾¹ä¸ºä¸­ä»‘åº„ï¼ŒåŒ—è¾¹ä¸ºé‡‘é¢åº„ã€æ–°å…´åº„ã€‚\n1901å¹´ï¼ˆæ—¥æ²»æ˜æ²»ä¸‰åå››å¹´ï¼‰11æœˆï¼ŒåºŸå¿å…æ”¹è®¾äºŒåå…ï¼Œè¯¥åº„éš¶å±äºå®œå…°å…ï¼Œç¼–å…¥ç¬¬å››åŒºã€‚1905å¹´ï¼ˆæ˜æ²»ä¸‰åå…«å¹´ï¼‰7æœˆï¼Œç¬¬å››åŒºæ”¹åã€ŒäºŒå›´åŒºã€ã€‚1909å¹´ï¼ˆæ˜æ²»å››åäºŒå¹´ï¼‰10æœˆï¼Œåˆå¹¶äºŒåå…ä¸ºåäºŒå…ï¼Œè¯¥åº„ä»éš¶å±äºå®œå…°å…ã€‚1920å¹´ï¼ˆå¤§æ­£ä¹å¹´ï¼‰ï¼ŒåºŸåäºŒå…æ”¹è®¾äº”å·äºŒå…ï¼Œè¯¥åº„æ”¹åˆ¶ä¸ºã€Œä¸‹åŸ”ã€å¤§å­—ï¼Œéš¶å±äºå°åŒ—å·å®œå…°éƒ¡å¤´å›´åº„ï¼Œå¤§å­—ä¸‹æœ‰ã€Œä¸‹åŸ”ã€ã€ã€Œé¡¶åŸ”ã€å°å­—åã€‚\næˆ˜åå¤´å›´åº„æ”¹åˆ¶ä¸ºå¤´å›´ä¹¡ï¼Œéš¶å±äºå°åŒ—å¿ï¼Œå¤§å­—äº¦æ”¹åˆ¶ä¸ºæ‘ã€‚1946å¹´9æœˆï¼Œå¤´å›´ä¹¡æ›´åä¸ºå¤´åŸä¹¡ã€‚1948å¹´å†æ”¹åˆ¶ä¸ºå¤´åŸé•‡ï¼Œæ‘æ”¹åˆ¶ä¸ºé‡Œã€‚1950å¹´10æœˆï¼ŒåŒ—ã€åŸºã€å®œåˆ†æ²»ï¼Œå¤´åŸé•‡æ”¹éš¶å±äºå®œå…°å¿ã€‚\nèšè½\næœ¬åœ°åŒºå‘å±•è¾ƒæ—©çš„èšè½æœ‰é¡¶åŸ”ã€ä¸‹åŸ”ç­‰ï¼Œåœ¨æ—¥æ²»æœŸåˆæœŸçš„å®˜æ–¹åœ°å›¾ä¸Šå·²æœ‰è®°è½½ã€‚\näº¤é€š\nå°é“å®œå…°çº¿æ˜¯å°æ¹¾ä¸œåŒ—éƒ¨é“è·¯å¹²çº¿ï¼Œå¤§è‡´ä»¥ä¸œåŒ—â€”è¥¿å—èµ°å‘ç»è¿‡ä¸‹åŸ”åœ°åŒºè¥¿åŒ—éƒ¨ã€‚å¢ƒå†…è®¾æœ‰é¡¶åŸ”è½¦ç«™ï¼Œå±æ‹›å‘¼ç«™ï¼Œåªåœé åŒºé—´è½¦ã€‚ç”±æ­¤å¯å‰å¾€å°é“æ²¿çº¿å„åœ°ã€‚\nçœé“å°2çº¿ï¼ˆå¤´æ»¨è·¯ä¸‰æ®µï¼‰æ˜¯å°æ¹¾æ»¨æµ·å…¬è·¯ç³»ç»Ÿä¹‹ä¸€ï¼Œå…¶ä¸­åŸºéš†è‡³è‹æ¾³è·¯æ®µåˆç§°ä¸ºã€ŒåŒ—éƒ¨æ»¨æµ·å…¬è·¯ã€ï¼Œå¤§è‡´ä»¥çºµå‘è½¬è¥¿åŒ—â€”ä¸œå—èµ°å‘ç»è¿‡æœ¬åœ°åŒºä¸œåŒ—ç«¯ã€‚ç”±è¯¥é“è·¯å‘åŒ—å¯å‰å¾€å¤´åŸå¸‚åŒºã€è´¡å¯®ã€ç‘èŠ³ã€åŸºéš†ç­‰åœ°ï¼Œå‘ä¸œå—è½¬å—å¯å‰å¾€å£®å›´ã€äº”ç»“ã€è‹æ¾³ç­‰åœ°ã€‚\nçœé“å°2åºšçº¿ï¼ˆé’äº‘è·¯äºŒæ®µï¼‰æ˜¯å¤´åŸè‡³äºŒåŸçš„å¹²çº¿ï¼Œå¤§è‡´ä»¥ä¸œåŒ—â€”è¥¿å—èµ°å‘ç»è¿‡æœ¬åœ°åŒºè¥¿åŒ—éƒ¨è¾¹ç•Œåœ°å¸¦ã€‚ç”±è¯¥é“è·¯å‘ä¸œåŒ—å¯å‰å¾€å¤´åŸå¸‚åŒºå—ä¾§å¹¶æ­¢äºçœé“å°2çº¿è·¯å£ï¼Œå‘è¥¿å—å¯å‰å¾€ä¸­ä»‘åŒ—éƒ¨ã€äºŒåŸå¹¶æ­¢äºçœé“å°9çº¿è·¯å£ï¼Œäº¦å¯äºè¯¥å¤„è¿æ¥å›½é“5å·å¤´åŸäº¤æµé“ã€‚\nå¿é“191å·æ˜¯é¡¶åŸ”è‡³å®œå…°çš„é“è·¯ï¼Œå…¶åŒ—ä¾§ç«¯ç‚¹ä½äºæœ¬åœ°åŒºè¥¿åŒ—éƒ¨è¾¹ç•Œä¸Šçš„çœé“å°2åºšçº¿è·¯å£ã€‚ç”±æ­¤å‘å—å—è¥¿å‡ºå¢ƒåï¼Œå¯å‰å¾€ä¸­ä»‘ã€å¤§å¡­ã€äº”è‚¡ã€èŒ…åŸ”ã€è¸è¸ã€æŠµç¾ã€äº”é—´ã€å£®ä¸ƒå¹¶æ­¢äºå®œå…°å¸‚åŒºä¸œä¾§çš„çœé“å°7çº¿è·¯å£ã€‚\nå­¦æ ¡\n* äºŒåŸå›½å°ï¼ˆå¤§éƒ¨åˆ†ï¼‰",
    "source": "wikipedia.zh2307"
}
```

é˜¿é‡Œçš„data-juicerå·¥å…·å°†æ¯ä¸ªæ•°æ®å¤„ç†æ­¥éª¤æŠ½è±¡ä¸ºä¸€ä¸ªç®—å­ï¼Œç”¨æˆ·å¯ä»¥æ–¹ä¾¿çš„é…ç½®yamlæ–‡ä»¶å®ç°è‡ªå®šä¹‰çš„æ•°æ®å¤„ç†æµç¨‹ã€‚

## ç»§ç»­é¢„è®­ç»ƒ

ç»§ç»­é¢„è®­ç»ƒæ˜¯åœ¨å·²ç»é¢„è®­ç»ƒçš„æ¨¡å‹åŸºç¡€ä¸Šï¼Œè¿›ä¸€æ­¥åœ¨ç‰¹å®šé¢†åŸŸçš„æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»¥æé«˜æ¨¡å‹å¯¹è¯¥é¢†åŸŸçš„ç†è§£å’Œé€‚åº”èƒ½åŠ›ã€‚æ•°æ®é›†é€šå¸¸æ˜¯æœªæ ‡æ³¨çš„ï¼Œå¹¶ä¸”è§„æ¨¡è¾ƒå¤§ã€‚
1. æ··åˆæ•°æ®ï¼Œå¦‚æœæƒ³è¦é¢†åŸŸçš„æ¨¡å‹è¿˜å…·å¤‡ä¸€å®šçš„é€šç”¨èƒ½åŠ›ï¼Œå³é€šç”¨çš„èƒ½åŠ›ä¸ä¼šé€€åŒ–ï¼ˆæˆ–è€…ç¾éš¾æ€§é—å¿˜ï¼‰è¿™å°±éœ€è¦åœ¨è¯­è¨€æ¨¡å‹è®­ç»ƒçš„æ—¶å€™æ··æ‚é€šç”¨çš„æ•°æ®ã€‚
2. è¦ä¸è¦ä»é›¶è®­ã€‚å›é¡¾äººå¯¹çŸ¥è¯†çš„ç†è§£ï¼šå°å­¦ä¸­å­¦éƒ½åœ¨å­¦ä¹ é€šç”¨é¢†åŸŸçš„çŸ¥è¯†ï¼Œç„¶åå¤§å­¦é˜¶æ®µç»§ç»­è¿›ä¸€æ­¥å­¦ä¹ ç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†ã€‚æ‰€ä»¥åœ¨é€šç”¨æ¨¡å‹çš„åŸºç¡€ä¸Šç»§ç»­äºŒæ¬¡é¢„è®­ç»ƒæ³¨å…¥é¢†åŸŸçŸ¥è¯†æ˜¯åˆç†çš„ã€‚ä½†æ˜¯å¦‚æœæƒ³é€šè¿‡äºŒæ¬¡é¢„è®­ç»ƒè¿›è¡Œè¯­è¨€å±‚é¢çš„è¿ç§»å°±ä¼šæ¯”è¾ƒéš¾ï¼Œæ²¡æœ‰ä»é›¶å¼€å§‹è®­ç»ƒå¥½ã€‚å›é¡¾äººå¯¹è¯­è¨€çš„å­¦ä¹ ï¼Œå¦‚æœåˆšâ€œå‡ºç”Ÿâ€æ—¶å€™å°±åœ¨å­¦ä¹ ä¸€é—¨è¯­è¨€ï¼Œè¿›è¡Œå¬è¯´è¯»å†™çš„è®­ç»ƒï¼Œè¿™å°±æ˜¯æ¯è¯­äº†ã€‚ä¼šæ¯”é•¿å¤§ä»¥åå†å»å­¦ä¹ ä¸€é—¨å¤–è¯­è¦å®¹æ˜“çš„å¤šï¼Œæ•ˆæœä¹Ÿè¦å¥½å¾ˆå¤šã€‚æ‰€ä»¥åŸºäºllamaåšçš„ä¸­æ–‡é€‚é… ä¸å¦‚ çº¯ä¸­æ–‡è®­ç»ƒçš„baichuan åœ¨ä¸­æ–‡ä»»åŠ¡ä¸Šæ•ˆæœå¥½ã€‚

[æµ…è°ˆ-é¢†åŸŸæ¨¡å‹è®­ç»ƒ](https://mp.weixin.qq.com/s/qZ97QM0qV-vfWYQ0KGG6UQ) æåˆ°äº†å¾ˆå¤špre-training å’Œpost-training çš„why/trickã€‚
pretrain æœ€é‡è¦çš„å‡ ä¸ªä¸œè¥¿ï¼šæ•°æ®ï¼Œå­¦ä¹ ç‡ï¼Œä¼˜åŒ–å™¨ï¼
1. æ•°æ®å°±ä¸å¤šè¯´äº†ï¼Œè´¨é‡ä¸ºç‹ï¼Œè®°å¾—å»é‡ï¼
2. å­¦ä¹ ç‡ï¼šæ¨¡å‹çš„æ›´æ–°å¹…åº¦ï¼Œsizeè¶Šå¤§çš„æ¨¡å‹ï¼Œç‰¹å¾ç©ºé—´è¶Šå¤§ã€è¡¨è¾¾èƒ½åŠ›å’Œå­¦ä¹ èƒ½åŠ›è¶Šå¼ºï¼Œå› æ­¤å­¦ä¹ ç‡ä¹Ÿåº”è¯¥å°ä¸€ç‚¹ï¼ˆåšä¸ªå‡è®¾ï¼Œæ¨¡å‹ size æ— é™å¤§ï¼Œæœ‰æ— æ•°çš„ç¥ç»å…ƒï¼Œé‚£ä¹ˆå®ƒå®Œå…¨å¯ä»¥å¯ç”¨æ²¡ç”¨åˆ°çš„ç¥ç»å…ƒæ¥å­¦ä¹ æ–°çŸ¥è¯†ï¼Œè¿™æ ·å°±é¿å…äº†é—å¿˜æ—§çŸ¥è¯†è¿™ä¸ªç°è±¡çš„å‘ç”Ÿï¼‰ã€‚
3. ä¼˜åŒ–å™¨ï¼šAdam çš„åŸºç¡€çŸ¥è¯†æˆ‘å°±ä¸è°ˆäº†ï¼Œè¿™é‡Œåªå¼ºè°ƒä¸€ç‚¹ï¼Œæ¨¡å‹çš„ä¼˜åŒ–æ–¹å‘æ˜¯â€œå†å²åŠ¨é‡â€å’Œâ€œå½“å‰æ•°æ® gradâ€å…±åŒå†³å®šçš„ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œä¸ç®¡å½“å‰æ•°æ®å¤š badï¼Œä¼˜åŒ–å™¨éƒ½ä¼šé™åˆ¶ä½ åšå‡ºå¤ªå¤§å¹…åº¦çš„æ›´æ–°ï¼Œæ¢¯åº¦è£å‰ª/æ¢¯åº¦æ­£åˆ™ç±»ä¼¼ã€‚å› æ­¤ï¼ŒåŸºæœ¬å¯ä»¥è®¤ä¸ºæˆ‘ä»¬çš„æ¨¡å‹å…·æœ‰ä¸€å®šçš„æŠ—å™ªèƒ½åŠ›ã€‚

ç›®å‰ï¼Œå¤§å®¶åŸºæœ¬éƒ½é»˜è®¤ä½¿ç”¨å¦‚ä¸‹ä¸‰ä¸ªæ­¥éª¤è¿›è¡Œ pretrainï¼š
1. warmupï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå°†å­¦ä¹ ç‡æ…¢æ…¢æé«˜ã€‚ï¼ˆå¯ä»¥è¿™ä¹ˆç†è§£ï¼Œä½ çš„æ¨¡å‹è¿˜æ²¡æœ‰ç§¯æ”’è¶³å¤Ÿçš„åŠ¨é‡å»æŠ—å™ªï¼Œå¤ªå¤§çš„å­¦ä¹ ç‡å®¹æ˜“é€ æˆä¸å¯é€†çš„å½±å“ï¼‰
2. linear / constant / cosine decayï¼šç»´æŒç¨³å®šçš„å­¦ä¹ ç‡ï¼Œæˆ–è€…ç¼“æ…¢è¡°å‡çš„å­¦ä¹ ç‡ã€‚
3. Annealï¼šç”¨å°å­¦ä¹ ç‡å»å­¦é«˜ç²¾æ•°æ®ï¼ŒIFTæ•°æ®ï¼Œé€»è¾‘æ•°æ®ï¼Œå»æé«˜é€šç”¨é€»è¾‘èƒ½åŠ›èƒ½åŠ›å’Œæ‰“æ¦œèƒ½åŠ›ã€‚

ä¹‹å‰çœ‹è§æœ‰ç§è¯´æ³•è¯´æ´—æ•°æ®æ˜¯è„ç®€å†çš„å·¥ä½œï¼Œæ•æˆ‘ä¸èƒ½è®¤åŒã€‚**å¦‚æœ infra å›¢é˜Ÿå·²ç»å¸®å¿™è°ƒé€šäº† megatron çš„è®­ç»ƒä»£ç ï¼Œé‚£ä¹ˆè®­ç»ƒæ‰æ˜¯çœŸçš„æœ€æ²¡æŠ€æœ¯å«é‡çš„å·¥ä½œ**ï¼Œæ”¹å‡ ä¸ªå‚æ•°ï¼Œç„¶å bash train.shï¼Œè®­ç»ƒæŒ‚äº†å°±é‡å¯ï¼Œè¿™äº›å·¥ä½œè°åšä¸æ¥å‘¢ï¼Ÿåå€’æ˜¯æ´—æ•°æ®æ—¶çš„çµå…‰ä¸€ç°ï¼Œå¾€å¾€èƒ½å¤§å¤§æå‡æ¨¡å‹çš„æ•ˆæœã€‚

å¤§æ¨¡å‹é¢„è®­ç»ƒä¸­ï¼Œé€€ç«é€šå¸¸æŒ‡çš„æ˜¯ä¸€ç§å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥ã€‚åˆå§‹é˜¶æ®µä½¿ç”¨è¾ƒå¤§çš„å­¦ä¹ ç‡ä»¥å¿«é€Ÿæ¢ç´¢å‚æ•°ç©ºé—´ï¼Œéšåé€æ¸å‡å°ï¼Œä»¥ç»†åŒ–æ¨¡å‹çš„å‚æ•°è°ƒæ•´ã€‚

### æ•°æ®å®éªŒï¼šåŒæºå°æ¨¡å‹æ˜¯å¤§æ¨¡å‹çš„å®éªŒåœº

[å¤§æ¨¡å‹ VS å°æ¨¡å‹](https://mp.weixin.qq.com/s/QLq64i3VSWTO6vzeVnr3mQ)scaling law å‘Šè¯‰æˆ‘ä»¬ï¼šå°æ¨¡å‹çš„æ€§èƒ½è¡¨ç°èƒ½ç”¨æ¥é¢„æµ‹å¤§æ¨¡å‹çš„æ€§èƒ½è¡¨ç°ã€‚è¿™ä¹Ÿå°±æ˜¯è¯´ï¼Œå¤§éƒ¨åˆ†æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æ˜¯å¯ä»¥é€šè¿‡åœ¨åŒæºå°æ¨¡å‹ä¸Šåšå®éªŒï¼Œå»é¢„æµ‹å¤§æ¨¡å‹çš„æ•ˆæœçš„ã€‚

åœ¨ pretrain / post_pretrain é˜¶æ®µæœ‰å¾ˆå¤šéœ€è¦åšå®éªŒæ‰èƒ½çŸ¥é“ç­”æ¡ˆçš„é—®é¢˜ã€‚æ€ä¹ˆæ ·çš„æ•°æ®é…æ¯”æœ€åˆç†ï¼Œè¯¾ç¨‹å­¦ä¹ ä¸­å“ªç§å­¦ä¹ é¡ºåºæ•ˆæœæœ€å¥½ï¼Œæ•°æ®çš„è´¨é‡æ˜¯å¦è¿‡å…³ï¼Œæ•°æ®çš„å»é‡ç¨‹åº¦æ˜¯å¦è¿‡å…³ï¼Œå…ˆè®­4kã€å†æ‰©åˆ° 32k å’Œç›´æ¥è®­ 32k çš„æ•ˆæœå·®å¼‚ï¼Œpost_pretrain çš„æ—¶å€™æ€æ ·è°ƒæ•´å­¦ä¹ ç‡å’Œæ•°æ®åˆ†å¸ƒæ¥é˜²æ­¢æ¨¡å‹æ–­å´–å¼çš„èƒ½åŠ›é—å¿˜ï¼Ÿ

ç›´æ¥å¯åŠ¨å¤§æ¨¡å‹çš„æˆæœ¬å®åœ¨æ˜¯åœ¨å¤ªé«˜æ˜‚äº†ï¼Œå¯èƒ½è®­ç»ƒä¸¤ä¸‰å‘¨ï¼Œloss æ›²çº¿æ‰ä¼šè¡¨ç°å‡ºä¸€ç‚¹ç‚¹å·®å¼‚ã€‚ä½†æˆ‘ä»¬å®Œå…¨å¯ä»¥åœ¨å°æ¨¡å‹ä¸Šå¤§èƒ†çš„è®­ï¼Œæ¯å¤©è®­ 100B tokenï¼Œä¸¤å¤©å°±èƒ½å‡ºä¸€ç‰ˆå®éªŒç»“æœã€‚è§‚å¯Ÿ tensorbord çš„ loss æ›²çº¿ï¼Œåˆ· benchmark æ‰“æ¦œï¼Œæˆ–æ˜¯åš sft çœ‹æ•ˆæœï¼Œæ€»ä¹‹å°æ¨¡å‹å¯ä»¥å¸®åŠ©æˆ‘ä»¬å¿«é€Ÿåœ°æ•²å®š pretrain é˜¶æ®µä½¿ç”¨çš„æ•°æ®é…ç½®ã€‚

åœ¨ alignment é˜¶æ®µï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å»å€ŸåŠ©å°æ¨¡å‹å’Œ scaling law æ¥æŒ‡å¯¼å·¥ä½œã€‚æˆ‘è¦å¼ºåŒ–æ¨¡å‹çš„æŸä¸ªèƒ½åŠ›ï¼Œå‡†å¤‡äº† N æ¡è®­ç»ƒæ•°æ®ï¼Œèƒ½è®©æ¨¡å‹è¾¾åˆ°å¤šå¤§çš„æå‡å‘¢ï¼Ÿå¯ä»¥çœ‹çœ‹è¿™ä»½æ•°æ®åœ¨å°æ¨¡å‹ä¸Šèƒ½æœ‰å¤§æå‡ï¼Œç»˜åˆ¶ä¸€æ¡æ›²çº¿ï¼Œå»é¢„ä¼°å¤§æ¨¡å‹çš„æ€§èƒ½è¡¨ç°ã€‚è¯´çš„å†é€šä¿—ä¸€ç‚¹ï¼Œ100B token èƒ½è®© 0.5B æ¨¡å‹ä¸‹é™ 0.2 lossï¼Œèƒ½è®© 72B æ¨¡å‹ä¸‹é™ 0.1 lossï¼Œ alignment æ•°æ®èƒ½è®© 0.5B æ¨¡å‹æé«˜ x% çš„ task èƒ½åŠ›ï¼Œé‚£ä¹ˆå¤§æ¦‚ç‡è¿™ä»½æ•°æ®ä¹Ÿåªèƒ½è®© 72B æ¨¡å‹æå‡ 0.5x % çš„ task èƒ½åŠ›ã€‚

å…·ä½“çš„å®éªŒå†…å®¹ï¼Œå¯ä»¥æ ¹æ®è‡ªå·±çš„æ—¶é—´ã€äººåŠ›æ¥ä¸‰ä¸ªé˜¶æ®µèµ°ï¼š
1. ç²—ç³™ä¸€ç‚¹çš„å·¥ä½œï¼šåœ¨å°æ¨¡å‹ä¸Šèµ·å¤šä¸ªæ•°æ®é…æ¯”ã€æ•°æ®é¡ºåºï¼Œè®­ç»ƒ 500B å·¦å³çš„æ•°æ®é‡ï¼Œç„¶å**é€‰æ‹© loss æ›²çº¿æœ€å®Œç¾ï¼Œæˆ–è€… loss ä¸‹é™æœ€ä½çš„é‚£ä¸ªæ¨¡å‹**ï¼ˆè¿™ä¸ªé˜¶æ®µåˆ· benchmark æ„ä¹‰ä¸å¤§ï¼Œæ¨¡å‹å°ï¼Œè®­å¾—å°‘ï¼Œå¤§æ¦‚ç‡éƒ½æ˜¯çè’™ï¼‰ï¼›
2. ä¸“ä¸šä¸€ç‚¹çš„å·¥ä½œï¼šé¢å¤–èµ·å¤šä¸ª size çš„å°æ¨¡å‹ï¼Œè·‘å‡º loss ç»“æœï¼Œç»“åˆ scaling_law å…¬å¼ï¼Œå»æ¨ç®—å¤§æ¨¡å‹æœ€é€‚åˆçš„æ•°æ®é…æ¯”ã€å­¦ä¹ ç‡ã€è®­ç»ƒ token é‡ç­‰å‚æ•°ï¼›
3. åˆ›æ–°ä¸€ç‚¹çš„å·¥ä½œï¼šåƒ llama å’Œ deepseek æŠ€æœ¯æŠ¥å‘Šé‡Œæåˆ°çš„ä¸€æ ·ï¼Œå»ç»˜åˆ¶å‡º loss åˆ° benchmark çš„ scaling_lawï¼Œæå‰é¢„çŸ¥æ¨¡å‹è®­å¤šå°‘ token é‡èƒ½åœ¨æŸä¸ª benchmark è¾¾åˆ°ä»€ä¹ˆæ ·çš„èƒ½åŠ›ã€‚
è¿™ä¸ªåœ°æ–¹å±•å¼€è¯´çš„è¯ï¼Œèƒ½è¯´å·¨å¤šçš„ä¸œè¥¿ï¼Œä½†ä¸æ–¹ä¾¿ç»†è¯´ï¼Œæ„Ÿå…´è¶£çš„åŒå­¦è¿˜æ˜¯å¤šè¯»è¯»å„å…¬å¸çš„æŠ€æœ¯æŠ¥å‘Šå§ã€‚scaling_law åªæ˜¯æ”¾ç¼“äº†ï¼Œä¸æ˜¯æ­»äº†ï¼Œåœ¨æ²¡æœ‰æ–°çš„æŠ€æœ¯æŒ‡å¼•çš„æƒ…å†µä¸‹ï¼Œscaling_law ä½ ä¸ä¿¡ä¹Ÿå¾—ä¿¡ï¼Œå®ƒæ¯•ç«Ÿæ˜¯ç”¨æŸç§è§„åˆ™åœ¨åšè®­ç»ƒï¼ŒæŒ‰ç…§è‡ªå·±çš„ç›´è§‰æ¥åšè®­ç»ƒåŸºæœ¬ç­‰äºâ€œrandomâ€ã€‚

### å¤§æ¨¡å‹èƒŒåçš„æ— æ•°å°æ¨¡å‹

ä¸€ä¸ªä¼˜ç§€çš„å¤§æ¨¡å‹ï¼Œæ— è®ºæ˜¯åœ¨è®­ç»ƒé˜¶æ®µï¼Œè¿˜æ˜¯çº¿ä¸Šéƒ¨ç½²é˜¶æ®µï¼Œå…¶èƒŒåé»˜é»˜ä»˜å‡ºçš„å°æ¨¡å‹éƒ½æ•°ä¸èƒœæ•°ã€‚
1. æ•°æ®è´¨é‡åˆ†ç±»å™¨ï¼šllama3 å’Œ qwen2 éƒ½æåˆ°äº†ï¼Œä»–ä»¬çš„ pretrain è®­ç»ƒæ•°æ®æ˜¯æœ‰å¾—åˆ†çš„ï¼Œç„¶åé€šè¿‡é˜ˆå€¼æ¥æ‰¾å‡ºæœ€é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ï¼Œå¼€æº pretrain æ•°æ®é›† fineweb ä¹Ÿæåˆ°äº†ä»–ä»¬ç»™æ•°æ®æ‰“åˆ†çš„å·¥ä½œã€‚Good data makes good model performanceï¼ææ²å¤§ä½¬åœ¨ä»–çš„è§†é¢‘é‡Œè¯´åˆ°ï¼Œllama3 çš„æ•°æ®æ‰“åˆ†å™¨æ˜¯ RoBERTaï¼Œè¿™å¾ˆåˆç†ï¼Œæ•ˆæœåˆå¥½ã€æ¨ç†åˆå¿«çš„åˆ†ç±»æ¨¡å‹ç¡®å®è¿˜è¦çœ‹ BERT å®¶æ—ã€‚
2. æ•°æ® domain åˆ†ç±»å™¨ï¼šå‚ç›´é¢†åŸŸæ¨¡å‹çš„ post_pretrain å·¥ä½œï¼Œå¾€å¾€éœ€è¦éå¸¸ç²¾å‡†çš„æ•°æ®é…æ¯”ï¼Œdomain æ•°æ®çš„æ•°æ®è´¨é‡ä¹Ÿéœ€è¦éå¸¸ä¼˜è´¨ã€‚è¿™ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªåˆ†ç±»å™¨ï¼Œå»æå–æµ·é‡æ•°æ®ä¸­çš„ domain æ•°æ®ï¼Œè¿™ä¸ªåˆ†ç±»å™¨æœ€å¥½è¿˜èƒ½æŠŠä½è´¨é‡çš„ domain æ•°æ®ä¹Ÿè§†ä¸ºé domain æ•°æ®ï¼Œé€šå¸¸æ‰¿æ‹…è¿™ä¸ªå·¥ä½œçš„æ¨¡å‹ä¹Ÿæ˜¯ BERT å®¶æ—ã€‚

### Tokenizer

å¤§æ¨¡å‹æ— æ³•ç›´æ¥å¤„ç†æ–‡æœ¬ï¼Œéœ€è¦è½¬æ¢ä¸ºæ•°å­—ã€‚è½¬æ¢è¿‡ç¨‹å¯ä»¥ç®€åŒ–ä¸ºä»¥ä¸‹å‡ æ­¥ï¼šè¾“å…¥æ–‡æœ¬åºåˆ—ï¼Œå¯¹åºåˆ—è¿›è¡Œåˆ‡åˆ†æˆä¸ºTokenåºåˆ—ï¼Œå†æ„å»ºè¯å…¸å°†æ¯ä¸ªTokenæ˜ å°„ä¸ºæ•´æ•°å‹çš„indexã€‚**é€šå¸¸æƒ…å†µä¸‹ï¼ŒTokenizeræœ‰ä¸‰ç§ç²’åº¦ï¼šword/char/subword**ã€‚llmä½¿ç”¨çš„ Tokenizer ä¸€èˆ¬ä¸ºsubwordã€‚

æ‰©è¯è¡¨å®¹æ˜“æŠŠè¯è¡¨æ‰©é”™ï¼Œè¿™å’Œå­—å…¸æ ‘çš„é€»è¾‘æœ‰å…³ã€‚ç®€å•æ¥è¯´ï¼Œå°±æ˜¯ä½ åŠ å…¥â€œä¸­åäººæ°‘â€è¿™ä¸ªæ–° tokenï¼Œå¹¶ä¸”å¼•å…¥ç›¸å¯¹åº”çš„ merge token çš„é€»è¾‘ï¼Œå°±å¯èƒ½å¯¼è‡´â€œä¸­åäººæ°‘å…±å’Œå›½â€è¿™ä¸ªæ—§ token æ°¸è¿œä¸ä¼šè¢« encode å‡ºæ¥ï¼Œé‚£â€œä¸­åäººæ°‘å…±å’Œå›½â€è¿™ä¸ª token å¯¹åº”çš„çŸ¥è¯†ä¹Ÿå°±ä¸¢å¤±äº†ã€‚å› æ­¤ï¼Œæå‰å‡†å¤‡å¥½è‡ªå·±çš„ tokenizer çœŸçš„éå¸¸é‡è¦ï¼Œè¿™å°±æ˜¯ä¸€ä¸ªæ‰“åœ°åŸºçš„å·¥ä½œã€‚ä½ å¦‚æœæƒ³ç€åæœŸå¯ä»¥æ‰©è¯è¡¨è§£å†³ï¼Œé‚£å’Œæˆ¿å­æ­ªäº†ä½ å†åŠ ä¸€æ ¹æŸ±å­æ’‘ç€æœ‰å•¥åŒºåˆ«å‘¢ï¼Ÿllama + æ‰©ä¸­æ–‡è¯è¡¨ + conitnue pretrain è®­å‡ºæ¥è¿‡æ•ˆæœæƒŠè‰³çš„ä¸­æ–‡æ¨¡å‹å—ï¼Ÿè‡³äºæ€ä¹ˆè®­ tokenizerï¼šæ‰¾ä¸€ä¸ªå†…å­˜ç©ºé—´å¾ˆå¤§çš„ cpu æœºå™¨ï¼Œå†æ‰¾ä¸€ä»½å¾ˆå¤§çš„ common æ•°æ®é›†ï¼Œç„¶ååˆ©ç”¨ BPE / BBPE ç®—æ³•å»è·‘ï¼ˆChatGPTä¼šå†™ï¼‰ï¼Œè¿™é‡Œåªæé†’ä¸€äº›ç»†èŠ‚ã€‚

1. æ•°å­—åˆ‡åˆ†ï¼ˆè™½ç„¶ä¸çŸ¥é“ OpenAI ä¸ºä»€ä¹ˆä¸åšäº†ï¼Œä½†æˆ‘ä»¬è¿˜æ˜¯åšå§ï¼Œé¿å… 9.9 > 9.11 çš„é—®é¢˜å›ç­”ä¸æ­£ç¡®)ï¼›
2. æ§åˆ¶å‹ç¼©ç‡ï¼Œ1 ä¸ª token å¯¹åº”å¤šå°‘ä¸ªæ±‰å­—ï¼šå‹ç¼©ç‡å¤ªä½ï¼Œé‚£å°±æ˜¯å­—å¤ªå¤šã€è¯å¤ªå°‘ï¼Œå¾ˆå½±å“è§£ç æ•ˆç‡ï¼›å‹ç¼©ç‡å¤ªå¤§ï¼Œä¹Ÿå°±æ˜¯è¯å¤ªå¤šï¼Œåˆä¼šå½±å“æ¨¡å‹çš„çŸ¥è¯†èƒ½åŠ›ã€‚é€šå¸¸ï¼Œå‹ç¼©ç‡è¶Šä½çš„æ¨¡å‹ï¼Œloss ä¹Ÿä¼šä½ï¼Œå¤§éƒ¨ä»½ä¸­æ–‡å¤§æ¨¡å‹çš„1 ä¸ª token ä¼šæ˜ å°„æˆ 1.5 ä¸ªæ±‰å­—å·¦å³ï¼›
3. æ‰‹åŠ¨ç§»é™¤è„ tokenï¼Œä¹‹å‰ GPT4o çš„ token è¯è¡¨æ³„æ¼ï¼Œ å°±è¢«å‘ç°æœ‰å¾ˆå¤šä¸­æ–‡çš„è‰²æƒ…ã€èµŒåš tokenï¼›
å¦‚æœæå‰çŸ¥é“è‡ªå·±çš„ä¸šåŠ¡åœºæ™¯ï¼Œé‚£å°±åº”è¯¥è¡¥å……ä¸šåŠ¡åœºæ™¯å¯¹åº”çš„ tokenï¼Œå¢åŠ ä¸šåŠ¡åœºæ™¯æ–‡æœ¬çš„å‹ç¼©ç‡ï¼Œæ¯”å¦‚åŒ»ç–—åœºæ™¯ï¼Œå°±æå‰æŠŠé˜¿è«è¥¿æ—ã€é’éœ‰ç´ ç­‰ä½œä¸ºä¸€ä¸ª tokenï¼›
4. è¯è¡¨çš„ä¸­ã€è‹±è¦†ç›–ç‡è¦è¶³å¤Ÿå¤§ï¼Œè‡³äºå…¶ä»–å°è¯­ç§æ˜¯å¦è¦åŠ ï¼Œåˆ™çœ‹ä¸šåŠ¡éœ€æ±‚ï¼›
5. tokenizer çš„ vocab_len å’Œæ¨¡å‹çš„ embdding_size ä¹‹é—´ï¼Œè¦æœ‰ä¸€åƒä¸ªå·¦å³çš„ bufferï¼Œåç»­çš„ alignment ç¯èŠ‚ï¼Œéœ€è¦å¤§é‡åœ¨ pretrain é˜¶æ®µæ²¡è§è¿‡çš„å…¨æ–° token æ¥åšè®­ç»ƒã€‚

[LLMå®è·µç³»åˆ—-è¯¦è°ˆTokenizerè®­ç»ƒç»†èŠ‚](https://mp.weixin.qq.com/s/oMpikx1J0XhjQbIJsF9CFQ)

â€œå¤§æ¨¡å‹è§†è§’â€ä¸‹çš„æ–‡æœ¬ä¸æˆ‘ä»¬â€œäººç±»è§†è§’â€å¹¶ä¸ä¸€æ ·ã€‚ä»¥Strawberryä¸ºä¾‹ï¼Œå…¶è¢«æ‹†åˆ†ä¸ºã€â€œstrâ€, â€œawâ€, â€œberryâ€ã€‘ï¼Œè‡ªç„¶ä¹Ÿå°±ä¸éš¾ç†è§£ä¸ºä»€ä¹ˆllmå¯¹äºè¿™ä¸ªé—®é¢˜çš„å›ç­”æ˜¯â€œä¸¤ä¸ªrâ€äº†ï¼Œæ¯•ç«Ÿå¯¹äºå®ƒæ¥è¯´ï¼Œè¿™ä¸‰è€…å‡æ˜¯ä½œä¸ºä¸€ä¸ªæ•´ä½“å­˜åœ¨çš„ï¼Œè€Œä¸æ˜¯ä»å­—æ¯ç»´åº¦åˆ‡åˆ†çš„ã€‚

### æ¨¡å‹ç»“æ„

ä¸€ä¸ªåŸåˆ™ï¼šèƒ½æŠ„ llama çš„ç»“æ„å°±ä¸è¦éšä¾¿åˆ›æ–°ï¼Œå°± rope + gqa + rms_norm + swigluï¼Œå°‘åˆ›æ–° = å°‘è¸©å‘ï¼Œåˆ›æ–°çš„å‰ææ˜¯å¤§é‡é²æ£’çš„å®éªŒã€‚å¦‚æœæ˜¯ 1B å·¦å³å¾ˆå°çš„æ¨¡å‹ï¼Œé‚£ä¹ˆ embedding å’Œ lm_head è¿˜éœ€è¦å…±äº«å‚æ•°ï¼Œç›®çš„æ˜¯è®© layer çš„å‚æ•°å å…¨å±€å‚æ•°çš„æ¯”ä¾‹å¤§ä¸€äº›ï¼Œå¤§ä¸€ç‚¹çš„æ¨¡å‹åˆ™æ²¡æœ‰è¿™ä¸ªå¿…è¦ã€‚

pretrain æ˜¯ä¸ªæˆæœ¬æé«˜çš„å·¥ä½œï¼Œä¸€åˆ‡éƒ½è¦ä»¥ç¨³å¥ä¸ºä¸»ã€‚å‡è®¾ä½ æ˜¯ pretrain è´Ÿè´£äººï¼Œä½ ä¸ºäº†å†™è®ºæ–‡ï¼Œä¸ºäº†å¼ºè¡Œå®£ä¼ è‡ªå·±çš„æ¨¡å‹æ¯” llama æ›´å…ˆè¿›ï¼Œåšäº†ä¸€ä¸¤å‘¨å®éªŒï¼Œåœ¨ 0.5B è¿™ç§ size å‘ç°æ–°ç»“æ„çš„ loss ä¸‹é™æ›´å¿«ã€‚ä½ å¤§å–œï¼Œä¹Ÿæ²¡å»æ‰¾ä¸¤ä¸ªæ•°å­¦ç³»åšå£«åšåšç†è®ºè¯æ˜æˆ–æ¨å¯¼ï¼Œå°±å»è‰è‰çš„å»æ›´æ”¹ llama ç»“æ„ã€‚ç­‰è®­ç»ƒä¸€ä¸ªå¤šæœˆä¹‹åå‘ç°è¿™ä¸ªæ–°ç»“æ„æœ‰æŸä¸ªè‡´å‘½ç¼ºé™·ï¼Œå‡ åƒä¸‡çš„ç®—åŠ›èµ„é‡‘å·²ç»æŠ•è¿›å»äº†ã€‚è€æ¿é—®ä½ ä»€ä¹ˆæƒ…å†µï¼Œä½ è¯´å½“åˆåœ¨å°æ¨¡å‹ä¸Šæ”¹ç»“æ„æ²¡å‡ºé—®é¢˜å•Šï¼è€æ¿ç»™ä½ æ‰“äº†ä½ç»©æ•ˆï¼Œè‡ªå·±å¿ƒé‡Œåˆä¸€ç™¾ä¸ªä¸æœæ°”ã€‚

æˆ‘è§‰ç€ï¼Œå›½å†… 99% çš„å¤§æ¨¡å‹æŠ€æœ¯å²—ï¼Œå¹¶ä¸é¼“åŠ±åˆ›æ–°å’Œè¯•é”™ï¼Œè€æ¿ä»¬åªé¼“åŠ±ç”¨æœ€å¿«çš„é€Ÿåº¦ã€æœ€å°‘çš„é’±å»è¿½èµ¶ OpenAIã€‚é™¤éä½ çš„è€æ¿çœŸçš„æ”¯æŒä½ ç§¯æåˆ›æ–°ï¼Œå¦åˆ™ä¸è€ƒè™‘è¯•é”™æˆæœ¬ç›²ç›®è¿½æ±‚åˆ›æ–°çš„äººçœŸçš„æœ‰ç‚¹å¤§ç—…ã€‚

### æ¨¡å‹å‚æ•°

1. æ¨¡å‹çš„ sizeã€‚æˆ‘å»ºè®®ä¸è¦æ ¹æ®è‡ªå·±çš„åœºæ™¯éœ€æ±‚æ¥æ•²å®šæ¨¡å‹ sizeï¼ˆé™¤éæ˜¯ Math ç­‰å¤æ‚ä»»åŠ¡å¿…é¡»æ˜¯æ¥è¿‘åƒäº¿å‚æ•°çš„æ¨¡å‹ï¼‰ï¼Œå°æ¨¡å‹çš„æé™åœ¨å“ªé‡Œç›®å‰ä»ç„¶æ˜¯ä¸ªæœªçŸ¥æ•°ï¼Œqwen2.5 ä¹Ÿè´´äº†ä¸€å¼ å›¾ï¼Œåœ¨ benchmark ä¸Šèƒ½è¾¾åˆ°æŸä¸ªåˆ†æ•°çš„æ¨¡å‹ sizeï¼Œåœ¨æŒç»­å˜å°ã€‚å› ä¸ºæ¨¡å‹çš„ size é€‰å°äº†ï¼Œå¯¼è‡´åç»­åº”ç”¨çš„æ—¶å€™ä¸šåŠ¡åœºæ™¯æ‰›ä¸ä½ï¼Œè¿™ç§ç°è±¡å…¶å®å¹¶ä¸å¸¸è§ï¼Œalignment åŸºæœ¬éƒ½èƒ½æ•‘å›æ¥ï¼Œæ— éå°±æ˜¯ sft çš„æ—¶å€™åœ¨ä¸šåŠ¡èƒ½åŠ›ä¸Šè¿‡æ‹Ÿåˆä¸€äº›ç½¢äº†ã€‚å› æ­¤ï¼Œé€‰æ¨¡å‹ size ä¸»è¦çœ‹ä¸¤ä¸ªå› ç´ ï¼Œè®­ç»ƒç®—åŠ›å’Œæ¨ç†ç®—åŠ›ï¼š
    1. è®­ç»ƒç®—åŠ›ï¼šæ‰‹å¤´æœ‰å¤šå°‘æœºå™¨ï¼Œèƒ½åšå¤šä¹… pretrainï¼Œæœ‰å¤šå°‘è®­ç»ƒæ•°æ®ï¼Œä½¿ç”¨çš„è®­ç»ƒæ¡†æ¶ä¸€å¤©èƒ½åƒå¤šå°‘ tokenï¼Œè¿™äº›äº‹æƒ…éƒ½æ˜¯æå‰èƒ½ç¡®å®šçš„ã€‚å‡è®¾è¦åœ¨ 2 ä¸ªæœˆåè®­å®Œæ¨¡å‹ï¼Œç›®æ ‡è®­ 2T æ•°æ®ï¼Œé‚£ä¹ˆä¾¿å¯ä»¥è®¡ç®—å‡ºè‡ªå·±è¯¥è®­ä»€ä¹ˆ size çš„æ¨¡å‹ã€‚å°½é‡å’Œå¤§å‚çš„æ¨¡å‹ size ä¿æŒä¸€è‡´ï¼Œä¸çåˆ›æ–°å°±ä¸ä¼šè¸©å¥‡å¥‡æ€ªæ€ªçš„å‘ï¼Œè€Œä¸”æ¨¡å‹æ•ˆæœå¯¹æ¯”çš„æ—¶å€™ä¹Ÿæœ‰è¯´æœåŠ›ï¼›
    2. æ¨ç†ç®—åŠ›ï¼šå®æ“è¿‡æ¨¡å‹çš„åŒå­¦éƒ½çŸ¥é“ï¼Œç”¨ AutoModelForCausalLM åŠ è½½ 33B æ¨¡å‹åŸºæœ¬æ˜¯å¡ç€ä¸€å¼  80G æ˜¾å­˜çš„æé™åœ¨æ¨ç†ï¼ŒåŠ è½½ 72B æ¨¡å‹åŸºæœ¬ä¸Šæ˜¯å¡ç€ä¸¤å¼  80G æ˜¾å­˜çš„æé™åœ¨æ¨ç†ï¼Œç¨å¾®å¤šä¸€ç‚¹ seq_lenï¼Œç«‹åˆ» OOMï¼ˆæš‚ä¸è®¨è®ºé‡åŒ–ç­‰å› ç´ ï¼‰ã€‚æ¢å¥è¯ä¹Ÿå°±æ˜¯è¯´ï¼Œå‡è®¾ä½ æœ‰ä¸ª 40B çš„æ¨¡å‹ï¼Œä»–å’Œ 72B æ¨¡å‹æ•ˆæœä¸€æ ·ï¼Œé‚£åˆæ€æ ·å‘¢ï¼Ÿä»–è¿˜æ˜¯å¾—ç”¨ä¸¤å¼ å¡æ¨ç†ï¼Œåœ¨éƒ¨ç½²æˆæœ¬ä¸Šå’Œ 72B æ¨¡å‹è¿˜æ˜¯ä¸€ä¸ªé‡çº§çš„ã€‚æ‰€ä»¥ï¼Œæ•²å®šæ¨¡å‹ size çš„æ—¶å€™ï¼Œæˆ‘ä»¬éœ€è¦çŸ¥é“è‡ªå·±çš„æ¨ç†æœºå™¨æ˜¯ä»€ä¹ˆï¼Œ**ä¸è¦å‡ºç° 1 å¼ æ¨ç†å¡åˆšå¥½è£…ä¸ä¸‹æ¨¡å‹çš„å°´å°¬ç°è±¡**ã€‚é€‚å½“ç»™æ¨¡å‹å¢å¤§ 1Bã€å‡å° 1B å‚æ•°æ˜¯å¯ä»¥çš„ï¼Œè¿™ä¹Ÿèƒ½è§£é‡Šäº†ä¸ºä»€ä¹ˆä¸æ˜¯æ‰€æœ‰å…¬å¸éƒ½ç”¨ 70Bï¼Œè€Œæ˜¯ 65Bã€70Bã€72Bã€75B ç­‰ size éƒ½æœ‰çš„ç°è±¡ã€‚
2. æ¨¡å‹çš„è¶…å‚æ•° sizeã€‚ç›®å‰å­¦ç•Œéƒ½æœ‰ä¸€ä¸ªå…±è¯†ï¼šä¸€ä¸ªå¥½æ¨¡å‹æ˜¯â€œèº«æåŒ€ç§°â€çš„ã€‚ä¹Ÿå°±æ˜¯è¯´æ ‡å‡†çš„ llama ç»“æ„åº”è¯¥æ˜¯æ¨ªå‘å’Œçºµå‘å‘ˆæŸç§æ¯”ä¾‹çš„ï¼Œæ‰€ä»¥åœ¨å¢å¤§æ¨¡å‹ size çš„æ—¶å€™ï¼Œlayer_num å’Œ hidden_size è¦ä¸€èµ·é€’å¢ã€‚å…·ä½“å¦‚ä½•é€’å¢ï¼Œllama è®ºæ–‡å’Œæç‰§è€å¸ˆè§†é¢‘é‡Œéƒ½æœ‰ä»‹ç»ï¼Œè¿™é‡Œä¸åœ¨èµ˜è¿°äº†ã€‚æˆ‘çš„è§‚ç‚¹ä¾ç„¶æ˜¯æ™®é€šäººæ²¡å¿…è¦ç ”ç©¶è¿™ä¸ªï¼Œè¶…å‚æ•° size çš„é‡çº§å°±å’Œ llama ä¿æŒä¸€è‡´å³å¯ï¼Œå°‘åˆ›æ–°å°±ç­‰äºå°‘çŠ¯é”™ã€‚ä½†å…·ä½“åˆ°è¯¥ä½¿ç”¨ä»€ä¹ˆå€¼çš„å‚æ•°ï¼Œè¿˜æ˜¯æœ‰è¯´æ³•çš„ï¼šå°½é‡èƒ½è¢« 2 / 4 / 8 / 64 / 128 ç­‰æ•°æ•´é™¤ã€‚ä¸æ˜¯æœ‰ä»€ä¹ˆç†è®ºè¯æ˜ï¼Œè€Œæ˜¯è®­ç»ƒæ¡†æ¶è¦æ±‚ä½ è¿™æ ·ã€‚
    1. layer_numï¼šæœ‰å°½é‡å¤šçš„è´¨å› æ•°ï¼Œå®ƒä¸ pipeline_parallel æœ‰å…³ã€‚pipeline_parallel çš„è¦æ±‚æ˜¯ï¼šassert layer_num % pipeline_size == 0ã€‚å¦‚æœä½ çš„ layer_num = 30ï¼Œé‚£å®ƒå°±ä¸èƒ½æ”¯æŒ pipeline_size = 4ã€‚å½“ç„¶ï¼Œä½ å¯ä»¥ä¿®æ”¹è®­ç»ƒä»£ç ï¼Œè®© pipeline_parallel çš„æ—¶å€™ï¼Œä¸åŒçš„å¡æ”¾ç½®ä¸åŒæ•°é‡çš„ layerï¼Œä½†è¿™ä¸å°±åˆå¢åŠ å¼€å‘æˆæœ¬äº†ï¼›
    2. num_headï¼š8 çš„æ•´æ•°å€ï¼Œå®ƒä¸ tensor_parallel æœ‰å…³ã€‚tensor_parallel çš„æé™ä¸€èˆ¬æ˜¯ 8ï¼Œå› ä¸ºå¤§äº 8 å°±å¼•å…¥äº†æœºé—´é€šè®¯ï¼Œè®­ç»ƒæ•ˆç‡å°±ä½äº†ã€‚tensor_parallel çš„æ•ˆç‡å¾ˆé«˜ï¼Œèƒ½å¼€å¤§å°±å°½é‡å¼€å¤§ä¸€ç‚¹ï¼›
    3. hidden_statesï¼š128 çš„æ•´æ•°å€ï¼Œç›®å‰æ²¡æœ‰ç†ç”±ï¼Œä¿ä¸é½ä»¥åæœ‰ï¼›
    4. vocab_sizeï¼š128 çš„æ•´æ•°å€ï¼Œç›®å‰æ²¡æœ‰ç†ç”±ï¼Œä¿ä¸é½ä»¥åæœ‰ã€‚
    5. å¦å¤–ä¸€ä¸ªæ¯”è¾ƒé‡è¦çš„è¶…å‚æ•°æ˜¯ seq_len çš„é€‰å–ï¼šæ— è®ºä½ çš„ä¸šåŠ¡åœºæ™¯éœ€ä¸éœ€è¦é•¿æ–‡æœ¬ï¼Œéƒ½ä¸è¦ä¸€å¼€å§‹å°±ä½¿ç”¨ 32K / 200K è¿™ç§å¤¸å¼ çš„æ•°æ® seq_lenï¼Œç®—åŠ›æ ¹æœ¬æ‰¿å—ä¸èµ·ï¼Œattention çš„ seq_len^2 çš„è®¡ç®—é‡å®åœ¨å¯æ€•ã€‚rope çš„ NTK å¤–æ¨æ–¹æ³•å·²ç»æ˜¯å„å¤§å‚æ ‡é…çš„æ–¹æ¡ˆï¼š4K/8K + rope å° base + 90% æ•°æ®é‡ --> 32K/64K + rope å¤§ base + 10% æ•°æ®é‡ã€‚

## GPT-2å…»æˆè®° 

[Training and Fine-Tuning GPT-2 and GPT-3 Models Using Hugging Face Transformers and OpenAI API](https://www.it-jim.com/blog/training-and-fine-tuning-gpt-2-and-gpt-3-models-using-hugging-face-transformers-and-openai-api/)  éå¸¸ç»å…¸ï¼Œå…¥é—¨å¿…è¯»ã€‚
1.  it does not implement neural networks from scratch(ä»å¤´å¼€å§‹) but relies on lower-level frameworks PyTorch, TensorFlow, and FLAX. 
2. it heavily uses Hugging Face Hub, another Hugging Face project, a hub for downloadable neural networks for various frameworks. 
3. Model is a valid PyTorch model with some additional restrictions and naming conventions introduced by the transformers framework. 
4. Neural networks are not able to work with raw text; they only understand numbers. We need a tokenizer to convert a text string into a list of numbers. But first, it breaks the string up into individual tokens, which most often means â€œwordsâ€, although some models can use word parts or even individual characters. Tokenization is a classical natural language processing task. Once the text is broken into tokens, each token is replaced by an integer number called encoding from a fixed dictionary. Note that a tokenizer, and especially its dictionary, is model-dependent: you cannot use Bert tokenizer with GPT-2, at least not unless you train the model from scratch. Some models, especially of the Bert family, like to use special tokens, such as `[PAD]`,`[CLS]`, `[SEP]`, etc. GPT-2, in contrast, uses them very sparingly.

![](/public/upload/machine/gpt_architecture.jpg)

different GPT versions differ pretty much only in size, minor details, and the dataset+training regime. If you understand how GPT-2 or even GPT-1 works, you can, to a large extent, understand GPT-4 also. PSï¼š ä¸åŒçš„gptä»æ¨¡å‹ç»“æ„ä¸Šå·®åˆ«ä¸å¤§ã€‚

ä»¥GPT-2 ä¸ºä¾‹ 
1. The transformer itself works with a D-dimensional vector at every position, for GPT-2 D=768. 
2. V=50257 is the GPT-2 dictionary size. 

![](/public/upload/machine/gpt_2_tensor_dimensions.jpg)

### GPT-2 modelä½¿ç”¨

```python
config = transformers.GPT2Config.from_pretrained(MODEL_NAME)
config.do_sample = config.task_specific_params['text-generation']['do_sample']
config.max_length = config.task_specific_params['text-generation']['max_length']
# print(config)
model = transformers.GPT2LMHeadModel.from_pretrained(MODEL_NAME, config=config)
# Tokenizer
tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)
# Tokenize the input
enc = tokenizer(['The elf queen'], return_tensors='pt')
print('enc =', enc)
print(tokenizer.batch_decode(enc['input_ids']))

input_ids = enc['input_ids']
attention_mask = torch.ones(input_ids.shape, dtype=torch.int64) # è®­ç»ƒæ—¶ä¸æ˜¯è‡ªé€’å½’çš„ï¼Œæ‰€ä»¥ç”¨åˆ°äº†mask
# predicts the next token at each position. ä¹Ÿå°±æ˜¯ input_ids = [v1,v2,v3] è¾“å‡ºä¸º [v20,v30,v4]]ã€‚ v20 æ˜¯æ ¹æ®v1 ç”Ÿæˆçš„ä¸‹ä¸€ä¸ªtokenï¼Œå¤§æ¦‚ç‡è·Ÿv2 ä¸ä¸€æ ·ï¼Œv4 æ˜¯æ ¹æ®v1,v2,v3 ç”Ÿæˆçš„ã€‚
out = model(input_ids=input_ids, attention_mask=attention_mask)
logits = out['logits']
# -1 åœ¨python list é‡Œè¡¨ç¤ºæœ€åä¸€ä¸ªå…ƒç´ ã€‚
new_id = logits[:, -1, :].argmax(dim=1)
print(new_id)
print(tokenizer.batch_decode(new_id))
```

[GPT2 æºç è§£æ](https://zhuanlan.zhihu.com/p/630970209) å»ºè®®ç»†è¯»

```python
input_ids = enc['input_ids']
for i in range(20):
    attention_mask = torch.ones(input_ids.shape, dtype=torch.int64)
 Â  Â logits = model(input_ids=input_ids,attention_mask=attention_mask)['logits']  Â  Â  Â  Â  Â  Â  Â  Â  Â  
 Â  Â new_id = logits[:, -1, :].argmax(dim=1) Â   # Generate new ID
 Â  Â input_ids = torch.cat([input_ids, new_id.unsqueeze(0)], dim=1)  # input_ids åŠ å…¥æ–°ç”Ÿæˆçš„å­—ç¬¦
```

|i|input_ids|decoded text|next token|
|---|---|---|---|
|0|[464,23878,16599]|the elf queen|11|
|1|[464,23878,16599,11]|the elf queen,|508|
|2|[464,23878,16599,11,508]|the elf queen,who|550|

### å¾®è°ƒGPT-2 model

GPT models are trained in an unsupervised way on a large amount of text (or text corpus). The corpus is broken into sequences, usually of uniform size (e.g., 1024 tokens each). PSï¼š é¢„è®­ç»ƒç´ æé€šå¸¸è¢«åˆ‡æˆç‰¹å®šé•¿åº¦çš„å¥å­ã€‚The model is trained to predict the next token (word) at each step of the sequence. For example (here, we write words instead of integer encodings for clarity) :

|position|1|2|3|4|5|6|7|8|9|
|---|---|---|---|---|---|---|---|---|---|
|input_ids|The|elf|queen|was|wearing|a|cloak|.|[END]|
|labels|elf|queen|was|wearing|a|cloak|.|[END]|[-1]

**The labels are identical to input_ids, but shifted to one position to the left**. Note that for GPT-2 in Hugging Face transformers this shift happens automatically when the loss is calculated, so from the user perspective, the tensor labels should be identical to input_ids.  PSï¼šå¸¸è§„æ·±åº¦æ¨¡å‹çš„è®­ç»ƒè¾“å…¥æ˜¯ `feature1,feature2,...,label`ï¼ŒLLMä¹Ÿæ˜¯ï¼Œä¸è¿‡label æœ‰æ—¶æ˜¯ç”±input_idå¾—åˆ°çš„ã€‚

There are two ways to train Hugging Face transformers models: with the Trainer class or with a standard PyTorch training loop. We start with Trainer. PS: ä¸‹é¢ä»£ç åŸºäºGPT-2 å·²æœ‰çš„å‚æ•°å¾®è°ƒGPT-2ï¼Œ**æ„Ÿè§‰æ¨¡å‹å¾®è°ƒ è·Ÿmodel = load_checkpoint(xx) æ–­ç‚¹é‡è®­æ²¡å•¥åŒºåˆ«**ï¼Œä¾§é‡ç‚¹åœ¨äºè®²Transformersåº“åŸç†ã€‚

```python
class MyDset(torch.utils.data.Dataset):
    """A custom dataset that serves 1024-token blocks as input_ids == labels"""
    def __init__(self, data: list[list[int]]):
        self.data = []
        for d in data:
            input_ids = torch.tensor(d, dtype=torch.int64)
            attention_mask = torch.ones(len(d), dtype=torch.int64)
            self.data.append({'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': input_ids})

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx: int):
        return self.data[idx]
def break_text_to_pieces(text_path: str, tokenizer: transformers.PreTrainedTokenizer, block_len: int = 512) -> list[str]:
    """Read a file and convert it to tokenized blocks, edding <|endoftext|> to each block"""
    with open(text_path) as f:
        text = f.read()
    chunk_len0 = block_len - 1  # Leave space for a TOKEN_ENDOFTEXT
    tokens = tokenizer.encode(text) # åŸæ–‡æœ¬ç›´æ¥å¼„ï¼Œå¤Ÿç²—æš´
    blocks = []
    pos = 0
    while pos < len(tokens):
        chunk = tokens[pos: pos + chunk_len0]
        chunk.append(TOKEN_ENDOFTEXT)
        blocks.append(chunk)
        pos += chunk_len0

    if len(blocks[-1]) < block_len:
        del blocks[-1]

    return blocks
def train_val_split(data: list[str], ratio: float):
    n = len(data)
    assert n >= 2
    n_val = max(1, int(n * ratio))
    return data[n_val:], data[:n_val]
def prepare_dsets(text_path: str, tokenizer: transformers.PreTrainedTokenizer, block_len: int):
    """Read the text, prepare the datasets """
    data = break_text_to_pieces(text_path, tokenizer, block_len)
    data_train, data_val = train_val_split(data, 0.2)
    return MyDset(data_train), MyDset(data_val)

# Load model and tokenizer
model = transformers.GPT2LMHeadModel.from_pretrained(MODEL_NAME)
tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)
training_args = transformers.TrainingArguments(output_dir="idiot_save/", learning_rate=1e-3,...)
# ä¼ ç»™trainer çš„å¿…é¡»æ˜¯é¢„å¤„ç†å¥½çš„datasetï¼ˆåŒ…å«input_ids ç­‰columnï¼‰
trainer = transformers.Trainer(model=model,args=training_args,train_dataset=dset_train,eval_dataset=dset_val)
trainer.train()
# Save the model if needed
model.save_pretrained('./trained_model/')
tokenizer.save_pretrained('./trained_model/')
# Now our model is trained, try the generation
text = 'Natural language understanding comprises a wide range of diverse tasks'
batch = tokenizer([text], return_tensors='pt')
for k, v in batch.items():
    batch[k] = v.to(DEVICE)
out = model.generate(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], max_length=20)
print('GENERATION=', tokenizer.batch_decode(out.cpu()))
```

ä¸€æŠŠæƒ…å†µä¸‹ you are not allowed to train a model from scratch.  Neither are you allowed to fine-tune on a text corpus or fine-tune with additional heads. The only type of fine-tuning allowed is fine-tuning on prompt+completion pairs, represented in JSONL format, for example:

```
{"prompt":"banana is ","completion":"yellow"}
{"prompt":"orange is ","completion":"orange"}
{"prompt":"sky is ","completion":"blue"}
```
How exactly is GPT-3 trained on such examples? We are not exactly sure (OpenAI is very secretive), but perhaps the two sequences of tokens are concatenated together, then GPT-3 is trained on such examples, **but the loss is only calculated in the â€œcompletionâ€ part**. PS: ç»ˆäºçŸ¥é“ä¸ºä½•è¦åˆ†æˆä¸¤æ®µï¼Œè€Œä¸æ˜¯å–‚ä¸€ä¸ªæ–‡æœ¬å°±ç®—äº†ã€‚labels ä¸­promptéƒ¨åˆ†çš„ä½ç½®éƒ½ç½®ä¸º-100ï¼Œ-100è¡¨ç¤ºåœ¨è®¡ç®—lossçš„æ—¶å€™ä¼šè¢«å¿½ç•¥ï¼Œè¿™ä¸ªç”±ä»»åŠ¡æ€§è´¨å†³å®šã€‚

