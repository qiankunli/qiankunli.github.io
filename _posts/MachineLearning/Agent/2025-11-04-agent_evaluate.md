---

layout: post
title: Agent评估
category: 技术
tags: MachineLearning
keywords: agent software

---

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$']], // 支持 $和$$ 作为行内公式分隔符
      displayMath: [['$$', '$$']], // 块级公式分隔符
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script async src="/public/js/mathjax/es5/tex-mml-chtml.js"></script>

* TOC
{:toc}

## 简介(未完成)



Agent评估采用三层金字塔模型，按重要性和实施优先级划分：
1. 第一层：核心能力，规划、工具使用、推理、记忆
2. 第二层：应用效果 ，任务完成、输出质量、用户满意度
3. 第三层：生产就绪度，成本、延迟、安全、稳定性 

## 评估什么

### 核心能力评估（第一层）

规划与推理能力，Agent能否将复杂任务分解并逐步执行
1. 任务完成进度 = 已完成步骤/理想步骤数
2. 正确选择工具比例 = 正确调用/总调用
3. 遇错误后调整能力 = 成功恢复次数/错误次数

工具使用能力，Agent能否正确调用和组合各种工具：L1: 单工具调用 → L2: 顺序调用 → L3: 并行调用 → L4: 动态发现
1. 工具名称+参数正确性
2. 优先使用API而非浏览器
3. 工具组合效率：最少调用达成目标

记忆管理能力，Agent能否维护和利用长期记忆
1. 准确检索，从历史中提取正确信息
2. 在线学习，对话中新增学习
3. 长程理解，跨多轮维持一致性
4. 选择遗忘，过滤无关信息

自我反思与改进能力，Agent能否从反馈中学习并改进
1. 初次尝试 → Agent执行任务（可能失败）
2. 提供反馈 → 给出错误原因或改进建议
3. 二次尝试 → Agent根据反馈重新执行
4. 评估改进 → 计算Reflection Score

## 应用效果评估（第二层）

Agent是否达成业务目标
1. 完全成功
2. 部分成功
3. 功能完成
4. 完全失败

Agent输出内容的质量
1. 准确性
2. 相关性
3. 完整性，是否覆盖所有要点，关键点检查清单
4. 可用性，用户能否直接使用

## 如何评估


评测的最主要价值是：把产品负责人（以及产品团队）的目标和用户的实际需求进行量化，而量化的手段就是构建评测数据集。目前的AI应用面对的场景如此之复杂，以至于很难写出一个reward的规则方式进行评价。特别是需要考虑的输入数据分布很难通过其他方式描述，最终都只能转化为一个数据集来进行量化表示。
1. 机评与人评。在实际应用中，能完全量化为单个选项或者数值的任务并不多，不少任务都是给出一种非结构化的文本输出。这种情况下，就很难机械地对其进行自动化评估。一般我们会使用构建一个评估workflow（或单个prompt），来进行评估。
  1. 难点：首先评估的维度经常需要显式的列出并进行定义，但要把产品负责人的产品sense、或者是对用户群对于产品效果的期望精确地转换为评估维度的描述实际上非常难，大多数时候只能把能描述清楚的部分描述清楚，放弃剩下的部分。
2. 评测方案是会过时的