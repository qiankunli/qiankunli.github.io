---

layout: post
title: å¤§æ¨¡å‹æ¨ç†æœåŠ¡æ¡†æ¶
category: æ¶æ„
tags: MachineLearning
keywords: llm inference

---

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>


* TOC
{:toc}

## ç®€ä»‹

1. vLLMæ˜¯ä¸€ä¸ªå¼€æºçš„å¤§æ¨¡å‹æ¨ç†åŠ é€Ÿæ¡†æ¶ï¼Œé€šè¿‡PagedAttentioné«˜æ•ˆåœ°ç®¡ç†attentionä¸­ç¼“å­˜çš„å¼ é‡ï¼Œå®ç°äº†æ¯”HuggingFace Transformersé«˜14-24å€çš„ååé‡ï¼Œå°±åƒåœ¨æ“ä½œç³»ç»Ÿä¸­ç®¡ç†CPUè™šæ‹Ÿå†…å­˜ä¸€æ ·
2. NVIDIA FasterTransformer (FT) æ˜¯ä¸€ä¸ªç”¨äºå®ç°åŸºäºTransformerçš„ç¥ç»ç½‘ç»œæ¨ç†çš„åŠ é€Ÿå¼•æ“ã€‚å®ƒåŒ…å«Transformerå—çš„é«˜åº¦ä¼˜åŒ–ç‰ˆæœ¬çš„å®ç°ï¼Œå…¶ä¸­åŒ…å«ç¼–ç å™¨å’Œè§£ç å™¨éƒ¨åˆ†ã€‚ä½¿ç”¨æ­¤æ¨¡å—ï¼Œæ‚¨å¯ä»¥è¿è¡Œç¼–ç å™¨-è§£ç å™¨æ¶æ„æ¨¡å‹ï¼ˆå¦‚ï¼šT5ï¼‰ã€ä»…ç¼–ç å™¨æ¶æ„æ¨¡å‹ï¼ˆå¦‚ï¼šBERTï¼‰å’Œä»…è§£ç å™¨æ¶æ„æ¨¡å‹ï¼ˆå¦‚ï¼šGPTï¼‰çš„æ¨ç†ã€‚FTæ¡†æ¶æ˜¯ç”¨C++/CUDAç¼–å†™çš„ï¼Œä¾èµ–äºé«˜åº¦ä¼˜åŒ–çš„ cuBLASã€cuBLASLt å’Œ cuSPARSELt åº“ï¼Œè¿™ä½¿æ‚¨å¯ä»¥åœ¨ GPU ä¸Šè¿›è¡Œå¿«é€Ÿçš„ Transformer æ¨ç†ã€‚ä¸ NVIDIA TensorRT ç­‰å…¶ä»–ç¼–è¯‘å™¨ç›¸æ¯”ï¼ŒFT çš„æœ€å¤§ç‰¹ç‚¹æ˜¯å®ƒæ”¯æŒä»¥åˆ†å¸ƒå¼æ–¹å¼è¿›è¡Œ Transformer å¤§æ¨¡å‹æ¨ç†ã€‚åœ¨åº•å±‚ï¼ŒèŠ‚ç‚¹é—´æˆ–èŠ‚ç‚¹å†…é€šä¿¡ä¾èµ–äº MPI ã€ NVIDIA NCCLã€Glooç­‰ã€‚å› æ­¤ï¼Œä½¿ç”¨FasterTransformerï¼Œæ‚¨å¯ä»¥åœ¨å¤šä¸ª GPU ä¸Šä»¥å¼ é‡å¹¶è¡Œè¿è¡Œå¤§å‹Transformerï¼Œä»¥å‡å°‘è®¡ç®—å»¶è¿Ÿã€‚åŒæ—¶ï¼ŒTP å’Œ PP å¯ä»¥ç»“åˆåœ¨ä¸€èµ·ï¼Œåœ¨å¤š GPU èŠ‚ç‚¹ç¯å¢ƒä¸­è¿è¡Œå…·æœ‰æ•°åäº¿ã€æ•°ä¸‡äº¿ä¸ªå‚æ•°çš„å¤§å‹ Transformer æ¨¡å‹ã€‚
3. DeepSpeed-MII æ˜¯ DeepSpeed çš„ä¸€ä¸ªæ–°çš„å¼€æº Python åº“ï¼Œæ—¨åœ¨ä½¿æ¨¡å‹ä¸ä»…ä½å»¶è¿Ÿå’Œä½æˆæœ¬æ¨ç†ï¼Œè€Œä¸”è¿˜æ˜“äºè®¿é—®ã€‚

ä½¿ç”¨å¤§æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬åœ¨huggingfaceæˆ–modelscope çœ‹åˆ°çš„ä»£ç ç±»ä¼¼ä¸‹é¢ï¼Œå¾ˆæ˜æ˜¾ä¸èƒ½ç›´æ¥å‘ç”¨æˆ·æä¾›æœåŠ¡ã€‚ 
```python
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True)
model = AutoModel.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True).half().cuda()
model = model.eval()
response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
print(response)
ä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚
response, history = model.chat(tokenizer, "æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ", history=history)
print(response)
æ™šä¸Šç¡ä¸ç€å¯èƒ½ä¼šè®©ä½ æ„Ÿåˆ°ç„¦è™‘æˆ–ä¸èˆ’æœ,ä½†ä»¥ä¸‹æ˜¯ä¸€äº›å¯ä»¥å¸®åŠ©ä½ å…¥ç¡çš„æ–¹æ³•:...
```

ä¸€èˆ¬æœ‰å‡ ä¸ªéœ€æ±‚
1. ç»Ÿä¸€apiï¼Œè¿™æ ·åˆ‡æ¢æ¨¡å‹æ—¶ä¸Šæ¸¸åº”ç”¨æ— æ„Ÿï¼Œæœ€å¥½æ˜¯ OpenAI-compatibleï¼Œå…¶api è¢«ä¸»è¦ä¸Šæ¸¸æ¡†æ¶ï¼ˆæ¯”å¦‚langchainï¼‰å…¼å®¹
    1. æ”¯æŒæµå¼è¾“å‡ºå’Œæ™®é€šè¾“å‡º
2. æ”¯æŒå¤šå®ä¾‹ï¼Œè¿›è€Œæ”¯æŒç°åº¦å‘å¸ƒç­‰
3. æ”¯æŒé€šç”¨çš„åŠ é€Ÿåº“æ¯”å¦‚vllmç­‰

è®¾è®¡æ€è·¯
1. å› ä¸ºè¦æ”¯æŒä¸åŒçš„llm åº“æˆ–åŠ é€Ÿåº“ï¼Œæ¯”å¦‚Transformerã€vllmç­‰ï¼Œä¸”ä¸åŒçš„llmåœ¨ä¸€äº›ç»†èŠ‚ä¸Šæœ‰å·®å¼‚ï¼Œå› æ­¤æ¨ç†ä¾§å¿…é¡»æœ‰ä¸€ä¸ªç»Ÿä¸€çš„LLM æŠ½è±¡ï¼Œåœ¨Fastchaté‡Œæ˜¯XXModelWorkerï¼Œåœ¨xinference é‡Œæ˜¯XXLLM
2. å°†python llm åº“ apiåŒ–ï¼Œä¸€ä¸ªapi è¦æœ‰ä¸€ä¸ªapi handler å‡½æ•°ï¼Œä¸€èˆ¬æŠ½è±¡ä¸ºä¸€ä¸ªå¯¹è±¡ ä½œä¸ºapi handlerçš„è½½ä½“ï¼Œè¿™ä¸ªå¯¹è±¡æŒæœ‰ä¸Šé¢çš„XxLLM æ‰§è¡Œchat/generate æ–¹æ³•ï¼Œæœ‰æ—¶å€™è¿˜è¦æ”¯æŒ/å°è£…åˆ†å¸ƒå¼ã€å¼‚æ­¥ç­‰ç»†èŠ‚ã€‚åœ¨Fastchaté‡Œæ˜¯ModelWorkerï¼Œåœ¨xinference é‡Œæ˜¯WorkerActor
3. ä¸åŒçš„llm è¿˜æœ‰å¾ˆå¤šå·®åˆ«çš„ï¼ˆæ¯”å¦‚åŠ è½½ load_modelã€è¿è¡Œchat/generateã€æ¨¡å‹é…ç½®è½¬æ¢ï¼‰ï¼Œä¹Ÿæœ‰å¾ˆå¤šå…±æ€§ï¼Œæ‰€ä»¥æ¨¡å‹è®¾è®¡çš„åˆ†å±‚æŠ½è±¡å¾ˆé‡è¦ï¼ŒFastchat çš„æ€è·¯æ˜¯ æä¾›äº†ä¸€ä¸ªModelAdapterï¼ˆä¸»è¦å·®å¼‚åŒ–äº†åŠ è½½ï¼‰ å’Œä¸€ä¸ª generate_stream_gate å‡½æ•°æˆå‘˜ï¼ˆå·®å¼‚åŒ–textç”Ÿæˆï¼‰ï¼Œinferenceçš„æ€è·¯æ˜¯ä¸€ä¸ªæ¨¡å‹ï¼ˆæ¯”å¦‚chatglmã€llamaç­‰ï¼‰ä¸€ä¸ªXXLLM
  1. è¿™é‡Œçš„æ¨¡å‹é…ç½®è½¬æ¢è¯´çš„æ˜¯ï¼Œæ¯”å¦‚ä¸€ä¸ªchat message åŒ…å«role å’Œcontent ä¸¤ä¸ªéƒ¨åˆ†ï¼Œrole=system/user/assistant å„å®¶å„æœ‰å·®å¼‚ï¼Œä½†å› ä¸ºå¯¹å¤–æä¾›çš„æ¥å£ä¸€èˆ¬æ˜¯openai é£æ ¼ï¼Œæ‰€ä»¥æœ‰ä¸€ä¸ªè½¬æ¢çš„è¿‡ç¨‹ã€‚
4. é™¤äº†æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œè¿˜ç»å¸¸éœ€è¦éƒ¨ç½²embeddingæ¨¡å‹ã€rerankæ¨¡å‹ã€å›¾ç”Ÿå›¾ã€æ–‡ç”Ÿå›¾ç­‰ï¼ˆå…¥å‚å‡ºå‚ä¸LLM è‚¯å®šä¸ä¸€æ ·äº†ï¼‰ï¼ŒFastchat çš„æ–¹å¼æ˜¯ è®©ModelWorkeræ”¯æŒé™¤äº†generate_stream_xx å¤–çš„get_embeddingsã€get_rerankæ–¹æ³•ï¼Œinferenceçš„æ€è·¯é™¤äº†LLMä¹‹å¤–è¿˜å®šä¹‰äº† EmbeddingModelã€RerankModelç­‰ã€‚

## ç®€å•å°è£…

[ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) åœ¨github æœ‰ä¸€ä¸ªä»“åº“ï¼Œä¸€èˆ¬åŒ…å«
1. æ¨¡å‹ä»‹ç» README.md
2. æ¨¡å‹çš„å¯¹å¤–æ¥å£ api.py/cli_demo.py/web_demo.pyã€‚ è‡ªå·±ä½¿ç”¨ fastapi åŸºäºpythonåº“ç›´æ¥å¯¹å¤–æä¾›RESTful APIs.

ä»¥api.py ä¸ºä¾‹
```python
from fastapi import FastAPI, Request
from transformers import AutoTokenizer, AutoModel
import uvicorn, json, datetime
import torch

app = FastAPI()

@app.post("/")
async def create_item(request: Request):
    global model, tokenizer
    json_post_raw = await request.json()
    json_post = json.dumps(json_post_raw)
    json_post_list = json.loads(json_post)
    prompt = json_post_list.get('prompt')
    history = json_post_list.get('history')
    max_length = json_post_list.get('max_length')
    top_p = json_post_list.get('top_p')
    temperature = json_post_list.get('temperature')
    response, history = model.chat(tokenizer,prompt, history=history,
                                   max_length=max_length if max_length else 2048,
                                   top_p=top_p if top_p else 0.7,
                                   temperature=temperature if temperature else 0.95)
    now = datetime.datetime.now()
    time = now.strftime("%Y-%m-%d %H:%M:%S")
    answer = {"response": response,"history": history,"status": 200,"time": time}
    log = "[" + time + "] " + '", prompt:"' + prompt + '", response:"' + repr(response) + '"'
    print(log)
    torch_gc()
    return answer

if __name__ == '__main__':
    tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)
    model = AutoModel.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True).half().cuda()
    model.eval()
    uvicorn.run(app, host='0.0.0.0', port=8000, workers=1)
```

## FastChat

[ä¸€æ–‡å…¥é—¨æœ€çƒ­çš„LLMåº”ç”¨å¼€å‘æ¡†æ¶LangChain](https://mp.weixin.qq.com/s/bYzNNL3F0998Do2Jl0PQtw)

FastChatåŠŸèƒ½è¦†ç›–è®­ç»ƒï¼Œæ¨ç†ï¼Œè¯„ä¼°çš„å…¨è¿‡ç¨‹ã€‚è®¾è®¡ç›®æ ‡éå¸¸æ˜ç¡®ï¼Œå°±æ˜¯åœ¨æ€§èƒ½ã€åŠŸèƒ½åŠé£æ ¼ä¸Šå…¨é¢å¯¹æ ‡OpenAI ChatGPTï¼Œä»¥æˆä¸ºChatGPTçš„å¼€æºå¹³æ›¿ã€‚åœ¨ç”Ÿæ€é›†æˆä¸Šï¼Œç”±äºå®ƒå®Œå…¨å…¼å®¹OpenAIçš„é£æ ¼ï¼ŒåŸºäºChatGPTçš„langchainåº”ç”¨ï¼Œå¯ä»¥æ— ç¼åœ°ä½¿ç”¨FastChatæ›¿ä»£ã€‚ æ¨ç†ä¾§ç±»ä¼¼å·¥å…·Xinference/OpenLLM/RayLLM

[FastChat](https://github.com/lm-sys/FastChat)æ˜¯ä¸€ä¸ªç”¨äºè®­ç»ƒã€æœåŠ¡å’Œè¯„ä¼°åŸºäºèŠå¤©æœºå™¨äººçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„å¼€æ”¾å¹³å°ã€‚The core features include:
1. The training and evaluation code for state-of-the-art models (e.g., Vicuna).
2. A distributed multi-model serving system with web UI and OpenAI-compatible RESTful APIs.


```sh
# å‘½ä»¤è¡Œæ–¹å¼ä¸llm äº¤äº’
python3 -m fastchat.serve.cli --model-path lmsys/vicuna-7b-v1.3
# webuiæ–¹å¼ä¸llmäº¤äº’ï¼Œæ­¤æ—¶éœ€å¯åŠ¨3ä¸ªç»„ä»¶ web servers ==> controller ==> model workers
python3 -m fastchat.serve.controller
python3 -m fastchat.serve.model_worker --model-path lmsys/vicuna-7b-v1.3
python3 -m fastchat.serve.gradio_web_server
# æä¾›OpenAI-compatible RESTful APIs  openai_api_server ==> controller ==> model workers
python3 -m fastchat.serve.controller
python3 -m fastchat.serve.model_worker --model-path lmsys/vicuna-7b-v1.3
python3 -m fastchat.serve.openai_api_server --host localhost --port 8000
```

![](/public/upload/machine/langchain_chatchat_call.jpg)

### FastChatæºç åˆ†æ

```
fastchat
    /fastchat
        /model
            /model_adapter.py        # BaseModelAdapter  ChatGLMAdapter
            /model_chatglm.py        # generate_stream_chatglm
            /model_exllama.py
            /model_registry.py
        /protocol
            /api_protocol.py
            /openai_api_protocol.py
        /serve
            /model_worker.py         # app = FastAPI()     ModelWorker
            /controller.py.          # app = FastAPI().    Controller
            /openai_api_server.py    # app = fastapi.FastAPI()
        /train
```

ä½¿ç”¨ModelWorker åŠ è½½model æä¾›http æ¥å£ 

```python
app = FastAPI()
@app.post("/worker_generate")
async def api_generate(request: Request):
    params = await request.json()
    await acquire_worker_semaphore()
    output = worker.generate_gate(params)
    release_worker_semaphore()
    return JSONResponse(output)
if __name__ == "__main__":
    ...
    worker = ModelWorker(...,args.model_path,)
    uvicorn.run(app, host=args.host, port=args.port, log_level="info")
```
ModelWorkerå®ç°
```python
BaseModelWorker
     init_heart_beat
         # å°†modelWorker idæ³¨å†Œåˆ°controllerï¼Œå¹¶ä¿æŒå¿ƒè·³ã€‚å‡é€šè¿‡httpæ¥å£

# åŠ è½½æ¨¡å‹ï¼Œè°ƒç”¨æ¨¡å‹ï¼ˆåº•å±‚éƒ½æ˜¯è°ƒç”¨æµå¼æ¥å£ï¼‰
ModelWorker
     def __init__():
          self.model, self.tokenizer = load_model(model_path, device=device,...)
            # load_model å¯¹åº”ä¸€ä¸ªä¸“é—¨çš„ ModelAdapter æŠ½è±¡ï¼Œç”¨æ¥é€‚é…æ¨¡å‹çš„åŠ è½½
            adapter = get_model_adapter(model_path)
            model, tokenizer = adapter.load_model(model_path, kwargs)
     generate_stream_gate(self, params) 
     generate_gate(self, params)    # æ ¹æ®å‚æ•°è¿”å›è¾“å‡ºï¼Œè°ƒç”¨generate_stream_gate
        for x in self.generate_stream_gate(params):
            pass
        return json.loads(x[:-1].decode())
```
api => ModelWorker.generate_gate ==> ModelWorker.generate_stream_gate ==> ModelWorker.model.stream_generate
```python
generate_stream_gate
    get_generate_stream_function(model: torch.nn.Module, model_path: str)
       # æ ¹æ®æ¨¡å‹ä¸åŒé€‰æ‹©å¯¹åº”çš„å‡½æ•° 
       generate_stream_chatglm
            prompt = params["prompt"]
            inputs = tokenizer([prompt], return_tensors="pt").to(model.device)
            for total_ids in model.stream_generate(**inputs, **gen_kwargs):
                  response = tokenizer.decode(output_ids)
                  response = process_response(response)
```


### FastChat, How to support a new model?

1. FastChat uses the Conversation class to handle prompt templates and BaseModelAdapter class to handle model loading.
2. Implement a conversation template for the new model at `fastchat/conversation.py`. You can follow existing examples and use register_conv_template to add a new one. Please also add a link to the official reference code if possible. PSï¼š æ¯•ç«Ÿfastcaht æœåŠ¡chat åœºæ™¯å˜›ï¼Œå¯¹è¯è¯·æ±‚ä¼ å…¥çš„æ—¶å€™ ä¸€èˆ¬æ˜¯ `prompt = "\n###user:å¤©ä¸ºä»€ä¹ˆè¿™ä¹ˆè“ï¼Ÿ\n###"`ï¼Œè¦æŠŠè¿™ä¸ªè¿˜åŸä¸º `history = [{"role": "user", "content": "å¤©ä¸ºä»€ä¹ˆè¿™ä¹ˆè“ï¼Ÿ"}]`ï¼Œä¸åŒçš„æ¨¡å‹ å¯¹roleçš„ç§°å‘¼ä¸åŒã€‚
3. Implement a model adapter for the new model at `fastchat/model/model_adapter.py`. You can follow existing examples and use register_model_adapter to add a new one. PSï¼šä¸åŒçš„æ¨¡å‹åŠ è½½æ—¶æœ‰ä¸€äº›ç‰¹å®šçš„å‚æ•°ï¼Œæ¯”å¦‚ chatglm çš„trust_remote_code å‚æ•°ï¼Œ`model = AutoModel.from_pretrained(model_path, trust_remote_code=True, **from_pretrained_kwargs)`
4. ModelWorker ä¸»è¦é€»è¾‘æ˜¯æ‰§è¡Œ `generate_stream(model,tokenizer,params)` ï¼Œå¾ˆå¸¸è§„çš„ `input_ids = tokenizer(prompt); output_ids = model(input_ids,xx)`ã€‚ å¦‚æœæ¨¡å‹çš„generate é€»è¾‘æœ‰ä¸€äº›ç‰¹åˆ«çš„å¤„ç†ï¼Œåˆ™éœ€è¦è‡ªå®šä¹‰generate_stream_xxï¼Œå¹¶åŠ å…¥get_generate_stream_function é€»è¾‘ï¼ˆæ ¹æ®æ¨¡å‹åç­‰ è·¯ç”±åˆ°ä¸åŒçš„generate_stream_xxï¼‰
5. (Optional) add the model name to the "Supported models" section above and add more information in `fastchat/model/model_registry.py.`

å¦‚ä½•ç†è§£FastChat éƒ½å¹²äº†ä»€ä¹ˆï¼Ÿæœ¬è´¨æ˜¯å¯¹ä¸‹é¢çš„ åŸå§‹çš„å¤§æ¨¡å‹æ¨ç†ä»£ç è¿›è¡ŒæŠ½è±¡ï¼ˆæ¨¡å‹åŠ è½½ã€æ¨¡å‹æ¨ç†=tokenizer+modelï¼‰å’Œå°è£…ï¼Œå¯¹å¤–æä¾›rest apiã€‚

```python
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True)
model = AutoModel.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True).half().cuda()
model = model.eval()
response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
print(response)
ä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚
response, history = model.chat(tokenizer, "æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ", history=history)
print(response)
æ™šä¸Šç¡ä¸ç€å¯èƒ½ä¼šè®©ä½ æ„Ÿåˆ°ç„¦è™‘æˆ–ä¸èˆ’æœ,ä½†ä»¥ä¸‹æ˜¯ä¸€äº›å¯ä»¥å¸®åŠ©ä½ å…¥ç¡çš„æ–¹æ³•:...
```

## vllm æºç åˆ†æ

[vLLMä»£ç åŠé€»è¾‘ä»‹ç»](https://zhuanlan.zhihu.com/p/675322419)

[å¦‚ä½•è®©vLLMé€‚é…ä¸€ä¸ªæ–°æ¨¡å‹](https://zhuanlan.zhihu.com/p/680636375)




