---

layout: post
title: ceph学习
category: 技术
tags: Storage
keywords: ceph

---

## 前言

* TOC
{:toc}

![](/public/upload/storage/ceph_arch.png)

对象存储和文件存储的区别是不大的，存储的都是一样的东西，只是抛弃了统一的命名空间和目录树的结构

## 存储集群 RADOS

与hdfs 对比来看
1. hdfs 支持目录和文件等概念。包含datanode 和namenode，client 先通过namenode 获取文件的存储位置，再去namenode 获取真实的文件数据。
2. ceph 虽然客户端提供了多种访问形式，但Server 端本质是对象存储。 包含monitor 和 OSD，对于任何读或写操作，client首先向monitor 请求集群的map ，然后与osd 通信获取对象数据。

一般一个 OSD 对应一块磁盘。OSD支持多种存储引擎，以插件式的方式来进行管理使用，目前支持filestore，kvstore，memstore以及最新的bluestore.
1. FileStore需要通过操作系统自带的本地文件系统间接管理磁盘，所以所有针对RADOS层的对象操作，都需要预先转换为能够被本地文件系统识别、符合posix语义的文件操作，这个转换过程极其繁琐，效率低下。
2. BlueStore选择绕过本地文件系统，由自身接管裸设备（例如磁盘），直接进行对象操作，不再进行对象和文件之间的转换，从而使得整个对象存储的I/O路径大大缩短，这是BlueStore能提升性能的根本原因。 

BlueStore 实现了直接管理裸设备的方式，抛弃了本地文件系统。基于一系列技术
1. BlockDevice实现在用户态下使用linux aio直接对裸设备进行I/O操作，去除了本地文件系统的消耗
2. 为了管理裸设备就需要一个磁盘的空间管理系统，Bluestore的元数据是以KEY-VALUE的形式保存到RockDB里的，而RockDB又不能直接操作裸盘，为此 Bluestore自己实现了一个简洁的文件系统BlueFS。  Bluestore ==> rocksdb ==> BlueFS ==> BlockDevice

## 逻辑概念 pg 和 pool（未完成）
## 如何访问 RADOS

Ceph是一个高性能、可扩容的分布式存储系统，它提供三大功能：

1. 对象存储（RADOSGW）：提供RESTful接口，也提供多种编程语言绑定。兼容S3（是AWS里的对象存储）、Swift（是openstack里的对象存储）；
2. 块存储（RDB）：由RBD提供，可以直接作为磁盘挂载，内置了容灾机制；
3. 文件系统（CephFS）：提供POSIX兼容的网络文件系统CephFS，专注于高性能、大容量存储；

### RGW

Ceph为了支持通用的HTTP接口设计了RGW（RADOS GateWay即对象网关系统）系统。

![](/public/upload/storage/ceph_rgw.png)

### RBD

![](/public/upload/storage/ceph_rbd.png)

[每天10分钟玩转Ceph(二)探索RBD块存储接口](https://cloud.tencent.com/developer/article/1592961)
### cephfs

Ceph 文件系统 (CephFS) 是 RADOS 集群基于文件的接口。它提供了可扩展并且兼容 POSIX 的并行文件系统，将其数据和元数据作为对象存储在  Ceph 存储中。

![](/public/upload/storage/ceph_cephfs.png)


使用cephfs 需额外部署MDS。 元数据服务器 (MDS) 管理 CephFS 客户端的元数据。这一守护进程提供 CephFS 客户端访问相应 RADOS 对象所需要的信息，如文件在文件系统树中存储的位置。它管理目录层次结构，并且存储 RADOS 集群中各个文件的元数据（所有者、时间戳和模式等）。它也负责缓存元数据访问权限，并且管理客户端缓存来维护缓存一致性。


[Ceph 文件系统 CephFS 的介绍与配置](https://amito.me/2018/CephFS-Introduction-Installation-and-Configuration/)部署一个 CephFS, 步骤如下:

1. 在一个 Mon 节点上创建 Ceph 文件系统.

    ```sh
    # CephFS 文件系统需要至少两个池，cephfs_data用于存储 CephFS 数据，cephfs_metadata用于存储 CephFS 元数据。
    $ ceph osd pool create cephfs-data 256 256
    $ ceph osd pool create cephfs-metadata 64 64
    # 安装 ceph-common 包
    $ sudo yum install -y ceph-common
    # 创建一个 CephFS, 名字为 cephfs
    $ ceph fs new cephfs cephfs-metadata cephfs-data
    # 验证至少有一个 MDS 已经进入 Active 状态
    $ ceph fs status cephfs
    ```
2. 若使用 CephX 认证, 需要创建一个访问 CephFS 的客户端
    ```sh
    # 在 Monitor 上, 创建一个用户
    $ ceph auth get-or-create client.cephfs mon 'allow r' mds 'allow rw' osd 'allow rw pool=cephfs-data, allow rw pool=cephfs-metadata'
    # 验证生成的 key
    $ ceph auth get client.cephfs
    # 将 client keyring 复制到 client 节点的 /etc/ceph 目录下，并修改权限
    $ sudo scp root@mon1:/etc/ceph/ceph.client.cephfs.keyring /etc/ceph/ceph.client.cephfs.keyring
    $ sudo chmod 644 /etc/ceph/ceph.client.cephfs.keyring
    ```
3. 挂载 CephFS 到一个专用的节点.
    4. 以 kernel client 形式挂载 CephFS
    5. 以 FUSE client 形式挂载 CephFS [自制文件系统 — 02 FUSE 框架，开发者的福音](https://mp.weixin.qq.com/s/HvbMxNiVudjNPRgYC8nXyg)

以 kernel client 形式挂载 CephFS

```sh
# 建立挂载点, 例如 /cephfs
$ sudo mkdir -p /cephfs
# 挂载 CephFS. 列出多个 Monitors 的地址, 指定 CephX 所需的密钥文件和客户端名, 注意不是 keyring file:
$ sudo mount -t ceph mon1:6789,mon2:6789,mon3:6789/ /cephfs -o name=cephfs,secretfile=/etc/ceph/cephfs.secret
# 验证 CephFS 已经成功挂载
$ stat -f /cephfs
```

## 安装

[每天10分钟玩转Ceph(一)让Ceph集群运行起来](https://cloud.tencent.com/developer/article/1592733)

## 使用

1. [API DOCUMENTATION](https://docs.ceph.com/en/latest/api/) restful api
2. restful api 功能不全面，有时需要操作 ceph c library，非c 语言要通过对应机制 调用这些library


[CephFS+Kubernetes 在网易轻舟容器平台的实践](https://mp.weixin.qq.com/s/lBVRrPHni75WZeJdDQ6BCg)