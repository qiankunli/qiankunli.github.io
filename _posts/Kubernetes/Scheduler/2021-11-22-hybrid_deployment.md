---

layout: post
title: 在离线业务混部
category: 架构
tags: Kubernetes
keywords:  Kubernetes 混部

---

## 简介

* TOC
{:toc}

[数据中心日均 CPU 利用率 45% 的运行之道--阿里巴巴规模化混部技术演进](https://mp.weixin.qq.com/s?__biz=MzUzNzYxNjAzMg==&mid=2247483986&idx=1&sn=44e9ad3c4bc4529a79547ba506773881&chksm=fae5099dcd92808b9af6e8f28a661b8c16284efb4656131479d21e9092922b03728c1140042c&mpshare=1&scene=23&srcid=%23rd)

[阿里大规模业务混部下的全链路资源隔离技术演进](https://mp.weixin.qq.com/s/_DTQ4Q2dC-kN3zyozGf9QA)

整体脉络：粗放的资源评估；节点分时复用 ==> 节点超卖，根据负载动态调度（调整超卖比例、动态驱逐等）。即先超卖，然后又有一系列辅助动作保证Pod QoS。精细化编排==> 本质是对节点上不同pod的分类管理，干脆提出一个资源池的概念。节点划分资源池，高优池支持绑核、cpu负载控制在50%以下，池间强隔离，不同池运行不同类型的任务，池资源大小随负载动态调整。PS：内存是硬隔离的，没啥好说的，一般混部主要是在 cpu 的共享程度上做文章。 

## 问题

[一文看懂业界在离线混部技术](https://mp.weixin.qq.com/s/pPinyOY7s016mIWiQV2gFA)而造成资源利用率不高的原因主要有如下几个：
1. 粗放的资源评估/业务申请的资源配额超过实际使用量+服务的资源使用量存在波峰波谷：研发更关注如何快速稳定的迭代产品需求，所以在服务部署时，一般按照最大流量来估计服务所需资源。但在线服务大都具有明显的潮汐特征，导致大部分时间段资源利用率都很低（10% 以下）从而造成浪费。
2. 集群资源整合度不高：服务器的资源占用常常呈现非均衡状态，例如在线服务尤其是调用主链路上的扇出节点业务，高峰期往往呈现出 CPU 和带宽吃紧，但内存绰绰有余的情况。这导致虽然内存有冗余，但依然无法聚合等比例的其它闲置资源去形成有意义的计算实体。
3. 业务部署隔离：因为东西部机房成本差异较大和以及容量规划等问题，很多企业会将在线机房、离线机房完全隔离开，这样不同 AZ 甚至不同地域间的在离线作业完全无法融合，资源池也无法互通流转。

## 技术要求

### 可观测性体系

### 操作系统级/资源隔离

容器的本质是一个受限制的进程，进程之间通过 namespace 做隔离，cgroups 做资源限制。在云原生时代，大部分业务资源都是基于容器来隔离和限制，但是在资源超售叠加混部场景下，CPU、内存等方面依然可能存在争抢。
1. 例如在 CPU 方面，为了保证在线服务稳定性，普遍做法是进行绑核，将在线服务绑定在某个逻辑核心上避免其他业务占用。但是绑核对于有并行计算要求的服务并不友好，核数直接决定并行效率。
2. 在内存方面，离线作业往往会读取大量文件数据，导致操作系统会做 page cache，而原生操作系统对 page cache 的管理是全局的，不是容器维度的。

### 任务冲突时的资源保障：Priority 和 Qos

![](/public/upload/kubernetes/priority_vs_qos.png)

[阿里云容器服务差异化 SLO 混部技术实践](https://mp.weixin.qq.com/s/fkX_lStva96HEbmPbR6iZw)
1. CPU 资源质量
    1. [添加 K8S CPU limit 会降低服务性能？](https://mp.weixin.qq.com/s/cR6MpQu-n1cwMbXmVaXqzQ) 可以查看container_cpu_cfs_throttled_periods_total 指标
    1. CPU Burst，例如对于 CPU Limit = 2 的容器，操作系统内核会限制容器在每 100 ms 周期内最多使用 200 ms 的 CPU 时间片，进而导致请求的响应时延（RT）变大。当容器真实 CPU 资源使用小于 cfs_quota 时，内核会将多余的 CPU 时间“存入”到 cfs_burst 中；当容器有突发的 CPU 资源需求，需要使用超出 cfs_quota 的资源时，内核的 CFS 带宽控制器（CFS Bandwidth Controller，简称 BWC） 会允许其消费其之前存到 cfs_burst 的时间片。最终达到的效果是将容器**更长时间的平均 CPU 消耗限制在 quota 范围内，允许短时间内的 CPU 使用超过其 quota**。
    2. CPU 拓扑感知调度。在多核节点下，进程在运行过程中经常会被迁移到其不同的核心，考虑到有些应用的性能对 CPU 上下文切换比较敏感，kubelet 提供了 static 策略，允许 Guarantee 类型 Pod 独占 CPU 核心。CPU 绑核并不是“银弹”，若同一节点内大量 Burstable  类型 Pod 同时开启了拓扑感知调度，CPU 绑核可能会产生重叠，在个别场景下反而会加剧应用间的干扰。因此，拓扑感知调度更适合针对性的开启。
    3. 针对低优先级离线容器的 CPU 资源压制能力：内核Group Identity，Group Identity 功能可以对每一个容器设置身份标识，以区分容器中的任务优先级，系统内核在调度包含具有身份标识的任务时，会根据不同的优先级做相应处理。比如高优先级任务 有更多资源抢占机会。 
    4. 在 CPU 被持续压制的情况下，BE 任务自身的性能也会受到影响，将其驱逐重调度到其他空闲节点反而可以使任务更快完成。
2. 内存资源质量
    1. 时延敏感型业务（LS）和资源消耗型（BE）任务共同部署时，资源消耗型任务时常会瞬间申请大量的内存，使得系统的空闲内存触及全局最低水位线（global wmark_min），引发系统所有任务进入直接内存回收的慢速路径，进而导致延敏感型业务的性能抖动。
    2. 后台异步回收，当容器内存使用超过 memory.wmark_ratio 时，内核将自动启用异步内存回收机制，提前于直接内存回收，改善服务的运行质量。

[如何合理使用 CPU 管理策略，提升容器性能？](https://mp.weixin.qq.com/s/N7UWOjqEnZ8oojWgFGBOlQ)
1. Kubelet 默认的 CPU 管理策略会通过 Linux 内核的 CFS 带宽控制器（CFS Bandwidth Controller）来控制容器 CPU 资源的使用上限。在多核节点下，进程在运行过程中经常会被迁移到其不同的核心，考虑到有些应用的性能对 CPU 上下文切换比较敏感，Kubelet 还提供了 static 策略，允许 Guaranteed 类型 Pod 独占 CPU 核心。
2. 内核 CFS 调度是通过 cfs_period 和 cfs_quota 两个参数来管理容器 CPU 时间片消耗的，cfs_period 一般为固定值 100 ms，cfs_quota 对应容器的 CPU Limit。例如对于一个 CPU Limit = 2 的容器，其 cfs_quota 会被设置为 200ms，表示该容器在每 100ms 的时间周期内最多使用 200ms 的 CPU 时间片，即 2 个 CPU 核心。让应用管理员常常感到疑惑的是，为什么容器的资源利用率并不高，但却频繁出现应用性能下降的问题？从 CPU 资源的角度来分析，问题通常来自于以下两方面：
  1. 内核在根据 CPU Limit 限制容器资源消耗时产生的 CPU Throttle 问题；由于应用突发性的 CPU 资源需求（如代码逻辑热点、流量突增等），假设每个请求的处理时间均为 60 ms，即使容器在最近整体的 CPU 利用率较低，由于在 100 ms～200 ms 区间内连续处理了4 个请求，将该内核调度周期内的时间片预算（200ms）全部消耗，Thread 2 需要等待下一个周期才能继续将 req 2 处理完成，该请求的响应时延（RT）就会变长。这种情况在应用负载上升时将更容易发生，导致其 RT 的长尾情况将会变得更为严重。若想彻底解决 CPU Throttle，通常需要将 CPU Limit 调大两三倍，有时甚至五到十倍，问题才会得到明显缓解。而为了降低 CPU Limit 超卖过多的风险，还会降低容器的部署密度，进而导致整体资源成本上升。
  2. 受 CPU 拓扑结构的影响，部分应用对进程在 CPU 间的上下文切换比较敏感，尤其是在发生跨 NUMA 访问时的情况。在 NUMA 架构下，节点中的 CPU 和内存会被切分成了两部分甚至更多（例如图中 Socket0，Socket1），CPU 被允许以不同的速度访问内存的不同部分，当 CPU 跨 Socket 访问另一端内存时，其访存时延相对更高。因此我们需要避免将 CPU 分散绑定到多个 Socket 上，提升内存访问时的本地性。Kubelet 提供的 CPU 管理策略 “static policy”、以及拓扑管理策略 “single-numa-node”，会将容器与 CPU 绑定，可以提升应用负载与 CPU Cache，以及 NUMA 之间的亲和性，但绑核策略并不是“银弹”，某 CPU Limit = 2 的容器，其应用在 100ms 时间点收到了 4 个请求需要处理，在 Kubelet 提供的 static 模式下，容器会被固定在 CPU0 和 CPU1 两个核心，各线程只能排队运行。CPU 绑核解决的是进程在不同 Core，特别是不同 NUMA 间上下文切换带来的性能问题，但解决的同时也损失了资源弹性。

CPU Burst 解决了内核 BWC 调度时针对 CPU Limit 的限流问题，可以有效提升延时敏感型任务的性能表现。但 CPU Burst 本质并不是将资源无中生有地变出来，若容器 CPU 利用率已经很高（例如大于50%），CPU Burst 能起到的优化效果将会受限，此时应该通过 HPA 或 VPA 等手段对应用进行扩容。

### 调度层

[百度混部实践：如何提高 Kubernetes 集群资源利用率？](https://mp.weixin.qq.com/s/12XFN2lPB3grS5FteaF__A)

[历经 7 年双 11 实战，阿里巴巴是如何定义云原生混部调度优先级及服务质量的？](https://mp.weixin.qq.com/s/GrgWzxAfHe2Ml4biwai8eQ)在这些在线和离线的 Pod 之间，我们就需要用不同的调度优先级和服务质量等级，以满足在线和离线的实际运行需求。

[一文看懂业界在离线混部技术](https://mp.weixin.qq.com/s/pPinyOY7s016mIWiQV2gFA)目前主要有几种决策方式：
1. 整机分时复用：在固定的时间点 (比如凌晨以后) 跑离线作业，白天让出资源给在线服务。这种以时间维度切分的混部方式比较简单易理解，但整体资源利用率提升有限。
2. 资源部分共享：**将单机的资源整体划分为在线资源、离线资源以及在离线共享资源**，各资源之间隔离，提前划分预留。这种从单机资源维度切分的混部方式比分时复用相对更精细一些，但是需要资源规格较大的机器切分才有意义。
3. 资源完全共享：通过及时准确的资源预测手段、快速响应资源变化的能力，以及一套可以在资源水位发生变化时的服务保障措施，更高效自动化地实现机器资源复用。**资源归属不预设**，完全依据实时指标决策。

前一种属于静态决策，相对来说对底层可观测性体系的要求、对调度系统的高可用高性能的要求较低。后两种属于动态决策，在资源利用率的提升上比静态决策更优，但对前述支撑系统要求也更高。

调度执行：由于在线服务和离线作业工作模式的差异，往往需要采用不同的调度器进行调度（比如K8s和Yarn）。混部场景下，在线调度器和离线调度器同时部署在集群中，当资源比较紧张的时候，调度器会发生资源冲突，只能重试，此时调度器的吞吐量和调度性能会受到较大影响，最终影响调度的 SLA。同时，对于大规模批量调度场景，原生的K8s只支持在线服务的调度，从而带来改造成本。


## 多种工作负载的混部成为常态

[Koordinator 0.6：企业级容器调度系统解决方案，引入 CPU 精细编排、资源预留与全新的重调度框架](https://mp.weixin.qq.com/s/YdoxVxz_91ZFemF8JuxRvQ)当企业的应用越来越多，为每一种类型的应用单独规划集群，在运维成本和资源成本上将不再可行。企业管理不同业务类型的方式逐步从切分集群到共享集群，从切分节点池到共享节点池这样的方式演进。打造现代化的云原生调度系统，Koordinator 坚持了如下的设计思路：

1. 拥抱 Kubernetes 上游标准，基于 Scheduler-Framework 来构建调度能力，而不是实现一个全新的调度器。构建标准形成共识是困难的，但破坏是容易的，Koordinator 社区与 Kubernetes sig-scheduling 社区相向而行。
2. **QoS 是系统的一等公民**，与业界大多数调度器更多的关注编排结果（静态）不同，Koordinator 非常关注 Pod 运行时质量（QoS），因为对于调度系统的用户而言，运行时稳定性是其业务成功的关键。PS：精细化编排，QoS分的更细，除了k8s默认3个外，还考虑独占cpu、numa等。**感觉这个也不太好，需要用户了解各种QoS区别，并精确了解自己业务的特点**。
3. 状态自闭环，Koordinator 认为调度必须是一个完整的闭环系统，才能满足企业级应用要求。因此，我们在第一个版本就引入了状态反馈回路，节点会根据运行时状态调优容器资源，中心会根据节点运行时状态反馈做调度决策。
    1. 中心调度 + 单机调度联合决策，中心调度看到全局视角，其决策可以找到集群中最适合应用需求的节点，而单机调度可以在节点侧做一定的灵活度，以应对应用突发的流量颠簸。
    2. 调度 + 重调度密切配合，调度解决 Pod 一次性放置的问题，而重调度才是驱动集群资源编排长期保持最优化的关键。Koordinator 将建设面向 SLO 的重调度能力，持续的驱动应用的编排符合预定义的 SLO。
4. 智能化、简单化，Koordinator 并不是就所有的选择暴露把问题留给客户，而是根据应用特征智能的为用户提供优化配置建议，简化用户使用 Kubernetes 的成本。   
    1. 调度系统不止于解决调度、重调度、弹性伸缩等领域问题，一个完整的调度系统，需要具备基于历史数据驱动的自我迭代演进的能力。为工作负载的运行历史状态建立数据仓库，基于这些运行历史的大数据分析，持续的改进应用间在各个维度的的亲和、互斥关系，才能在用户运行时体验、集群资源利用效率同时达到最佳状态。
 
Koordinator 针对智能化调度的设计思路如下：
1. 智能资源超卖，Koordinator 首先解决的是节点资源充分利用的问题，通过分析节点容器的运行状态计算可超卖的资源量，并结合 QoS 的差异化诉求将超卖的资源分配给不同类型的任务，大幅提高集群的资源利用率。
2. QoS 感知的重调度，当节点中 Pod 的运行时 QoS 不符合预期时，Koordinator 将智能决策抑制更低优先级的任务亦或是迁移当前收到干扰的容器，从而解决应用 QoS 不满足导致的问题。问题：重调度的细节问题很多，Pod驱逐后 集群是否有资源保证可以运行，涉及到资源预留。 
PS：先超卖，QoS保证不了了再调整。

### 在离线混部

[基于Volcano的离在线业务混部技术探索](https://www.bilibili.com/video/BV1AZ4y1X7AQ) 视频未看

[一文看懂业界在离线混部技术](https://mp.weixin.qq.com/s/pPinyOY7s016mIWiQV2gFA)

[B站云原生混部技术实践](https://mp.weixin.qq.com/s/pPEkfrLm0XEpgMU1KjiD4A) **把离线pod往在线节点调**，有一些有趣的实现：把多余的cpu mem看做扩展资源，看request 这个节点已经满了，但是看扩展cpu/mem 还有很多，由于没有申请原生资源类型，那么k8s会自动将这类pod归类为best effort类型，并使用agent 根据负载调整 best effort的额度。 k8s原生调度器的基本原理和问题
1. 混部pod会占用原生的资源配额（例如cpu request），这会导致在线任务发布的时候没有可用资源；PS：把离线业务调到在线集群节点，得调的上去，调上去之后不能影响在线业务pod调度。 
2. 原生调度本质是静态调度，没有考虑机器实际负载，因此没法有效地将混部任务调度到实际负载有空闲的机器上。PS：得感知实际的机器负载。得调的上去，调上去之后要有措施防止出事。

[阿里巴巴云原生混部系统 Koordinator 正式开源](https://mp.weixin.qq.com/s/QpDuHCSTwwnYqSShHrG_tA) 代码已开源

在线资源通常要给一个预留，预留的部分就是浪费的部分，所以要挖掘在线业务占用的资源给离线业务用，但是要注意：资源隔离，在线随时可以抢占离线。


## 美团

[提升资源利用率与保障服务质量，鱼与熊掌不可兼得？](https://mp.weixin.qq.com/s/hQKM9beWcx7CKMvpJxznfQ)LAR全称是集群负载自动均衡管理系统（LAR，Load Auto-Regulator）

按照很多同学的理解，通过非常简单的操作即可达成这个目标——提高单机的服务部署密度。但如此简单的操作，为何全球数据中心资源利用率仅为10%~20%呢？利用率如此之低，这里最为关键的因素有三个：
1. 部署到同一台物理机的服务在资源使用上存在相互干扰。
2. 服务在流量上存在高低峰，反映在资源使用上也有高低峰。
3. 关键核心在线服务的服务质量下降无法接受。

传统的方案通过节点资源超卖来解决资源申请和实际资源使用之间存在的Gap，并引入根据负载的动态调度策略。
1. 调整节点资源超售，虽然能在一定程度上缓解资源申请和使用的Gap问题，但由于Gap在不同的服务间并不相同，加上服务资源使用的波峰波谷分布集中的情况（美团在线业务的典型特征），此方法在整体上过于粗放，会导致节点间的负载分布不均衡，部分节点负载很高，影响服务质量；另一部分节点负载极低，实际上形成资源浪费。
2. 而根据负载直接进行资源调度，由于负载是动态变化的，在调度算法设计及计算框架实现上会非常复杂，且效果一般。

提升资源利用率的本质是提升资源共享复用水平，而保障服务质量则需要通过资源隔离能力，保障服务的性能稳定。针对上述两个根本点，LAR在Kubernetes上提出两个核心创新点：
1. 资源池化分级
  1. 通过将单机资源划分到不同的资源池，提升资源在池内的共享复用水平。
  2. 不同的资源池之间有不同的优先级，并提供不同的资源隔离水平（资源隔离水平越高，资源共享复用水平越低）。
  3. 资源在不同优先级的资源池之间根据优先级和资源池的资源负载水平流动，优先保障高优资源池服务的资源使用，从而保障其服务质量。
2. 动态负载和静态资源映射
  1. 资源的分配，本质上是负载空间的分配。假设单机整体CPU利用率小于50%的情况下，运营在其上的服务的服务质量不会有影响，那么这个机器的静态资源其实对应的就是节点50% CPU利用率的负载空间。换个角度看，就是无论如何调度分配资源，只要这个节点的负载不超过50%即可。
  2. 业务静态的资源申请，根据服务的特征经过调度计算后，服务被放入对应的资源池，而资源池的资源配置则根据池内所有服务的实际负载进行资源配置，并可以实时地根据负载调整资源配置，实现静态资源分配和动态负载的映射管理。

通过池间资源隔离达到池间服务的干扰隔离。资源池内资源的配置**依据服务的负载进行动态调整**，并通过资源配置的调整，控制资源池内部的资源负载维系在相对稳定的范围内，从而保证服务质量。

以3级资源池为例，节点资源被划分为0、1、2三类资源池，优先级依次降低。初始整个机器无服务调度其上，资源全部集中在Pool2。随着服务的调度，Pool1先调度了服务1，这时会根据上述的资源计算方式，LAR将Pool2的对应的资源调整至Poo1，Pool2资源减少。随着Pool1中服务增多，配置的资源随之增多，Pool2相应资源减少。优先级最高的Pool0调入服务后，同样的资源从Pool2调整至Pool0；Pool2调度入服务时，Pool2资源不变。
3个资源池配置不同的资源配置管理策略，0号池优先级最高，池内目标CPU负载控制在30%～50%之间；1号池优先级次之，池内目标CPU负载控制在45%～60%之间；2号池优先级最低，池内目标CPU负载控制在50%～80%。已分配的资源由资源池内服务共享，在池间相互隔离。在负载低时，不同资源池根据资源池管理策略，自动调整各资源池的资源配置，保证资源池内负载稳定；出现资源紧张时，高优资源池可以从低优资源池抢占资源，优先保障高优服务的资源需求。

池内分配资源会随着负载进行变化，引起池间的资源流动。池间资源流动遵循以下规则：
1. 所有资源池的池内分配资源之和为节点可分配的资源总量。
2. 当池内负载降低，释放资源到最低等级的资源池，复用闲时资源。
3. 当池内负载升高，向等级低于自身的资源池，根据从低到高的顺序进行资源请求，根据优先级满足服务资源需求。
4. 池内的资源最多不会超过用户申请的量。

以3级资源池为例：
1. 当Pool1负载升高时，从等级更低的Pool2抢占资源，优先保障自身的服务资源需求，Pool1负载降低时，将冗余的资源释放回Pool2。
2. 当Pool0负载升高时，优先从Pool2抢占资源，当Pool2资源不足时，从Pool1抢占资源，保证更高等级的服务资源需求，当Pool0负载降低时，冗余的资源被释放回Pool2，此时3. 若Pool1存在负载压力，则会重新从Pool2抢占资源。

QoS服务质量保障机制，为提升资源利用率会导致资源竞争，LAR通过池间、池内两层QoS服务质量保障机制，分级保证服务的隔离性和稳定性。
1. 池间多维度资源隔离，LAR对资源池进行了多维度的资源隔离与限制。除了基础资源（CPU、Memory），还对磁盘I/O、CPU调度、Memory Cache、内存带宽、L3 Cache、OOM Score、网络带宽等更细粒度的资源进行了隔离，进一步提升不同等级服务间的隔离性，保证服务不会受到其他资源池的影响。PS：由MTOS 的相关特性支持
2. 池内多层级保障策略，当资源池内负载出现不符合预期的情况时（如容器负载异常），由于资源池内资源共享，整个资源池的服务都可能受到影响。LAR基于资源池内不同的负载等级，制定了多级保障策略。QoSAdaptor周期性（秒级）地获取节点负载的数据，并计算资源池的负载等级。当负载达到一定的资源等级时，执行对应的负载策略。通过CPU降级、驱逐等行为，根据优先级对部分容器进行资源降级，保障池内绝大多数容器的稳定性。
  1. 容器驱逐：当池内Memory使用接近Cgroup限制，避免整个资源池出现OOM，影响所有容器的正常运行，会结合优先级筛选Memory使用较多的容器进行驱逐操作。PS：Kubernetes原生的驱逐策略基于整个节点的负载，LAR中将策略缩小到了资源池维度
  2. CPU降级：池内CPU负载超过一定负载等级，避免高负载导致的容器间互相影响，LAR会结合优先级筛选CPU使用较多的容器，对其CPU使用进行单独的限制。降级操作存在定时检查机制，当负载恢复正常，或有资源可以抢占的情况下，会将CPU限制进行恢复。
  3. 强制抢占：从更低等级的资源池抢占资源，与普通资源抢占的区别为，即使资源已经被其他池使用，强制抢占会优先满足高等级资源池的需求。

LAR基于资源池的历史负载与历史分配情况，对池内高峰资源使用情况进行预测，为节点资源调整提供指导。由于资源池负载变化比较频繁，同时受到池内服务变更、资源总量、高低峰时间区间等因素的影响，节点基于实时负载进行池内资源的变更较不稳定。Recommender周期性地根据各节点资源池的历史负载与分配情况进行高峰资源预测，并下发到节点，提供高峰负载控制指导，提升资源池资源保障的稳定性。同时通过RCF完成动态负载和静态资源的转换，在调度层屏蔽了动态负载变化，减少负载频繁变化对调度准确性的影响。

LAR的设计目标是在保障服务质量的同时提升整体资源的利用率，在资源分池分级的设计上，针对通用的在线服务进行服务分级，对接不同资源池，提供不同的服务质量保障，从而提升资源的利用率。而对于离线服务，本身相对于在线服务的服务质量要求低，故而LAR天然地适用于混部场景。PS：**就是抛开在离线的概念，独立搞出一个带有优先级的资源池的概念**
1. 对于在线服务，通过对服务进行分级，并通过服务画像对服务进行细致刻画，将资源敏感型服务和关键核心服务部署到LAR优先级最高的资源池中
2. 而对于一般的在线服务，部署在次优先级资源池。
3. 在混部场景中，假设将资源池分为0、1、2三个级别，优先级依次由高到低。0和1号池分别对应核心关键在线服务和一般的在线服务，而2号池对应离线服务使用的资源池。

一方面我们对高优资源池配置更强的资源隔离策略（比如CPU绑核、进程优先调度等），另一方面高优池资源利用率控制在一个安全较低的水位；而低优池，则相对在一个更高的水平。LAR的资源动态调整保障负载能力，会自动将0号池与1号池在业务低峰期（负载低）的闲置资源回收，提供给2号池的离线服务使用。并且QoS服务质量保障机制，可以确保在业务高峰来临时，秒级抢占2号池资源（对于内存等非复用型资源，通过驱逐方式强制回收），从而保障在线服务的资源使用。

## 字节

[字节跳动的云原生技术历程演进](https://mp.weixin.qq.com/s/YM77RAZhkLiqZ3rqg3uXHA)为了解决资源统一管理这个问题，我们提出了三个思路：
1. **抽象能够提供的资源售卖模型，方便不同的业务线、业务系统准确地表达自身的需求**；我们给应用提供的资源形态以 CPU 维度为例一共分为三级：
  1. 独占核/dedicated_core：以独占的形式去获得物理核，这些 core 上除了应用自身以外不会运行其他租户的进程。我们又细分了 Numa 的拓扑分配以及忽略拓扑结构的两个子类，提供了对微拓扑结构上的优化选项；
  2. 共享核/shared_core：把不同的应用的 Pod 运行在一个共享 CPU 的 Pool 上，这样可以同时针对不同应用形态在 CPU 调度域上的划分，更细粒度地隔离开应用之间的影响；
  3. 回收核/reclaimed_core：在共享核的基础上，通过混部控制系统的方式去回收部分的低优资源，我们可以低优混部的共享方式去提供算力的供给。
2. 创建一套统一的 Quota 管理平台，这个平台可以让开发者们灵活地管理自身的各类资源；
3. 资源分层调度系统使得单机集群对字节内部所有计算资源做到快速灵活的交付。
  1. 单机调度主要是扩展了 Kubernetes 的单机资源管控：资源的微拓扑结构感知和资源的分配策略，主要解决了如何让不同 cores 形态的 Pod 统一运行在一个节点之上。
  2. 集群中心调度器需要解决的核心问题是如何让不同形态的应用在整个集群里自由地调度。需要满足不同的调度语义细粒度的要求，充分降低集群空置率。在调度性能方面，同时要满足低频次和批式的大吞吐的调度场景。针对各种应用场景提升调度场景的质量也是集群中心调度器需要解决的问题。
  3. 全局调度，考虑到字节跳动的整体规模，单一的集群能力不足以满足管理字节全球数据中心的需求，并且在应用之间的隔离、多区域的容灾以及算力的标准化问题上字节也有更加细粒度的调度要求。
